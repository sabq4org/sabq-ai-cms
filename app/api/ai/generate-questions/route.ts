import { NextRequest } from "next/server";
import OpenAI from "openai";

const CACHE_TTL = 60 * 60 * 1000; // 1 Ø³Ø§Ø¹Ø© Ø¨Ø§Ù„Ù…ÙŠÙ„ÙŠ Ø«Ø§Ù†ÙŠØ©

type CacheEntry = { value: string; expiresAt: number };

declare global {
  // eslint-disable-next-line no-var
  var __SMART_Q_CACHE__: Map<string, CacheEntry> | undefined;
}

function getMemoryCache(): Map<string, CacheEntry> {
  if (!globalThis.__SMART_Q_CACHE__) {
    globalThis.__SMART_Q_CACHE__ = new Map<string, CacheEntry>();
  }
  return globalThis.__SMART_Q_CACHE__ as Map<string, CacheEntry>;
}

export async function POST(req: NextRequest) {
  try {
    const body = await req.json();
    const { articleId, title, content } = body || {};
    if (!articleId || (!title && !content)) {
      return new Response(JSON.stringify({ error: "missing articleId or content/title" }), { status: 400 });
    }

    const cacheKey = `smart_q:${articleId}`;
    const mem = getMemoryCache();
    const cached = mem.get(cacheKey);
    if (cached && cached.expiresAt > Date.now()) {
      return new Response(cached.value, { status: 200, headers: { "Content-Type": "application/json", "X-Cache": "HIT" } });
    }

    const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
    const prompt = `
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© Ø¹Ù…ÙŠÙ‚Ø© Ù„Ø®Ø¨Ø± ØµØ­ÙÙŠ. Ø£Ù†Ø´Ø¦ 5 Ø£Ø³Ø¦Ù„Ø© ØªØ­Ù„ÙŠÙ„ÙŠØ© Ù…ÙˆØ¬Ù‡Ø© Ù„Ù„Ù‚Ø§Ø±Ø¦ ØªØ³Ø§Ø¹Ø¯ Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„Ø®Ù„ÙÙŠØ§Øª ÙˆØ§Ù„Ø³ÙŠØ§Ù‚Ø§Øª ÙˆØ§Ù„ØªØ¨Ø¹Ø§ØªØŒ ÙˆØªØ¬Ù†Ù‘Ø¨ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø©. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµÙŠØ­Ø© Ø§Ù„Ù…Ø®ØªØµØ±Ø©. Ø£Ø¹Ø¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ØµÙŠØºØ© JSON Ø¨Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„ØªØ§Ù„ÙŠ:
[{"question":"...","type":"analysis","icon":"â“"}, {"question":"...","type":"why","icon":"ğŸ§­"}, {"question":"...","type":"impact","icon":"ğŸ“ˆ"}, {"question":"...","type":"context","icon":"ğŸ§©"}, {"question":"...","type":"what_next","icon":"â¡ï¸"}]

Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: ${title || ""}
Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø®ØªØµØ± (Ø§Ù‚ØªØ·Ø¹ Ø£Ù‡Ù… 800-1200 Ø­Ø±Ù): ${content ? String(content).slice(0, 1200) : ""}
    `.trim();

    const completion = await client.chat.completions.create({
      model: process.env.OPENAI_MODEL || "gpt-4o-mini",
      messages: [
        { role: "system", content: "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ ÙŠÙƒØªØ¨ Ø£Ø³Ø¦Ù„Ø© Ø¹Ù…ÙŠÙ‚Ø© Ù…Ø®ØµØµØ© Ù„ÙƒÙ„ Ø®Ø¨Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰." },
        { role: "user", content: prompt }
      ],
      temperature: 0.4,
      max_tokens: 600
    });

    const text = completion.choices?.[0]?.message?.content?.trim() || "[]";
    let questions: any[] = [];
    try {
      questions = JSON.parse(text);
      if (!Array.isArray(questions)) questions = [];
    } catch {
      questions = [];
    }

    const payload = JSON.stringify({ questions });
    mem.set(cacheKey, { value: payload, expiresAt: Date.now() + CACHE_TTL });
    return new Response(payload, { status: 200, headers: { "Content-Type": "application/json", "X-Cache": "MISS" } });
  } catch (e: any) {
    return new Response(JSON.stringify({ error: e?.message || "failed" }), { status: 500 });
  }
}


