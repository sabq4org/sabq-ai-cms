# Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
# Advanced Arabic Text Processor for Sentiment Analysis

import re
import string
import logging
from typing import List, Dict, Tuple, Optional, Any, Union
from dataclasses import dataclass
import unicodedata
import emoji
import camel_tools
from camel_tools.utils.normalize import normalize_alef_maksura_ar, normalize_alef_ar, normalize_teh_marbuta_ar
from camel_tools.utils.dediac import dediac_ar
from camel_tools.tokenizers.word import simple_word_tokenize
from camel_tools.morphology.database import MorphologyDB
from camel_tools.morphology.analyzer import Analyzer
from camel_tools.sentiment import SentimentAnalyzer
import farasa
from transformers import AutoTokenizer
import numpy as np

logger = logging.getLogger(__name__)

@dataclass
class TextProcessingConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ"""
    remove_diacritics: bool = True
    normalize_alef: bool = True
    normalize_teh_marbuta: bool = True
    remove_tatweel: bool = True
    remove_english: bool = False
    remove_numbers: bool = False
    remove_punctuation: bool = False
    remove_emojis: bool = False
    extract_emojis: bool = True
    min_word_length: int = 2
    max_word_length: int = 50
    preserve_negation: bool = True
    handle_repetition: bool = True
    detect_dialect: bool = True
    stem_words: bool = False
    lemmatize: bool = True

class ArabicEmotionExtractor:
    """Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ© ÙˆØ§Ù„Ù†ØµÙˆØµ"""
    
    def __init__(self):
        # Ø±Ù…ÙˆØ² ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©
        self.positive_emojis = {
            'ğŸ˜€', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜', 'ğŸ˜†', 'ğŸ˜Š', 'â˜ºï¸', 'ğŸ˜‹', 'ğŸ˜', 'ğŸ˜', 'ğŸ¥°', 'ğŸ˜˜',
            'ğŸ˜—', 'ğŸ˜™', 'ğŸ˜š', 'ğŸ¤—', 'ğŸ¤©', 'ğŸ¥³', 'ğŸ˜‡', 'ğŸ‘', 'ğŸ‘Œ', 'âœŒï¸', 'ğŸ¤', 'ğŸ’ª',
            'ğŸ‘', 'ğŸ™Œ', 'ğŸ‘', 'ğŸ¤', 'â¤ï¸', 'ğŸ’•', 'ğŸ’–', 'ğŸ’—', 'ğŸ’™', 'ğŸ’š', 'ğŸ’›',
            'ğŸ§¡', 'ğŸ’œ', 'ğŸ¤', 'ğŸ–¤', 'ğŸ’¯', 'âœ¨', 'â­', 'ğŸŒŸ', 'ğŸ’«', 'ğŸ”¥', 'â¤ï¸â€ğŸ”¥'
        }
        
        # Ø±Ù…ÙˆØ² ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ø³Ù„Ø¨ÙŠØ©
        self.negative_emojis = {
            'ğŸ˜', 'ğŸ˜”', 'ğŸ˜Ÿ', 'ğŸ˜•', 'ğŸ™', 'â˜¹ï¸', 'ğŸ˜£', 'ğŸ˜–', 'ğŸ˜«', 'ğŸ˜©', 'ğŸ¥º', 'ğŸ˜¢',
            'ğŸ˜­', 'ğŸ˜¤', 'ğŸ˜ ', 'ğŸ˜¡', 'ğŸ¤¬', 'ğŸ˜±', 'ğŸ˜¨', 'ğŸ˜°', 'ğŸ˜¥', 'ğŸ˜“', 'ğŸ¤•', 'ğŸ¤’',
            'ğŸ˜·', 'ğŸ¤§', 'ğŸ¤®', 'ğŸ¤¢', 'ğŸ˜µ', 'ğŸ¥´', 'ğŸ˜ª', 'ğŸ˜´', 'ğŸ’”', 'â¤ï¸â€ğŸ©¹', 'ğŸ‘',
            'ğŸ–•', 'ğŸ˜ˆ', 'ğŸ‘¿', 'ğŸ’€', 'â˜ ï¸', 'ğŸ‘»', 'ğŸ¤¡', 'ğŸ’©'
        }
        
        # Ø±Ù…ÙˆØ² ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ù…Ø­Ø§ÙŠØ¯Ø©
        self.neutral_emojis = {
            'ğŸ˜', 'ğŸ˜‘', 'ğŸ˜¶', 'ğŸ™„', 'ğŸ˜', 'ğŸ˜’', 'ğŸ¤”', 'ğŸ¤¨', 'ğŸ§', 'ğŸ¤“', 'ğŸ˜¬', 'ğŸ¤',
            'ğŸ¤«', 'ğŸ¤¯', 'ğŸ˜²', 'ğŸ˜®', 'ğŸ˜¯', 'ğŸ˜¦', 'ğŸ˜§', 'ğŸ˜®â€ğŸ’¨', 'ğŸ˜¤', 'ğŸ¤·', 'ğŸ¤·â€â™‚ï¸',
            'ğŸ¤·â€â™€ï¸', 'ğŸ¤', 'âœ‹', 'ğŸ¤š', 'ğŸ–ï¸', 'âœŒï¸', 'ğŸ‘‹'
        }

    def extract_emoji_sentiment(self, text: str) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ©"""
        emojis_found = []
        emoji_sentiment_scores = []
        
        for char in text:
            if char in emoji.EMOJI_DATA:
                emojis_found.append(char)
                
                if char in self.positive_emojis:
                    emoji_sentiment_scores.append(1.0)
                elif char in self.negative_emojis:
                    emoji_sentiment_scores.append(-1.0)
                elif char in self.neutral_emojis:
                    emoji_sentiment_scores.append(0.0)
                else:
                    # Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ
                    emoji_sentiment_scores.append(0.0)
        
        if emoji_sentiment_scores:
            avg_emoji_sentiment = np.mean(emoji_sentiment_scores)
            emoji_confidence = len(emoji_sentiment_scores) / max(len(text.split()), 1)
        else:
            avg_emoji_sentiment = 0.0
            emoji_confidence = 0.0
        
        return {
            'emojis': emojis_found,
            'emoji_sentiment_score': avg_emoji_sentiment,
            'emoji_confidence': emoji_confidence,
            'emoji_count': len(emojis_found)
        }

class ArabicDialectDetector:
    """ÙƒØ§Ø´Ù Ø§Ù„Ù„Ù‡Ø¬Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    
    def __init__(self):
        # ÙƒÙ„Ù…Ø§Øª Ù…Ù…ÙŠØ²Ø© Ù„ÙƒÙ„ Ù„Ù‡Ø¬Ø©
        self.dialect_keywords = {
            'saudi': [
                'ÙˆØ´', 'Ø§ÙŠØ´', 'ÙƒÙŠÙÙƒ', 'Ø¹ÙŠØ§Ù„', 'Ù…Ø§ØµØ¯Ù‚', 'Ø²ÙŠÙ†', 'ÙŠØµÙŠØ±', 'Ø§ÙŠÙˆØ©',
                'Ø£Ø®ÙˆÙŠ', 'Ø£Ø®ØªÙŠ', 'ÙŠØ§Ø¨Ù†', 'ÙŠØ§Ù„Ù„Ù‡', 'Ø£Ø­ÙŠÙ†', 'Ø£Ù‡Ù†ÙŠ', 'ØªØ¹Ø§Ù„'
            ],
            'egyptian': [
                'Ø§ÙŠÙ‡', 'Ø¥Ø²Ø§ÙŠ', 'Ø¥Ø²ÙŠÙƒ', 'Ø£Ù‡Ùˆ', 'ÙƒØ¯Ù‡', 'Ø¹Ù„Ø´Ø§Ù†', 'Ø¹Ø§ÙŠØ²', 'Ø¹Ø§ÙˆØ²',
                'Ø¯Ù„ÙˆÙ‚ØªÙŠ', 'Ù‡Ùˆ', 'Ù‡ÙŠ', 'Ø¯Ù‡', 'Ø¯ÙŠ', 'ÙŠÙ„Ø§', 'Ù…Ø¹Ù„Ø´', 'Ø¨Ø±Ø¶Ùˆ'
            ],
            'levantine': [
                'Ø´Ùˆ', 'ÙƒÙŠÙ', 'Ù…ÙŠÙ†', 'ÙˆÙŠÙ†', 'Ù‡ÙŠÙƒ', 'Ù‡Ø§ÙŠ', 'Ù‡Ø§Ø¯', 'ÙŠØ¹Ù†ÙŠ',
                'Ø£Ø­Ù„Ù‰', 'Ø­Ø¨ÙŠØ¨ÙŠ', 'Ø­Ø¨ÙŠØ¨ØªÙŠ', 'Ø®Ù„Øµ', 'Ø·ÙŠØ¨', 'ØªÙ…Ø§Ù…', 'Ù…Ø¹Ù‚ÙˆÙ„'
            ],
            'gulf': [
                'Ø´Ù„ÙˆÙ†', 'Ø´Ù†Ùˆ', 'ÙˆÙŠÙ†', 'Ù…Ø§Ù„', 'Ù…Ø§Ù„Øª', 'Ù‡Ø°ÙŠ', 'Ù‡Ø°Ø§', 'ÙŠØ§',
                'Ø­ÙŠØ§Ùƒ', 'ØªØ³Ù„Ù…', 'Ø²ÙŠÙ†', 'Ù…Ø¨', 'Ù…Ùˆ', 'ÙˆØ¯ÙŠ', 'Ø£Ø¨ÙŠ', 'Ø£Ø¨ØºÙ‰'
            ],
            'maghrebi': [
                'Ø£Ø´', 'Ø£Ø´Ù†Ùˆ', 'ÙƒÙŠÙØ§Ø´', 'ÙˆÙŠÙ†', 'Ù‡Ù†Ø§', 'Ù‡Ù†Ø§Ùƒ', 'Ø¨Ø²Ø§Ù', 'Ø´ÙˆÙŠØ©',
                'ÙˆØ§Ø®Ø§', 'Ù…Ø§Ø´ÙŠ', 'Ø¨Ø­Ø§Ù„', 'ØºÙŠØ±', 'ÙƒÙŠÙ…Ø§', 'Ø¯ÙŠÙƒ', 'Ø¯Ø§Ø¨Ø§'
            ]
        }
    
    def detect_dialect(self, text: str) -> Dict[str, Any]:
        """ÙƒØ´Ù Ø§Ù„Ù„Ù‡Ø¬Ø© Ù…Ù† Ø§Ù„Ù†Øµ"""
        text_lower = text.lower()
        dialect_scores = {}
        
        for dialect, keywords in self.dialect_keywords.items():
            score = 0
            found_keywords = []
            
            for keyword in keywords:
                if keyword in text_lower:
                    score += 1
                    found_keywords.append(keyword)
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Ù‚Ø§Ø· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ
            normalized_score = score / max(len(text.split()), 1)
            dialect_scores[dialect] = {
                'score': normalized_score,
                'keywords_found': found_keywords,
                'keyword_count': score
            }
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹
        if dialect_scores:
            best_dialect = max(dialect_scores.keys(), 
                             key=lambda x: dialect_scores[x]['score'])
            best_score = dialect_scores[best_dialect]['score']
            
            # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†Ù‚Ø§Ø· Ù…Ù†Ø®ÙØ¶Ø© Ø¬Ø¯Ø§Ù‹ØŒ Ø§Ø¹ØªØ¨Ø±Ù‡Ø§ ÙØµØ­Ù‰
            if best_score < 0.01:
                predicted_dialect = 'msa'
                confidence = 0.8
            else:
                predicted_dialect = best_dialect
                confidence = min(best_score * 10, 1.0)  # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ confidence score
        else:
            predicted_dialect = 'msa'
            confidence = 0.5
        
        return {
            'predicted_dialect': predicted_dialect,
            'confidence': confidence,
            'dialect_scores': dialect_scores,
            'is_dialectal': predicted_dialect != 'msa'
        }

class ArabicTextProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø´Ø§Ù…Ù„"""
    
    def __init__(self, config: TextProcessingConfig = None):
        self.config = config or TextProcessingConfig()
        
        # ØªØ­Ù…ÙŠÙ„ Ø£Ø¯ÙˆØ§Øª CAMeL
        try:
            self.db = MorphologyDB.builtin_db()
            self.analyzer = Analyzer(self.db)
            logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø£Ø¯ÙˆØ§Øª CAMeL Ø¨Ù†Ø¬Ø§Ø­")
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ CAMeL: {e}")
            self.db = None
            self.analyzer = None
        
        # ØªØ­Ù…ÙŠÙ„ Farasa
        try:
            self.farasa = farasa.FarasaSegmenter(interactive=True)
            logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Farasa Ø¨Ù†Ø¬Ø§Ø­")
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Farasa: {e}")
            self.farasa = None
        
        # Ù…ÙƒÙˆÙ†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        self.emotion_extractor = ArabicEmotionExtractor()
        self.dialect_detector = ArabicDialectDetector()
        
        # Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ¹Ø¨ÙŠØ±Ø§Øª Ø§Ù„Ù…Ù†ØªØ¸Ù…Ø©
        self.patterns = self._compile_patterns()
        
        # ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù†ÙÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        self.negation_words = {
            'Ù„Ø§', 'Ù…Ø§', 'Ù„Ù…', 'Ù„Ù†', 'Ù„ÙŠØ³', 'Ù„ÙŠØ³Øª', 'ØºÙŠØ±', 'Ø¨Ø¯ÙˆÙ†', 'Ù…Ø´', 'Ù…Ùˆ', 'Ù…Ø¨'
        }
        
        # ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªØ£ÙƒÙŠØ¯
        self.emphasis_words = {
            'Ø¬Ø¯Ø§Ù‹', 'Ø¬Ø¯Ø§', 'ÙƒØ«ÙŠØ±', 'ÙƒØªÙŠØ±', 'Ø¬Ø§Ù…Ø¯', 'Ù‚ÙˆÙŠ', 'Ø¨Ø²Ø§Ù', 'ÙˆØ§ÙŠØ¯', 'Ù…Ø±Ø©'
        }
    
    def _compile_patterns(self) -> Dict[str, re.Pattern]:
        """ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù†ØªØ¸Ù…Ø©"""
        return {
            # Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙ†Ø¸ÙŠÙ
            'emails': re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'),
            'urls': re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'),
            'mentions': re.compile(r'@[\w\u0600-\u06FF]+'),
            'hashtags': re.compile(r'#[\w\u0600-\u06FF]+'),
            'numbers': re.compile(r'\b\d+\b'),
            'english': re.compile(r'[a-zA-Z]+'),
            'punctuation': re.compile(r'[^\w\s\u0600-\u06FF]'),
            'extra_whitespace': re.compile(r'\s+'),
            'repetition': re.compile(r'(.)\1{2,}'),  # ØªÙƒØ±Ø§Ø± Ø§Ù„Ø£Ø­Ø±Ù
            'tatweel': re.compile(r'\u0640+'),  # Ø§Ù„ØªØ·ÙˆÙŠÙ„
            'diacritics': re.compile(r'[\u064B-\u0652\u0670\u0640]'),  # Ø§Ù„ØªØ´ÙƒÙŠÙ„
        }
    
    def basic_clean(self, text: str) -> str:
        """Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ù†Øµ"""
        if not text or not isinstance(text, str):
            return ""
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨ Ø°Ù„Ùƒ
        if self.config.remove_emojis:
            text = ''.join(char for char in text if char not in emoji.EMOJI_DATA)
        
        # ØªØ·Ø¨ÙŠØ¹ Unicode
        text = unicodedata.normalize('NFKC', text)
        
        # Ø¥Ø²Ø§Ù„Ø© URLs ÙˆØ§Ù„Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª
        text = self.patterns['urls'].sub(' ', text)
        text = self.patterns['emails'].sub(' ', text)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨ Ø°Ù„Ùƒ
        if self.config.remove_english:
            text = self.patterns['english'].sub(' ', text)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨ Ø°Ù„Ùƒ
        if self.config.remove_numbers:
            text = self.patterns['numbers'].sub(' ', text)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨ Ø°Ù„Ùƒ
        if self.config.remove_punctuation:
            text = self.patterns['punctuation'].sub(' ', text)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ·ÙˆÙŠÙ„
        if self.config.remove_tatweel:
            text = self.patterns['tatweel'].sub('', text)
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
        if self.config.handle_repetition:
            text = self.patterns['repetition'].sub(r'\1\1', text)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        text = self.patterns['extra_whitespace'].sub(' ', text)
        
        return text.strip()
    
    def normalize_arabic(self, text: str) -> str:
        """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
        if not text:
            return ""
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
        if self.config.remove_diacritics:
            text = dediac_ar(text)
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø£Ù„Ù
        if self.config.normalize_alef:
            text = normalize_alef_ar(text)
            text = normalize_alef_maksura_ar(text)
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„ØªØ§Ø¡ Ø§Ù„Ù…Ø±Ø¨ÙˆØ·Ø©
        if self.config.normalize_teh_marbuta:
            text = normalize_teh_marbuta_ar(text)
        
        return text
    
    def tokenize(self, text: str) -> List[str]:
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ²"""
        if not text:
            return []
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… CAMeL tools Ù„Ù„ØªÙ‚Ø³ÙŠÙ…
        try:
            tokens = simple_word_tokenize(text)
        except:
            # ØªÙ‚Ø³ÙŠÙ… Ø¨Ø³ÙŠØ· ÙƒØ¨Ø¯ÙŠÙ„
            tokens = text.split()
        
        # ØªØµÙÙŠØ© Ø§Ù„Ø±Ù…ÙˆØ²
        filtered_tokens = []
        for token in tokens:
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø£Ùˆ Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹
            if len(token) < self.config.min_word_length or len(token) > self.config.max_word_length:
                continue
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… ÙÙ‚Ø·
            if token.isdigit() and self.config.remove_numbers:
                continue
            
            filtered_tokens.append(token)
        
        return filtered_tokens
    
    def extract_linguistic_features(self, text: str) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ù„ØºÙˆÙŠØ©"""
        features = {}
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
        features['word_count'] = len(text.split())
        features['char_count'] = len(text)
        features['sentence_count'] = len(re.split(r'[.!?ØŸ]', text))
        
        # ÙƒØ´Ù Ø§Ù„Ù†ÙÙŠ
        negation_count = sum(1 for word in text.split() if word in self.negation_words)
        features['negation_ratio'] = negation_count / max(len(text.split()), 1)
        features['has_negation'] = negation_count > 0
        
        # ÙƒØ´Ù Ø§Ù„ØªØ£ÙƒÙŠØ¯
        emphasis_count = sum(1 for word in text.split() if word in self.emphasis_words)
        features['emphasis_ratio'] = emphasis_count / max(len(text.split()), 1)
        features['has_emphasis'] = emphasis_count > 0
        
        # Ù†Ø³Ø¨Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ¹Ø¬Ø¨ ÙˆØ§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…
        exclamation_count = text.count('!') + text.count('ØŸ')
        features['exclamation_ratio'] = exclamation_count / max(len(text), 1)
        
        # Ù†Ø³Ø¨Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„ÙƒØ¨ÙŠØ±Ø© (Ù„Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„Ø·Ø©)
        if any(c.isupper() for c in text):
            upper_count = sum(1 for c in text if c.isupper())
            features['uppercase_ratio'] = upper_count / max(len(text), 1)
        else:
            features['uppercase_ratio'] = 0.0
        
        return features
    
    def process_text(self, text: str, extract_emotions: bool = True, 
                    detect_dialect: bool = True) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ù†Øµ"""
        if not text or not isinstance(text, str):
            return self._empty_result()
        
        original_text = text
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ
        emoji_data = {}
        if extract_emotions:
            emoji_data = self.emotion_extractor.extract_emoji_sentiment(text)
        
        # ÙƒØ´Ù Ø§Ù„Ù„Ù‡Ø¬Ø©
        dialect_data = {}
        if detect_dialect:
            dialect_data = self.dialect_detector.detect_dialect(text)
        
        # Ø§Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØ§Ù„ØªØ·Ø¨ÙŠØ¹
        cleaned_text = self.basic_clean(text)
        normalized_text = self.normalize_arabic(cleaned_text)
        
        # Ø§Ù„ØªÙ‚Ø³ÙŠÙ…
        tokens = self.tokenize(normalized_text)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ù„ØºÙˆÙŠØ©
        linguistic_features = self.extract_linguistic_features(original_text)
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Øµ
        processing_stats = {
            'original_length': len(original_text),
            'cleaned_length': len(cleaned_text),
            'normalized_length': len(normalized_text),
            'token_count': len(tokens),
            'reduction_ratio': 1 - (len(normalized_text) / max(len(original_text), 1))
        }
        
        return {
            'original_text': original_text,
            'cleaned_text': cleaned_text,
            'normalized_text': normalized_text,
            'tokens': tokens,
            'linguistic_features': linguistic_features,
            'emoji_analysis': emoji_data,
            'dialect_analysis': dialect_data,
            'processing_stats': processing_stats,
            'is_valid': len(tokens) > 0
        }
    
    def _empty_result(self) -> Dict[str, Any]:
        """Ù†ØªÙŠØ¬Ø© ÙØ§Ø±ØºØ© Ù„Ù„Ù†ØµÙˆØµ ØºÙŠØ± Ø§Ù„ØµØ§Ù„Ø­Ø©"""
        return {
            'original_text': "",
            'cleaned_text': "",
            'normalized_text': "",
            'tokens': [],
            'linguistic_features': {},
            'emoji_analysis': {},
            'dialect_analysis': {},
            'processing_stats': {},
            'is_valid': False
        }
    
    def batch_process(self, texts: List[str], **kwargs) -> List[Dict[str, Any]]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…Ø¹Ø© Ù„Ù„Ù†ØµÙˆØµ"""
        results = []
        
        for text in texts:
            try:
                result = self.process_text(text, **kwargs)
                results.append(result)
            except Exception as e:
                logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ: {text[:50]}... - {str(e)}")
                results.append(self._empty_result())
        
        return results

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
    config = TextProcessingConfig(
        remove_diacritics=True,
        normalize_alef=True,
        handle_repetition=True,
        detect_dialect=True
    )
    
    processor = ArabicTextProcessor(config)
    
    # Ù†ØµÙˆØµ ØªØ¬Ø±ÙŠØ¨ÙŠØ©
    test_texts = [
        "Ø£Ø­Ø¨ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙƒØ«ÙŠØ±Ø§Ù‹! Ø¥Ù†Ù‡ Ø±Ø§Ø¦Ø¹ Ø¬Ø¯Ø§Ù‹ ğŸ˜âœ¨",
        "Ù‡Ø°Ø§ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø³ÙŠØ¡ ÙˆÙ…Ù…Ù„Ù„ ğŸ˜ğŸ‘",
        "ÙˆØ´ Ø±Ø§ÙŠÙƒ ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ù„ØŸ Ø²ÙŠÙ† ÙˆÙ„Ø§ Ù„Ø§ØŸ",  # Ø³Ø¹ÙˆØ¯ÙŠ
        "Ø§ÙŠÙ‡ Ø±Ø£ÙŠÙƒ ÙÙŠ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¯Ù‡ØŸ Ø­Ù„Ùˆ ÙˆÙ„Ø§ ÙˆØ­Ø´ØŸ",  # Ù…ØµØ±ÙŠ
        "Ø´Ùˆ Ø¨Ø¯Ùƒ ØªØ¹Ù…Ù„ Ø¨Ù‡Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ØŸ Ø­Ù„Ùˆ ÙƒØªÙŠØ±!"  # Ø´Ø§Ù…ÙŠ
    ]
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ
    for text in test_texts:
        print(f"\nğŸ“ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ: {text}")
        result = processor.process_text(text)
        
        print(f"ğŸ§¹ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù†Ø¸Ù: {result['normalized_text']}")
        print(f"ğŸ” Ø§Ù„Ø±Ù…ÙˆØ²: {result['tokens']}")
        
        if result['dialect_analysis']:
            dialect = result['dialect_analysis']['predicted_dialect']
            confidence = result['dialect_analysis']['confidence']
            print(f"ğŸ—£ï¸ Ø§Ù„Ù„Ù‡Ø¬Ø©: {dialect} (Ø«Ù‚Ø©: {confidence:.2f})")
        
        if result['emoji_analysis']:
            emoji_sentiment = result['emoji_analysis']['emoji_sentiment_score']
            emoji_count = result['emoji_analysis']['emoji_count']
            print(f"ğŸ˜Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ù…ÙˆØ²: {emoji_sentiment:.2f} ({emoji_count} Ø±Ù…ÙˆØ²)")
        
        if result['linguistic_features']:
            negation = result['linguistic_features']['has_negation']
            emphasis = result['linguistic_features']['has_emphasis']
            print(f"ğŸ“Š Ø§Ù„Ù…Ø¹Ø§Ù„Ù…: Ù†ÙÙŠ={negation}, ØªØ£ÙƒÙŠØ¯={emphasis}")
