# Ø®Ø¯Ù…Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ø¹Ø§Ù…
# Public Opinion Analysis Service

import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Set
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation
import redis.asyncio as redis
import re
import hashlib

logger = logging.getLogger(__name__)

@dataclass
class OpinionData:
    """Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ø£ÙŠ"""
    text: str
    timestamp: datetime
    source: str
    sentiment_score: float
    sentiment_label: str
    emotion_scores: Dict[str, float]
    confidence: float
    user_id: Optional[str] = None
    location: Optional[str] = None
    influence_score: float = 1.0
    topic_relevance: float = 1.0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'text': self.text,
            'timestamp': self.timestamp.isoformat(),
            'source': self.source,
            'sentiment_score': self.sentiment_score,
            'sentiment_label': self.sentiment_label,
            'emotion_scores': self.emotion_scores,
            'confidence': self.confidence,
            'user_id': self.user_id,
            'location': self.location,
            'influence_score': self.influence_score,
            'topic_relevance': self.topic_relevance
        }

@dataclass
class OpinionTrend:
    """Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ø¹Ø§Ù…"""
    topic: str
    overall_sentiment: str
    sentiment_distribution: Dict[str, float]
    opinion_strength: float
    polarization_level: float
    dominant_emotions: List[Tuple[str, float]]
    key_themes: List[str]
    influencer_opinions: List[Dict[str, Any]]
    geographic_distribution: Dict[str, Any]
    temporal_trends: Dict[str, float]
    controversy_score: float
    reliability_score: float
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'topic': self.topic,
            'overall_sentiment': self.overall_sentiment,
            'sentiment_distribution': self.sentiment_distribution,
            'opinion_strength': self.opinion_strength,
            'polarization_level': self.polarization_level,
            'dominant_emotions': self.dominant_emotions,
            'key_themes': self.key_themes,
            'influencer_opinions': self.influencer_opinions,
            'geographic_distribution': self.geographic_distribution,
            'temporal_trends': self.temporal_trends,
            'controversy_score': self.controversy_score,
            'reliability_score': self.reliability_score
        }

class PublicOpinionAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self, redis_client: redis.Redis = None):
        self.redis_client = redis_client
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„
        self.min_opinions_threshold = 20
        self.polarization_threshold = 0.6
        self.controversy_threshold = 0.7
        self.influence_weight_factor = 0.3
        self.geographic_weight_factor = 0.2
        
        # Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©
        self.supported_sources = {
            'twitter', 'facebook', 'instagram', 'tiktok', 'youtube',
            'news_comments', 'forums', 'blogs', 'surveys', 'interviews'
        }
        
        # ÙØ¦Ø§Øª Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹
        self.topic_categories = {
            'politics': ['Ø³ÙŠØ§Ø³Ø©', 'Ø­ÙƒÙˆÙ…Ø©', 'Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª', 'Ø³ÙŠØ§Ø³ÙŠ', 'ÙˆØ²ÙŠØ±', 'Ø±Ø¦ÙŠØ³'],
            'economy': ['Ø§Ù‚ØªØµØ§Ø¯', 'Ù…Ø§Ù„ÙŠ', 'ØªØ¬Ø§Ø±Ø©', 'Ø£Ø³Ø¹Ø§Ø±', 'Ø±ÙˆØ§ØªØ¨', 'Ø§Ø³ØªØ«Ù…Ø§Ø±'],
            'sports': ['Ø±ÙŠØ§Ø¶Ø©', 'ÙƒØ±Ø©', 'ÙØ±ÙŠÙ‚', 'Ø¨Ø·ÙˆÙ„Ø©', 'Ù„Ø§Ø¹Ø¨', 'Ù…Ø¨Ø§Ø±Ø§Ø©'],
            'technology': ['ØªÙ‚Ù†ÙŠØ©', 'ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§', 'Ø°ÙƒÙŠ', 'ØªØ·Ø¨ÙŠÙ‚', 'Ø¨Ø±Ù†Ø§Ù…Ø¬', 'Ø±Ù‚Ù…ÙŠ'],
            'health': ['ØµØ­Ø©', 'Ø·Ø¨', 'Ù…Ø±Ø¶', 'Ø¹Ù„Ø§Ø¬', 'Ø¯ÙˆØ§Ø¡', 'Ù…Ø³ØªØ´ÙÙ‰'],
            'education': ['ØªØ¹Ù„ÙŠÙ…', 'Ù…Ø¯Ø±Ø³Ø©', 'Ø¬Ø§Ù…Ø¹Ø©', 'Ø·Ø§Ù„Ø¨', 'Ù…Ø¹Ù„Ù…', 'ØªØ±Ø¨ÙŠØ©'],
            'social': ['Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ', 'Ù…Ø¬ØªÙ…Ø¹', 'Ø£Ø³Ø±Ø©', 'Ø²ÙˆØ§Ø¬', 'Ø´Ø¨Ø§Ø¨', 'Ù†Ø³Ø§Ø¡'],
            'culture': ['Ø«Ù‚Ø§ÙØ©', 'ÙÙ†', 'Ø£Ø¯Ø¨', 'Ù…ÙˆØ³ÙŠÙ‚Ù‰', 'Ù…Ø³Ø±Ø­', 'ØªØ±Ø§Ø«']
        }
        
        # Ù…ÙØ§ØªÙŠØ­ Redis
        self.keys = {
            'opinions': 'opinion:data',
            'topics': 'opinion:topics',
            'trends': 'opinion:trends',
            'influencers': 'opinion:influencers',
            'analysis_cache': 'opinion:cache'
        }
        
        # Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø± (Ù…Ø¯Ù‰ Ø§Ù„Ù…ØµØ¯Ø§Ù‚ÙŠØ©)
        self.source_weights = {
            'surveys': 1.0,
            'interviews': 0.9,
            'news_comments': 0.8,
            'forums': 0.7,
            'blogs': 0.6,
            'twitter': 0.5,
            'facebook': 0.4,
            'instagram': 0.3,
            'tiktok': 0.2,
            'youtube': 0.4
        }
    
    async def collect_opinion_data(self, topic: str, sources: List[str], 
                                 time_range: str = "7d") -> List[OpinionData]:
        """Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ø¹Ø§Ù… Ø­ÙˆÙ„ Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø¹ÙŠÙ†"""
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø²Ù…Ù†ÙŠ
            hours = self._parse_time_range(time_range)
            cutoff_time = datetime.now() - timedelta(hours=hours)
            
            all_opinions = []
            
            for source in sources:
                if source in self.supported_sources:
                    source_opinions = await self._collect_from_source(
                        topic, source, cutoff_time
                    )
                    all_opinions.extend(source_opinions)
            
            # ÙÙ„ØªØ±Ø© ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            filtered_opinions = await self._filter_and_clean_opinions(
                all_opinions, topic
            )
            
            # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹
            for opinion in filtered_opinions:
                opinion.topic_relevance = self._calculate_topic_relevance(
                    opinion.text, topic
                )
            
            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ØµÙ„Ø© ÙˆØ§Ù„ØªØ£Ø«ÙŠØ±
            filtered_opinions.sort(
                key=lambda x: x.topic_relevance * x.influence_score,
                reverse=True
            )
            
            logger.info(f"âœ… ØªÙ… Ø¬Ù…Ø¹ {len(filtered_opinions)} Ø±Ø£ÙŠ Ø­ÙˆÙ„ Ù…ÙˆØ¶ÙˆØ¹ '{topic}'")
            return filtered_opinions
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ø£ÙŠ: {str(e)}")
            return []
    
    def _parse_time_range(self, time_range: str) -> int:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø²Ù…Ù†ÙŠ Ø¥Ù„Ù‰ Ø³Ø§Ø¹Ø§Øª"""
        if time_range.endswith('d'):
            return int(time_range[:-1]) * 24
        elif time_range.endswith('h'):
            return int(time_range[:-1])
        else:
            return 168  # Ø£Ø³Ø¨ÙˆØ¹ Ø§ÙØªØ±Ø§Ø¶ÙŠ
    
    async def _collect_from_source(self, topic: str, source: str, 
                                 cutoff_time: datetime) -> List[OpinionData]:
        """Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…ØµØ¯Ø± Ù…Ø¹ÙŠÙ†"""
        # ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ù‡Ù†Ø§ Ø³ÙŠØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ APIs Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        # Ø­Ø§Ù„ÙŠØ§Ù‹ Ø³Ù†Ø³ØªØ®Ø¯Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ø§ÙƒØ§Ø© Ù…Ù† Redis
        
        try:
            source_key = f"{self.keys['opinions']}:{source}:{topic}"
            
            # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø©
            stored_data = await self.redis_client.lrange(source_key, 0, -1)
            
            opinions = []
            for data_str in stored_data:
                try:
                    data = json.loads(data_str)
                    timestamp = datetime.fromisoformat(data['timestamp'])
                    
                    if timestamp >= cutoff_time:
                        opinion = OpinionData(
                            text=data['text'],
                            timestamp=timestamp,
                            source=source,
                            sentiment_score=data['sentiment_score'],
                            sentiment_label=data['sentiment_label'],
                            emotion_scores=data['emotion_scores'],
                            confidence=data['confidence'],
                            user_id=data.get('user_id'),
                            location=data.get('location'),
                            influence_score=data.get('influence_score', 1.0)
                        )
                        opinions.append(opinion)
                        
                except (json.JSONDecodeError, KeyError, ValueError) as e:
                    logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ø£ÙŠ: {str(e)}")
                    continue
            
            return opinions
            
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† {source}: {str(e)}")
            return []
    
    async def _filter_and_clean_opinions(self, opinions: List[OpinionData], 
                                       topic: str) -> List[OpinionData]:
        """ÙÙ„ØªØ±Ø© ÙˆØªÙ†Ø¸ÙŠÙ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¢Ø±Ø§Ø¡"""
        filtered = []
        seen_texts = set()
        
        for opinion in opinions:
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
            text_hash = hashlib.md5(opinion.text.encode()).hexdigest()
            if text_hash in seen_texts:
                continue
            seen_texts.add(text_hash)
            
            # ÙÙ„ØªØ±Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ Ø£Ùˆ Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹
            if len(opinion.text) < 10 or len(opinion.text) > 1000:
                continue
            
            # ÙÙ„ØªØ±Ø© Ø§Ù„Ù†ØµÙˆØµ Ø°Ø§Øª Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø©
            if opinion.confidence < 0.5:
                continue
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†Øµ
            if self._is_quality_text(opinion.text):
                filtered.append(opinion)
        
        return filtered
    
    def _is_quality_text(self, text: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†Øµ"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±Ù…ÙˆØ² ÙƒØ«ÙŠØ±Ø©
        symbol_ratio = len(re.findall(r'[^\w\s\u0600-\u06FF]', text)) / len(text)
        if symbol_ratio > 0.3:
            return False
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙƒØ±Ø±Ø© (ØªÙƒØ±Ø§Ø± Ù†ÙØ³ Ø§Ù„ÙƒÙ„Ù…Ø©)
        words = text.split()
        if len(words) > 3:
            unique_words = set(words)
            if len(unique_words) / len(words) < 0.5:
                return False
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ ÙƒÙ„Ù…Ø§Øª Ø¹Ø±Ø¨ÙŠØ©
        arabic_chars = len(re.findall(r'[\u0600-\u06FF]', text))
        if arabic_chars / len(text) < 0.3:
            return False
        
        return True
    
    def _calculate_topic_relevance(self, text: str, topic: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹"""
        text_lower = text.lower()
        topic_lower = topic.lower()
        
        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø¹Ù† Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹
        if topic_lower in text_lower:
            base_score = 1.0
        else:
            base_score = 0.5
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø°Ø§Øª ØµÙ„Ø©
        relevance_boost = 0.0
        
        for category, keywords in self.topic_categories.items():
            for keyword in keywords:
                if keyword in text_lower:
                    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ÙŠØªØ¹Ù„Ù‚ Ø¨Ù‡Ø°Ù‡ Ø§Ù„ÙØ¦Ø©
                    if any(cat_word in topic_lower for cat_word in keywords):
                        relevance_boost += 0.1
                    else:
                        relevance_boost += 0.05
        
        final_score = min(1.0, base_score + relevance_boost)
        return final_score
    
    async def analyze_opinion(self, topic: str, sources: Optional[List[str]] = None,
                            time_range: str = "7d", 
                            sentiment_threshold: float = 0.6) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ø¹Ø§Ù… Ø­ÙˆÙ„ Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø¹ÙŠÙ†"""
        try:
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¥Ø°Ø§ Ù„Ù… ØªÙØ­Ø¯Ø¯
            if not sources:
                sources = list(self.supported_sources)
            
            # Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            opinions = await self.collect_opinion_data(topic, sources, time_range)
            
            if len(opinions) < self.min_opinions_threshold:
                return {
                    'success': False,
                    'error': 'insufficient_data',
                    'message': f'Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„. Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {self.min_opinions_threshold}, Ø§Ù„Ù…ØªÙˆÙØ±: {len(opinions)}',
                    'available_opinions': len(opinions)
                }
            
            # Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
            sentiment_analysis = self._analyze_sentiment_distribution(opinions)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªÙ‚Ø·Ø§Ø¨
            polarization_analysis = self._analyze_polarization(opinions)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹ÙˆØ§Ø·Ù
            emotion_analysis = self._analyze_emotions(opinions)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ ÙˆØ§Ù„Ø«ÙŠÙ…Ø§Øª
            theme_analysis = await self._extract_themes(opinions)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¤Ø«Ø±ÙŠÙ†
            influencer_analysis = self._analyze_influencers(opinions)
            
            # Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠ
            geographic_analysis = self._analyze_geographic_distribution(opinions)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©
            temporal_analysis = self._analyze_temporal_trends(opinions)
            
            # Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬Ø¯Ù„ ÙˆØ§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©
            controversy_score = self._calculate_controversy_score(
                sentiment_analysis, polarization_analysis
            )
            reliability_score = self._calculate_reliability_score(
                opinions, sources
            )
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ø¹Ø§Ù…
            overall_sentiment = self._determine_overall_sentiment(
                sentiment_analysis, sentiment_threshold
            )
            
            # Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø©
            opinion_trend = OpinionTrend(
                topic=topic,
                overall_sentiment=overall_sentiment,
                sentiment_distribution=sentiment_analysis['distribution'],
                opinion_strength=sentiment_analysis['strength'],
                polarization_level=polarization_analysis['level'],
                dominant_emotions=emotion_analysis['dominant'],
                key_themes=theme_analysis['themes'],
                influencer_opinions=influencer_analysis['top_influencers'],
                geographic_distribution=geographic_analysis,
                temporal_trends=temporal_analysis['trends'],
                controversy_score=controversy_score,
                reliability_score=reliability_score
            )
            
            # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            await self._cache_analysis_result(topic, opinion_trend)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª
            recommendations = self._generate_recommendations(opinion_trend)
            
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ØªÙØµÙŠÙ„ÙŠØ©
            detailed_stats = self._generate_detailed_statistics(
                opinions, sentiment_analysis, emotion_analysis
            )
            
            return {
                'success': True,
                'data': {
                    'opinion_trend': opinion_trend.to_dict(),
                    'sentiment_analysis': sentiment_analysis,
                    'polarization_analysis': polarization_analysis,
                    'emotion_analysis': emotion_analysis,
                    'theme_analysis': theme_analysis,
                    'influencer_analysis': influencer_analysis,
                    'geographic_analysis': geographic_analysis,
                    'temporal_analysis': temporal_analysis,
                    'recommendations': recommendations,
                    'detailed_statistics': detailed_stats,
                    'metadata': {
                        'total_opinions': len(opinions),
                        'sources_used': sources,
                        'time_range': time_range,
                        'analysis_timestamp': datetime.now().isoformat(),
                        'sentiment_threshold': sentiment_threshold
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ø¹Ø§Ù…: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'message': 'ÙØ´Ù„ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ø¹Ø§Ù…'
            }
    
    def _analyze_sentiment_distribution(self, opinions: List[OpinionData]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±"""
        positive_count = sum(1 for op in opinions if op.sentiment_label == 'positive')
        negative_count = sum(1 for op in opinions if op.sentiment_label == 'negative')
        neutral_count = sum(1 for op in opinions if op.sentiment_label == 'neutral')
        
        total = len(opinions)
        
        distribution = {
            'positive': positive_count / total,
            'negative': negative_count / total,
            'neutral': neutral_count / total
        }
        
        # Ø­Ø³Ø§Ø¨ Ù‚ÙˆØ© Ø§Ù„Ø±Ø£ÙŠ (Ù…Ø¯Ù‰ ÙˆØ¶ÙˆØ­ Ø§Ù„Ø§ØªØ¬Ø§Ù‡)
        max_sentiment = max(distribution.values())
        strength = max_sentiment - (1/3)  # Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ù„Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ØªØ³Ø§ÙˆÙŠ
        
        # Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø±Ø¬Ø­ Ù„Ù„Ù…Ø´Ø§Ø¹Ø±
        weighted_scores = []
        for opinion in opinions:
            weight = opinion.influence_score * opinion.topic_relevance
            weighted_scores.append(opinion.sentiment_score * weight)
        
        weighted_average = np.mean(weighted_scores) if weighted_scores else 0.0
        
        return {
            'distribution': distribution,
            'counts': {
                'positive': positive_count,
                'negative': negative_count,
                'neutral': neutral_count,
                'total': total
            },
            'strength': strength,
            'weighted_average': weighted_average,
            'standard_deviation': np.std([op.sentiment_score for op in opinions])
        }
    
    def _analyze_polarization(self, opinions: List[OpinionData]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªÙ‚Ø·Ø§Ø¨"""
        sentiment_scores = [op.sentiment_score for op in opinions]
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªÙ‚Ø·Ø§Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ²ÙŠØ¹
        # Ø§Ù„Ø§Ø³ØªÙ‚Ø·Ø§Ø¨ Ø¹Ø§Ù„ÙŠ Ø¹Ù†Ø¯Ù…Ø§ ØªØªØ¬Ù…Ø¹ Ø§Ù„Ø¢Ø±Ø§Ø¡ ÙÙŠ Ø§Ù„Ø£Ø·Ø±Ø§Ù
        extreme_positive = sum(1 for score in sentiment_scores if score > 0.5)
        extreme_negative = sum(1 for score in sentiment_scores if score < -0.5)
        moderate = len(sentiment_scores) - extreme_positive - extreme_negative
        
        total = len(sentiment_scores)
        extreme_ratio = (extreme_positive + extreme_negative) / total
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø± Ø§Ù„Ø§Ø³ØªÙ‚Ø·Ø§Ø¨
        # ÙŠØªØ±Ø§ÙˆØ­ Ù…Ù† 0 (Ø¥Ø¬Ù…Ø§Ø¹ ØªØ§Ù…) Ø¥Ù„Ù‰ 1 (Ø§Ø³ØªÙ‚Ø·Ø§Ø¨ ØªØ§Ù…)
        if total > 1:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ ÙƒÙ…Ø¤Ø´Ø±
            polarization_index = np.std(sentiment_scores) / 1.0  # ØªØ·Ø¨ÙŠØ¹ Ø¹Ù„Ù‰ Ø£Ù‚ØµÙ‰ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ù…ÙƒÙ†
        else:
            polarization_index = 0.0
        
        # ØªØµÙ†ÙŠÙ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªÙ‚Ø·Ø§Ø¨
        if polarization_index < 0.3:
            polarization_level = 'low'
            polarization_desc = 'Ø¥Ø¬Ù…Ø§Ø¹ Ù†Ø³Ø¨ÙŠ'
        elif polarization_index < 0.6:
            polarization_level = 'moderate'
            polarization_desc = 'Ø§Ø®ØªÙ„Ø§Ù Ù…Ø¹ØªØ¯Ù„'
        else:
            polarization_level = 'high'
            polarization_desc = 'Ø§Ø³ØªÙ‚Ø·Ø§Ø¨ ÙˆØ§Ø¶Ø­'
        
        return {
            'level': polarization_index,
            'level_category': polarization_level,
            'description': polarization_desc,
            'extreme_opinions_ratio': extreme_ratio,
            'distribution': {
                'extreme_positive': extreme_positive,
                'extreme_negative': extreme_negative,
                'moderate': moderate
            }
        }
    
    def _analyze_emotions(self, opinions: List[OpinionData]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹ÙˆØ§Ø·Ù ÙÙŠ Ø§Ù„Ø¢Ø±Ø§Ø¡"""
        emotion_totals = defaultdict(list)
        
        for opinion in opinions:
            for emotion, score in opinion.emotion_scores.items():
                emotion_totals[emotion].append(score)
        
        # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· ÙƒÙ„ Ø¹Ø§Ø·ÙØ©
        emotion_averages = {
            emotion: np.mean(scores) 
            for emotion, scores in emotion_totals.items()
        }
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¹ÙˆØ§Ø·Ù Ø­Ø³Ø¨ Ø§Ù„Ø´Ø¯Ø©
        dominant_emotions = sorted(
            emotion_averages.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙ†ÙˆØ¹ Ø§Ù„Ø¹Ø§Ø·ÙÙŠ
        emotion_values = list(emotion_averages.values())
        emotional_diversity = 1 - np.std(emotion_values) if emotion_values else 0
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø¹Ø§Ø·ÙÙŠ Ø§Ù„Ø³Ø§Ø¦Ø¯
        top_emotion = dominant_emotions[0] if dominant_emotions else ('neutral', 0.0)
        
        if top_emotion[1] > 0.6:
            emotional_pattern = f'Ù‡ÙŠÙ…Ù†Ø© Ø¹Ø§Ø·ÙØ© {top_emotion[0]}'
        elif len([e for e in emotion_averages.values() if e > 0.4]) > 2:
            emotional_pattern = 'ØªÙ†ÙˆØ¹ Ø¹Ø§Ø·ÙÙŠ'
        else:
            emotional_pattern = 'Ø­ÙŠØ§Ø¯ÙŠØ© Ø¹Ø§Ø·ÙÙŠØ©'
        
        return {
            'averages': emotion_averages,
            'dominant': dominant_emotions[:3],  # Ø£Ù‚ÙˆÙ‰ 3 Ø¹ÙˆØ§Ø·Ù
            'diversity': emotional_diversity,
            'pattern': emotional_pattern,
            'intensity_distribution': {
                'high_intensity': len([op for op in opinions if max(op.emotion_scores.values()) > 0.7]),
                'medium_intensity': len([op for op in opinions if 0.4 < max(op.emotion_scores.values()) <= 0.7]),
                'low_intensity': len([op for op in opinions if max(op.emotion_scores.values()) <= 0.4])
            }
        }
    
    async def _extract_themes(self, opinions: List[OpinionData]) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ ÙˆØ§Ù„Ø«ÙŠÙ…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        try:
            texts = [op.text for op in opinions if len(op.text) > 20]
            
            if len(texts) < 5:
                return {'themes': [], 'topics': [], 'keywords': []}
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF
            vectorizer = TfidfVectorizer(
                max_features=100,
                stop_words=self._get_arabic_stop_words(),
                ngram_range=(1, 2),
                max_df=0.8,
                min_df=2
            )
            
            tfidf_matrix = vectorizer.fit_transform(texts)
            feature_names = vectorizer.get_feature_names_out()
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª
            scores = tfidf_matrix.sum(axis=0).A1
            word_scores = list(zip(feature_names, scores))
            word_scores.sort(key=lambda x: x[1], reverse=True)
            
            keywords = [word for word, score in word_scores[:20]]
            
            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ (Topic Modeling) Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙ†Ø§ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©
            themes = []
            topics = []
            
            if len(texts) >= 10:
                try:
                    # Ø§Ø³ØªØ®Ø¯Ø§Ù… LDA Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹
                    lda = LatentDirichletAllocation(
                        n_components=min(5, len(texts) // 3),
                        random_state=42,
                        max_iter=10
                    )
                    
                    lda.fit(tfidf_matrix)
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹
                    for topic_idx, topic in enumerate(lda.components_):
                        top_words_idx = topic.argsort()[-5:][::-1]
                        top_words = [feature_names[i] for i in top_words_idx]
                        topics.append({
                            'id': topic_idx,
                            'words': top_words,
                            'weight': float(topic.max())
                        })
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø«ÙŠÙ…Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
                    themes = self._identify_themes_from_keywords(keywords)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹: {str(e)}")
            
            return {
                'themes': themes,
                'topics': topics,
                'keywords': keywords[:10],
                'total_texts_analyzed': len(texts)
            }
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø«ÙŠÙ…Ø§Øª: {str(e)}")
            return {'themes': [], 'topics': [], 'keywords': []}
    
    def _get_arabic_stop_words(self) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
        return [
            'ÙÙŠ', 'Ù…Ù†', 'Ø¥Ù„Ù‰', 'Ø¹Ù„Ù‰', 'Ø¹Ù†', 'Ù…Ø¹', 'Ù‡Ø°Ø§', 'Ù‡Ø°Ù‡', 'Ø°Ù„Ùƒ', 'ØªÙ„Ùƒ',
            'Ø§Ù„ØªÙŠ', 'Ø§Ù„Ø°ÙŠ', 'Ø§Ù„Ù„Ø°Ø§Ù†', 'Ø§Ù„Ù„ØªØ§Ù†', 'Ø§Ù„Ù„Ø°ÙŠÙ†', 'Ø§Ù„Ù„ØªÙŠÙ†', 'Ø§Ù„Ù„Ø§Ø¦ÙŠ', 'Ø§Ù„Ù„ÙˆØ§ØªÙŠ',
            'ÙƒØ§Ù†', 'ÙƒØ§Ù†Øª', 'ÙŠÙƒÙˆÙ†', 'ØªÙƒÙˆÙ†', 'Ø£Ù†', 'Ø¥Ù†', 'Ù„ÙƒÙ†', 'Ù„ÙƒÙ†', 'ØºÙŠØ±', 'Ø³ÙˆÙ‰',
            'Ù„ÙŠØ³', 'Ù„ÙŠØ³Øª', 'Ù…Ø§', 'Ù„Ø§', 'Ù„Ù…', 'Ù„Ù†', 'Ù‚Ø¯', 'ÙÙ‚Ø¯', 'ÙƒÙ„', 'Ø¨Ø¹Ø¶',
            'Ø¬Ù…ÙŠØ¹', 'ÙƒØ§ÙØ©', 'Ù…Ø¹Ø¸Ù…', 'Ø£ÙƒØ«Ø±', 'Ø£Ù‚Ù„', 'Ø£ÙˆÙ„', 'Ø¢Ø®Ø±', 'Ù†ÙØ³', 'Ø°Ø§Øª'
        ]
    
    def _identify_themes_from_keywords(self, keywords: List[str]) -> List[str]:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø«ÙŠÙ…Ø§Øª Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        themes = []
        
        # Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ø«ÙŠÙ…Ø§Øª Ù…Ø®ØªÙ„ÙØ©
        theme_patterns = {
            'Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ ÙˆØ§Ù„Ù…Ø§Ù„': ['Ø§Ù‚ØªØµØ§Ø¯', 'Ù…Ø§Ù„', 'Ø£Ø³Ø¹Ø§Ø±', 'ØªÙƒÙ„ÙØ©', 'Ø±Ø§ØªØ¨', 'Ø¯Ø®Ù„', 'Ø§Ø³ØªØ«Ù…Ø§Ø±'],
            'Ø§Ù„Ø³ÙŠØ§Ø³Ø© ÙˆØ§Ù„Ø­ÙƒÙ…': ['Ø³ÙŠØ§Ø³Ø©', 'Ø­ÙƒÙˆÙ…Ø©', 'ÙˆØ²ÙŠØ±', 'Ù‚Ø±Ø§Ø±', 'Ù‚Ø§Ù†ÙˆÙ†', 'Ø³ÙŠØ§Ø³ÙŠ'],
            'Ø§Ù„ØªØ¹Ù„ÙŠÙ… ÙˆØ§Ù„ØªØ±Ø¨ÙŠØ©': ['ØªØ¹Ù„ÙŠÙ…', 'Ù…Ø¯Ø±Ø³Ø©', 'Ø¬Ø§Ù…Ø¹Ø©', 'Ø·Ø§Ù„Ø¨', 'Ù…Ø¹Ù„Ù…', 'ØªØ±Ø¨ÙŠØ©'],
            'Ø§Ù„ØµØ­Ø© ÙˆØ§Ù„Ø·Ø¨': ['ØµØ­Ø©', 'Ø·Ø¨', 'Ù…Ø±Ø¶', 'Ø¹Ù„Ø§Ø¬', 'Ø¯ÙˆØ§Ø¡', 'Ù…Ø³ØªØ´ÙÙ‰'],
            'Ø§Ù„ØªÙ‚Ù†ÙŠØ© ÙˆØ§Ù„ØªØ·ÙˆÙŠØ±': ['ØªÙ‚Ù†ÙŠØ©', 'ØªØ·ÙˆÙŠØ±', 'ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§', 'Ø¨Ø±Ù†Ø§Ù…Ø¬', 'ØªØ·Ø¨ÙŠÙ‚'],
            'Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ ÙˆØ§Ù„Ø«Ù‚Ø§ÙØ©': ['Ù…Ø¬ØªÙ…Ø¹', 'Ø«Ù‚Ø§ÙØ©', 'ØªÙ‚Ù„ÙŠØ¯', 'Ø¹Ø§Ø¯Ø©', 'Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ'],
            'Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØ§Ù„Ø·Ø¨ÙŠØ¹Ø©': ['Ø¨ÙŠØ¦Ø©', 'Ø·Ø¨ÙŠØ¹Ø©', 'Ù…Ù†Ø§Ø®', 'ØªÙ„ÙˆØ«', 'Ù†Ø¸Ø§ÙØ©']
        }
        
        for theme, pattern_words in theme_patterns.items():
            matches = sum(1 for keyword in keywords if any(word in keyword for word in pattern_words))
            if matches >= 2:  # Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ ÙƒÙ„Ù…ØªØ§Ù† Ù…Ø·Ø§Ø¨Ù‚ØªØ§Ù†
                themes.append(theme)
        
        return themes
    
    def _analyze_influencers(self, opinions: List[OpinionData]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¤Ø«Ø±ÙŠÙ† ÙÙŠ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ø¹Ø§Ù…"""
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¢Ø±Ø§Ø¡ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        user_opinions = defaultdict(list)
        
        for opinion in opinions:
            if opinion.user_id:
                user_opinions[opinion.user_id].append(opinion)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¤Ø«Ø±ÙŠÙ†
        influencers = []
        
        for user_id, user_ops in user_opinions.items():
            if len(user_ops) >= 2:  # Ø§Ù„Ù…Ø¤Ø«Ø± Ù„Ù‡ Ø£ÙƒØ«Ø± Ù…Ù† Ø±Ø£ÙŠ ÙˆØ§Ø­Ø¯
                avg_influence = np.mean([op.influence_score for op in user_ops])
                avg_sentiment = np.mean([op.sentiment_score for op in user_ops])
                total_reach = sum(op.influence_score for op in user_ops)
                
                influencer_data = {
                    'user_id': user_id,
                    'opinion_count': len(user_ops),
                    'average_influence': avg_influence,
                    'average_sentiment': avg_sentiment,
                    'total_reach': total_reach,
                    'sentiment_consistency': 1 - np.std([op.sentiment_score for op in user_ops]),
                    'recent_opinions': [
                        {
                            'text': op.text[:100] + '...' if len(op.text) > 100 else op.text,
                            'sentiment': op.sentiment_label,
                            'timestamp': op.timestamp.isoformat()
                        }
                        for op in sorted(user_ops, key=lambda x: x.timestamp, reverse=True)[:3]
                    ]
                }
                
                influencers.append(influencer_data)
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø¤Ø«Ø±ÙŠÙ†
        influencers.sort(key=lambda x: x['total_reach'], reverse=True)
        
        return {
            'total_influencers': len(influencers),
            'top_influencers': influencers[:10],
            'influence_distribution': {
                'high_influence': len([inf for inf in influencers if inf['average_influence'] > 5.0]),
                'medium_influence': len([inf for inf in influencers if 2.0 < inf['average_influence'] <= 5.0]),
                'low_influence': len([inf for inf in influencers if inf['average_influence'] <= 2.0])
            }
        }
    
    def _analyze_geographic_distribution(self, opinions: List[OpinionData]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠ Ù„Ù„Ø¢Ø±Ø§Ø¡"""
        location_sentiments = defaultdict(list)
        
        for opinion in opinions:
            if opinion.location:
                location_sentiments[opinion.location].append(opinion.sentiment_score)
        
        geographic_analysis = {}
        
        for location, sentiments in location_sentiments.items():
            if len(sentiments) >= 3:  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªØ­Ù„ÙŠÙ„
                geographic_analysis[location] = {
                    'opinion_count': len(sentiments),
                    'average_sentiment': np.mean(sentiments),
                    'sentiment_std': np.std(sentiments),
                    'positive_ratio': len([s for s in sentiments if s > 0.1]) / len(sentiments),
                    'negative_ratio': len([s for s in sentiments if s < -0.1]) / len(sentiments)
                }
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø£ÙƒØ«Ø± Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© ÙˆØ³Ù„Ø¨ÙŠØ©
        if geographic_analysis:
            most_positive = max(geographic_analysis.items(), key=lambda x: x[1]['average_sentiment'])
            most_negative = min(geographic_analysis.items(), key=lambda x: x[1]['average_sentiment'])
            
            return {
                'regions': geographic_analysis,
                'most_positive_region': {
                    'name': most_positive[0],
                    'sentiment': most_positive[1]['average_sentiment']
                },
                'most_negative_region': {
                    'name': most_negative[0],
                    'sentiment': most_negative[1]['average_sentiment']
                },
                'total_regions': len(geographic_analysis)
            }
        
        return {'regions': {}, 'total_regions': 0}
    
    def _analyze_temporal_trends(self, opinions: List[OpinionData]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©"""
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¢Ø±Ø§Ø¡ Ø­Ø³Ø¨ Ø§Ù„ÙˆÙ‚Øª
        daily_sentiments = defaultdict(list)
        
        for opinion in opinions:
            date_key = opinion.timestamp.strftime('%Y-%m-%d')
            daily_sentiments[date_key].append(opinion.sentiment_score)
        
        # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù„ÙƒÙ„ ÙŠÙˆÙ…
        daily_averages = {
            date: np.mean(sentiments)
            for date, sentiments in daily_sentiments.items()
            if len(sentiments) >= 2
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
        if len(daily_averages) >= 3:
            dates = sorted(daily_averages.keys())
            values = [daily_averages[date] for date in dates]
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø®Ø·ÙŠ
            x = list(range(len(values)))
            slope, intercept, r_value, p_value, std_err = stats.linregress(x, values)
            
            trend_direction = 'stable'
            if abs(slope) > 0.01:
                trend_direction = 'improving' if slope > 0 else 'declining'
            
            # ÙƒØ´Ù Ù†Ù‚Ø§Ø· Ø§Ù„ØªØ­ÙˆÙ„
            turning_points = []
            for i in range(1, len(values) - 1):
                if (values[i] > values[i-1] and values[i] > values[i+1]) or \
                   (values[i] < values[i-1] and values[i] < values[i+1]):
                    turning_points.append({
                        'date': dates[i],
                        'sentiment': values[i],
                        'type': 'peak' if values[i] > values[i-1] else 'valley'
                    })
            
            return {
                'trends': daily_averages,
                'trend_direction': trend_direction,
                'trend_strength': abs(r_value),
                'slope': slope,
                'turning_points': turning_points,
                'volatility': np.std(values),
                'total_days': len(daily_averages)
            }
        
        return {'trends': daily_averages, 'trend_direction': 'insufficient_data'}
    
    def _calculate_controversy_score(self, sentiment_analysis: Dict[str, Any],
                                   polarization_analysis: Dict[str, Any]) -> float:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬Ø¯Ù„"""
        # Ø§Ù„Ø¬Ø¯Ù„ Ø¹Ø§Ù„ÙŠ Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ Ø§Ø³ØªÙ‚Ø·Ø§Ø¨ ÙˆÙ…Ø´Ø§Ø¹Ø± Ù‚ÙˆÙŠØ©
        polarization = polarization_analysis['level']
        opinion_strength = sentiment_analysis['strength']
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Ù‚Ø§Ø·
        controversy = (polarization * 0.7) + (opinion_strength * 0.3)
        return min(1.0, controversy)
    
    def _calculate_reliability_score(self, opinions: List[OpinionData], 
                                   sources: List[str]) -> float:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©"""
        # Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø± ÙˆØªÙ†ÙˆØ¹Ù‡Ø§
        source_weights = [self.source_weights.get(source, 0.5) for source in sources]
        source_reliability = np.mean(source_weights) if source_weights else 0.5
        
        # ØªÙ†ÙˆØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±
        source_diversity = len(set(sources)) / len(self.supported_sources)
        
        # Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        data_size_factor = min(1.0, len(opinions) / 100)  # ØªØ·Ø¨ÙŠØ¹ Ø¹Ù„Ù‰ 100 Ø±Ø£ÙŠ
        
        # Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        avg_confidence = np.mean([op.confidence for op in opinions])
        
        # Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        reliability = (
            source_reliability * 0.3 +
            source_diversity * 0.2 +
            data_size_factor * 0.2 +
            avg_confidence * 0.3
        )
        
        return min(1.0, reliability)
    
    def _determine_overall_sentiment(self, sentiment_analysis: Dict[str, Any],
                                   threshold: float) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ"""
        distribution = sentiment_analysis['distribution']
        
        if distribution['positive'] > threshold:
            return 'positive'
        elif distribution['negative'] > threshold:
            return 'negative'
        elif abs(distribution['positive'] - distribution['negative']) < 0.1:
            return 'neutral'
        elif distribution['positive'] > distribution['negative']:
            return 'lean_positive'
        else:
            return 'lean_negative'
    
    async def _cache_analysis_result(self, topic: str, opinion_trend: OpinionTrend):
        """ØªØ®Ø²ÙŠÙ† Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        try:
            if not self.redis_client:
                return
            
            cache_key = f"{self.keys['analysis_cache']}:{topic}"
            cache_data = {
                'opinion_trend': opinion_trend.to_dict(),
                'cached_at': datetime.now().isoformat()
            }
            
            await self.redis_client.setex(
                cache_key,
                3600 * 6,  # ØµØ§Ù„Ø­ Ù„Ù€ 6 Ø³Ø§Ø¹Ø§Øª
                json.dumps(cache_data, default=str)
            )
            
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù…Ø¤Ù‚ØªØ§Ù‹: {str(e)}")
    
    def _generate_recommendations(self, opinion_trend: OpinionTrend) -> List[Dict[str, str]]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ø¹Ø§Ù…"""
        recommendations = []
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø¹Ø§Ù…Ø©
        if opinion_trend.overall_sentiment == 'negative':
            recommendations.append({
                'type': 'sentiment_improvement',
                'priority': 'high',
                'title': 'ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø¹Ø§Ù…Ø©',
                'description': 'Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ø¹Ø§Ù… Ø³Ù„Ø¨ÙŠ Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹',
                'action': 'Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø³ÙŠØ§Ø³Ø§Øª Ø£Ùˆ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹'
            })
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªÙ‚Ø·Ø§Ø¨
        if opinion_trend.polarization_level > 0.7:
            recommendations.append({
                'type': 'polarization_management',
                'priority': 'high',
                'title': 'Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø§Ø³ØªÙ‚Ø·Ø§Ø¨',
                'description': 'Ù…Ø³ØªÙˆÙ‰ Ø¹Ø§Ù„ÙŠ Ù…Ù† Ø§Ù„Ø§Ø³ØªÙ‚Ø·Ø§Ø¨ ÙÙŠ Ø§Ù„Ø¢Ø±Ø§Ø¡',
                'action': 'ØªÙ†Ø¸ÙŠÙ… Ø­ÙˆØ§Ø±Ø§Øª Ù…Ø¬ØªÙ…Ø¹ÙŠØ© Ù„ØªÙ‚Ø±ÙŠØ¨ ÙˆØ¬Ù‡Ø§Øª Ø§Ù„Ù†Ø¸Ø±'
            })
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø¯Ù„
        if opinion_trend.controversy_score > 0.8:
            recommendations.append({
                'type': 'controversy_handling',
                'priority': 'medium',
                'title': 'Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¬Ø¯Ù„',
                'description': 'Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø«ÙŠØ± Ù„Ù„Ø¬Ø¯Ù„',
                'action': 'ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØªØ¹Ø²ÙŠØ² Ø§Ù„Ø´ÙØ§ÙÙŠØ©'
            })
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©
        if opinion_trend.reliability_score < 0.6:
            recommendations.append({
                'type': 'data_quality',
                'priority': 'medium',
                'title': 'ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª',
                'description': 'Ù…ØµØ¯Ø§Ù‚ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ­Ø³ÙŠÙ†',
                'action': 'ØªÙˆØ³ÙŠØ¹ Ù…ØµØ§Ø¯Ø± Ø¬Ù…Ø¹ Ø§Ù„Ø¢Ø±Ø§Ø¡ ÙˆØªØ­Ø³ÙŠÙ† Ø·Ø±Ù‚ Ø§Ù„ØªØ­Ù„ÙŠÙ„'
            })
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¹ÙˆØ§Ø·Ù Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø©
        if opinion_trend.dominant_emotions:
            top_emotion = opinion_trend.dominant_emotions[0]
            if top_emotion[0] in ['anger', 'fear', 'sadness'] and top_emotion[1] > 0.6:
                recommendations.append({
                    'type': 'emotion_management',
                    'priority': 'high',
                    'title': f'Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¹Ø§Ø·ÙØ© {top_emotion[0]}',
                    'description': f'Ø§Ù„Ø¹Ø§Ø·ÙØ© Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø© Ù‡ÙŠ {top_emotion[0]}',
                    'action': 'ØªØ·ÙˆÙŠØ± Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ø§Ø·ÙØ©'
                })
        
        return recommendations
    
    def _generate_detailed_statistics(self, opinions: List[OpinionData],
                                    sentiment_analysis: Dict[str, Any],
                                    emotion_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ØªÙØµÙŠÙ„ÙŠØ©"""
        return {
            'total_opinions': len(opinions),
            'unique_users': len(set(op.user_id for op in opinions if op.user_id)),
            'source_breakdown': dict(Counter(op.source for op in opinions)),
            'confidence_stats': {
                'average': np.mean([op.confidence for op in opinions]),
                'min': np.min([op.confidence for op in opinions]),
                'max': np.max([op.confidence for op in opinions]),
                'std': np.std([op.confidence for op in opinions])
            },
            'influence_stats': {
                'average': np.mean([op.influence_score for op in opinions]),
                'total': np.sum([op.influence_score for op in opinions]),
                'high_influence_count': len([op for op in opinions if op.influence_score > 5.0])
            },
            'temporal_spread': {
                'earliest': min(op.timestamp for op in opinions).isoformat(),
                'latest': max(op.timestamp for op in opinions).isoformat(),
                'span_hours': (max(op.timestamp for op in opinions) - 
                             min(op.timestamp for op in opinions)).total_seconds() / 3600
            },
            'text_length_stats': {
                'average': np.mean([len(op.text) for op in opinions]),
                'min': np.min([len(op.text) for op in opinions]),
                'max': np.max([len(op.text) for op in opinions])
            }
        }

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    import asyncio
    
    async def test_public_opinion():
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ù„Ù„ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ø¹Ø§Ù…
        analyzer = PublicOpinionAnalyzer()
        
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¢Ø±Ø§Ø¡ Ù…ØªÙ†ÙˆØ¹Ø©
        sample_opinions = []
        topics = ['Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ', 'Ø§Ù„ØµØ­Ø© Ø§Ù„Ø¹Ø§Ù…Ø©', 'Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ Ø§Ù„Ø±Ù‚Ù…ÙŠ']
        
        for i in range(50):
            # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: 40% Ø¥ÙŠØ¬Ø§Ø¨ÙŠØŒ 30% Ø³Ù„Ø¨ÙŠØŒ 30% Ù…Ø­Ø§ÙŠØ¯
            if i % 10 < 4:
                sentiment_score = np.random.uniform(0.2, 0.9)
                sentiment_label = 'positive'
            elif i % 10 < 7:
                sentiment_score = np.random.uniform(-0.9, -0.2)
                sentiment_label = 'negative'
            else:
                sentiment_score = np.random.uniform(-0.2, 0.2)
                sentiment_label = 'neutral'
            
            opinion = OpinionData(
                text=f"Ø±Ø£ÙŠ ØªØ¬Ø±ÙŠØ¨ÙŠ Ø±Ù‚Ù… {i} Ø­ÙˆÙ„ {np.random.choice(topics)}. Ù‡Ø°Ø§ Ù†Øµ Ù…Ø­Ø§ÙƒØ§Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„.",
                timestamp=datetime.now() - timedelta(hours=np.random.randint(1, 168)),
                source=np.random.choice(['twitter', 'facebook', 'surveys', 'news_comments']),
                sentiment_score=sentiment_score,
                sentiment_label=sentiment_label,
                emotion_scores={
                    'joy': max(0, sentiment_score + np.random.normal(0, 0.1)),
                    'sadness': max(0, -sentiment_score + np.random.normal(0, 0.1)),
                    'anger': max(0, np.random.normal(0, 0.1)),
                    'fear': max(0, np.random.normal(0, 0.1)),
                    'trust': max(0, sentiment_score * 0.8 + np.random.normal(0, 0.05))
                },
                confidence=np.random.uniform(0.6, 0.95),
                user_id=f"user_{np.random.randint(1, 20)}",
                location=np.random.choice(['Ø§Ù„Ø±ÙŠØ§Ø¶', 'Ø¬Ø¯Ø©', 'Ø§Ù„Ø¯Ù…Ø§Ù…', 'Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©']),
                influence_score=np.random.uniform(1.0, 10.0)
            )
            
            sample_opinions.append(opinion)
        
        # ØªØ´ØºÙŠÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ø¹Ø§Ù…
        result = await analyzer.analyze_opinion(
            topic='Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ',
            sources=['twitter', 'facebook', 'surveys'],
            time_range='7d',
            sentiment_threshold=0.6
        )
        
        if result['success']:
            opinion_data = result['data']['opinion_trend']
            print(f"ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ø¹Ø§Ù… Ø­ÙˆÙ„: {opinion_data['topic']}")
            print(f"ğŸ¯ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ø¹Ø§Ù…: {opinion_data['overall_sentiment']}")
            print(f"ğŸ’ª Ù‚ÙˆØ© Ø§Ù„Ø±Ø£ÙŠ: {opinion_data['opinion_strength']:.2f}")
            print(f"âš–ï¸ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªÙ‚Ø·Ø§Ø¨: {opinion_data['polarization_level']:.2f}")
            print(f"ğŸ”¥ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬Ø¯Ù„: {opinion_data['controversy_score']:.2f}")
            print(f"âœ… Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©: {opinion_data['reliability_score']:.2f}")
            
            print(f"\nğŸ“ˆ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±:")
            for sentiment, ratio in opinion_data['sentiment_distribution'].items():
                print(f"  {sentiment}: {ratio:.2%}")
            
            print(f"\nğŸ­ Ø§Ù„Ø¹ÙˆØ§Ø·Ù Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø©:")
            for emotion, intensity in opinion_data['dominant_emotions'][:3]:
                print(f"  {emotion}: {intensity:.2f}")
            
            print(f"\nğŸ·ï¸ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:")
            themes = result['data']['theme_analysis']['themes']
            for theme in themes[:5]:
                print(f"  - {theme}")
            
            recommendations = result['data']['recommendations']
            if recommendations:
                print(f"\nğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª:")
                for rec in recommendations[:3]:
                    print(f"  - {rec['title']}: {rec['description']}")
            
        else:
            print(f"âŒ ÙØ´Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {result.get('message', 'Ø®Ø·Ø£ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}")
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    asyncio.run(test_public_opinion())
