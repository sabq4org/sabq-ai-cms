# Ø®Ø¯Ù…Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ©
# Emotional Trends Analysis Service

import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import numpy as np
import pandas as pd
from scipy import stats
from scipy.signal import find_peaks
import redis.asyncio as redis
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import plotly.graph_objects as go
import plotly.express as px
from plotly.utils import PlotlyJSONEncoder

logger = logging.getLogger(__name__)

@dataclass
class TrendPoint:
    """Ù†Ù‚Ø·Ø© ÙÙŠ Ø§Ù„Ø§ØªØ¬Ø§Ù‡"""
    timestamp: datetime
    sentiment_score: float
    emotion_scores: Dict[str, float]
    volume: int
    confidence: float
    category: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'timestamp': self.timestamp.isoformat(),
            'sentiment_score': self.sentiment_score,
            'emotion_scores': self.emotion_scores,
            'volume': self.volume,
            'confidence': self.confidence,
            'category': self.category
        }

@dataclass
class TrendAnalysis:
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡"""
    trend_direction: str  # 'increasing', 'decreasing', 'stable', 'volatile'
    trend_strength: float  # 0-1
    trend_duration: timedelta
    peak_points: List[TrendPoint]
    valley_points: List[TrendPoint]
    anomalies: List[TrendPoint]
    correlation_factors: Dict[str, float]
    predictions: Dict[str, Any]
    confidence_interval: Tuple[float, float]
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'trend_direction': self.trend_direction,
            'trend_strength': self.trend_strength,
            'trend_duration': str(self.trend_duration),
            'peak_points': [p.to_dict() for p in self.peak_points],
            'valley_points': [v.to_dict() for v in self.valley_points],
            'anomalies': [a.to_dict() for a in self.anomalies],
            'correlation_factors': self.correlation_factors,
            'predictions': self.predictions,
            'confidence_interval': self.confidence_interval
        }

class TrendAnalysisService:
    """Ø®Ø¯Ù…Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
    
    def __init__(self, redis_client: redis.Redis = None):
        self.redis_client = redis_client
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„
        self.min_data_points = 10
        self.smoothing_window = 5
        self.anomaly_threshold = 2.0  # Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ù†Ø­Ø±Ø§ÙØ§Øª Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©
        self.trend_strength_threshold = 0.3
        self.peak_prominence = 0.1
        
        # Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¹ÙˆØ§Ø·Ù Ø§Ù„Ù…ØªØªØ¨Ø¹Ø©
        self.emotions = [
            'joy', 'sadness', 'anger', 'fear', 
            'surprise', 'disgust', 'trust', 'anticipation'
        ]
        
        # ÙØ¦Ø§Øª Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        self.content_categories = [
            'news', 'sports', 'politics', 'technology', 
            'entertainment', 'health', 'education', 'business'
        ]
        
        # Ù…ÙØ§ØªÙŠØ­ Redis
        self.keys = {
            'trend_data': 'trends:data',
            'trend_cache': 'trends:cache',
            'anomalies': 'trends:anomalies',
            'predictions': 'trends:predictions',
            'correlations': 'trends:correlations'
        }
    
    async def record_trend_point(self, analysis_result: Dict[str, Any], 
                                category: Optional[str] = None) -> bool:
        """ØªØ³Ø¬ÙŠÙ„ Ù†Ù‚Ø·Ø© Ø§ØªØ¬Ø§Ù‡ Ø¬Ø¯ÙŠØ¯Ø©"""
        try:
            if not self.redis_client:
                return False
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„
            sentiment_data = analysis_result.get('sentiment_analysis', {})
            emotion_data = analysis_result.get('emotion_analysis', {})
            
            # Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
            sentiment_score = self._calculate_sentiment_score(sentiment_data)
            emotion_scores = self._extract_emotion_scores(emotion_data)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù‚Ø·Ø© Ø§Ù„Ø§ØªØ¬Ø§Ù‡
            trend_point = TrendPoint(
                timestamp=datetime.now(),
                sentiment_score=sentiment_score,
                emotion_scores=emotion_scores,
                volume=1,  # ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ†Ù‡ Ù„ÙŠÙ…Ø«Ù„ Ø­Ø¬Ù… Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ
                confidence=sentiment_data.get('confidence', 0.0),
                category=category
            )
            
            # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù‚Ø·Ø©
            await self._store_trend_point(trend_point)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ
            await self._update_real_time_analysis(trend_point)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ³Ø¬ÙŠÙ„ Ù†Ù‚Ø·Ø© Ø§Ù„Ø§ØªØ¬Ø§Ù‡: {str(e)}")
            return False
    
    def _calculate_sentiment_score(self, sentiment_data: Dict[str, Any]) -> float:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ø´Ø§Ø¹Ø±"""
        probs = sentiment_data.get('probabilities', {})
        positive = probs.get('positive', 0.0)
        negative = probs.get('negative', 0.0)
        neutral = probs.get('neutral', 0.0)
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù†Ù‚Ø§Ø· Ù…Ù† -1 Ø¥Ù„Ù‰ +1
        return positive - negative
    
    def _extract_emotion_scores(self, emotion_data: Dict[str, Any]) -> Dict[str, float]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¹ÙˆØ§Ø·Ù"""
        emotions = emotion_data.get('emotions', {})
        return {
            emotion: data.get('probability', 0.0)
            for emotion, data in emotions.items()
            if emotion in self.emotions
        }
    
    async def _store_trend_point(self, trend_point: TrendPoint):
        """ØªØ®Ø²ÙŠÙ† Ù†Ù‚Ø·Ø© Ø§Ù„Ø§ØªØ¬Ø§Ù‡ ÙÙŠ Redis"""
        # ØªØ®Ø²ÙŠÙ† ÙÙŠ Ù‚Ø§Ø¦Ù…Ø© Ø²Ù…Ù†ÙŠØ© Ù…Ø±ØªØ¨Ø©
        timestamp_key = int(trend_point.timestamp.timestamp())
        
        # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        await self.redis_client.zadd(
            f"{self.keys['trend_data']}:sentiment",
            {json.dumps(trend_point.to_dict()): timestamp_key}
        )
        
        # ØªØ®Ø²ÙŠÙ† Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø© Ø¥Ø°Ø§ ØªÙˆÙØ±Øª
        if trend_point.category:
            await self.redis_client.zadd(
                f"{self.keys['trend_data']}:category:{trend_point.category}",
                {json.dumps(trend_point.to_dict()): timestamp_key}
            )
        
        # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¹ÙˆØ§Ø·Ù Ù…Ù†ÙØµÙ„Ø©
        for emotion, score in trend_point.emotion_scores.items():
            await self.redis_client.zadd(
                f"{self.keys['trend_data']}:emotion:{emotion}",
                {score: timestamp_key}
            )
        
        # Ø­Ø°Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© (Ø£ÙƒØ«Ø± Ù…Ù† 90 ÙŠÙˆÙ…)
        cutoff = int((datetime.now() - timedelta(days=90)).timestamp())
        await self.redis_client.zremrangebyscore(
            f"{self.keys['trend_data']}:sentiment", 0, cutoff
        )
    
    async def _update_real_time_analysis(self, trend_point: TrendPoint):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ"""
        # ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°
        if await self._is_anomaly(trend_point):
            await self._record_anomaly(trend_point)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
        await self._update_predictions(trend_point)
    
    async def _is_anomaly(self, trend_point: TrendPoint) -> bool:
        """ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° ÙÙŠ Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©"""
        try:
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø©
            recent_data = await self._get_recent_trend_data(hours=24)
            
            if len(recent_data) < self.min_data_points:
                return False
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            sentiment_scores = [p.sentiment_score for p in recent_data]
            mean_sentiment = np.mean(sentiment_scores)
            std_sentiment = np.std(sentiment_scores)
            
            # ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ
            if std_sentiment > 0:
                z_score = abs(trend_point.sentiment_score - mean_sentiment) / std_sentiment
                return z_score > self.anomaly_threshold
            
            return False
            
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°: {str(e)}")
            return False
    
    async def _record_anomaly(self, trend_point: TrendPoint):
        """ØªØ³Ø¬ÙŠÙ„ Ù†Ù‚Ø·Ø© Ø´Ø§Ø°Ø©"""
        anomaly_data = {
            'timestamp': trend_point.timestamp.isoformat(),
            'sentiment_score': trend_point.sentiment_score,
            'category': trend_point.category,
            'confidence': trend_point.confidence,
            'type': 'sentiment_spike'
        }
        
        await self.redis_client.lpush(
            self.keys['anomalies'],
            json.dumps(anomaly_data)
        )
        
        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø£Ø­Ø¯Ø« 1000 Ø´Ø°ÙˆØ°
        await self.redis_client.ltrim(self.keys['anomalies'], 0, 999)
    
    async def _update_predictions(self, trend_point: TrendPoint):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª"""
        try:
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø©
            recent_data = await self._get_recent_trend_data(hours=72)
            
            if len(recent_data) >= 20:  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªÙ†Ø¨Ø¤
                predictions = await self._generate_predictions(recent_data)
                
                await self.redis_client.setex(
                    self.keys['predictions'],
                    3600,  # ØµØ§Ù„Ø­ Ù„Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©
                    json.dumps(predictions, default=str)
                )
        
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª: {str(e)}")
    
    async def _get_recent_trend_data(self, hours: int = 24) -> List[TrendPoint]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø£Ø®ÙŠØ±Ø©"""
        try:
            cutoff = int((datetime.now() - timedelta(hours=hours)).timestamp())
            current = int(datetime.now().timestamp())
            
            # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Redis
            raw_data = await self.redis_client.zrangebyscore(
                f"{self.keys['trend_data']}:sentiment",
                cutoff, current
            )
            
            trend_points = []
            for data_str in raw_data:
                try:
                    data = json.loads(data_str)
                    trend_point = TrendPoint(
                        timestamp=datetime.fromisoformat(data['timestamp']),
                        sentiment_score=data['sentiment_score'],
                        emotion_scores=data['emotion_scores'],
                        volume=data['volume'],
                        confidence=data['confidence'],
                        category=data.get('category')
                    )
                    trend_points.append(trend_point)
                except (json.JSONDecodeError, KeyError) as e:
                    logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ù†Ù‚Ø·Ø© Ø§ØªØ¬Ø§Ù‡: {str(e)}")
                    continue
            
            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ÙˆÙ‚Øª
            trend_points.sort(key=lambda x: x.timestamp)
            return trend_points
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§ØªØ¬Ø§Ù‡: {str(e)}")
            return []
    
    async def analyze_trends(self, category: Optional[str] = None,
                           time_range: str = "24h",
                           include_emotions: bool = True,
                           min_samples: int = 10) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ©"""
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø²Ù…Ù†ÙŠ Ø¥Ù„Ù‰ Ø³Ø§Ø¹Ø§Øª
            hours = self._parse_time_range(time_range)
            
            # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if category:
                trend_data = await self._get_category_trend_data(category, hours)
            else:
                trend_data = await self._get_recent_trend_data(hours)
            
            if len(trend_data) < min_samples:
                return {
                    'error': 'insufficient_data',
                    'message': f'Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„. Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {min_samples}, Ø§Ù„Ù…ØªÙˆÙØ±: {len(trend_data)}',
                    'available_data_points': len(trend_data)
                }
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
            trend_analysis = await self._analyze_sentiment_trend(trend_data)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹ÙˆØ§Ø·Ù Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨ Ø°Ù„Ùƒ
            emotion_analysis = {}
            if include_emotions:
                emotion_analysis = await self._analyze_emotion_trends(trend_data)
            
            # ÙƒØ´Ù Ø§Ù„Ø£Ù†Ù…Ø§Ø·
            patterns = await self._detect_patterns(trend_data)
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
            predictions = await self._generate_predictions(trend_data)
            
            # Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ø¨Ø§Ø±Ø²Ø©
            notable_events = await self._identify_notable_events(trend_data)
            
            # Ø§Ù„ØªØµÙˆØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©
            visualizations = await self._generate_trend_visualizations(
                trend_data, emotion_analysis, category
            )
            
            return {
                'success': True,
                'data': {
                    'trend_analysis': trend_analysis.to_dict(),
                    'emotion_analysis': emotion_analysis,
                    'patterns': patterns,
                    'predictions': predictions,
                    'notable_events': notable_events,
                    'visualizations': visualizations,
                    'metadata': {
                        'category': category,
                        'time_range': time_range,
                        'data_points': len(trend_data),
                        'analysis_timestamp': datetime.now().isoformat(),
                        'includes_emotions': include_emotions
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'message': 'ÙØ´Ù„ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª'
            }
    
    def _parse_time_range(self, time_range: str) -> int:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø²Ù…Ù†ÙŠ Ø¥Ù„Ù‰ Ø³Ø§Ø¹Ø§Øª"""
        if time_range.endswith('h'):
            return int(time_range[:-1])
        elif time_range.endswith('d'):
            return int(time_range[:-1]) * 24
        else:
            return 24  # Ø§ÙØªØ±Ø§Ø¶ÙŠ
    
    async def _get_category_trend_data(self, category: str, hours: int) -> List[TrendPoint]:
        """Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§ØªØ¬Ø§Ù‡ ÙØ¦Ø© Ù…Ø¹ÙŠÙ†Ø©"""
        try:
            cutoff = int((datetime.now() - timedelta(hours=hours)).timestamp())
            current = int(datetime.now().timestamp())
            
            raw_data = await self.redis_client.zrangebyscore(
                f"{self.keys['trend_data']}:category:{category}",
                cutoff, current
            )
            
            trend_points = []
            for data_str in raw_data:
                try:
                    data = json.loads(data_str)
                    trend_point = TrendPoint(
                        timestamp=datetime.fromisoformat(data['timestamp']),
                        sentiment_score=data['sentiment_score'],
                        emotion_scores=data['emotion_scores'],
                        volume=data['volume'],
                        confidence=data['confidence'],
                        category=data.get('category')
                    )
                    trend_points.append(trend_point)
                except (json.JSONDecodeError, KeyError):
                    continue
            
            trend_points.sort(key=lambda x: x.timestamp)
            return trend_points
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª ÙØ¦Ø© {category}: {str(e)}")
            return []
    
    async def _analyze_sentiment_trend(self, trend_data: List[TrendPoint]) -> TrendAnalysis:
        """ØªØ­Ù„ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±"""
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©
        timestamps = [p.timestamp for p in trend_data]
        sentiment_scores = [p.sentiment_score for p in trend_data]
        
        # ØªÙ†Ø¹ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if len(sentiment_scores) >= self.smoothing_window:
            smoothed_scores = self._smooth_data(sentiment_scores, self.smoothing_window)
        else:
            smoothed_scores = sentiment_scores
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
        if len(smoothed_scores) > 1:
            slope, intercept, r_value, p_value, std_err = stats.linregress(
                range(len(smoothed_scores)), smoothed_scores
            )
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„ØªØ±Ù†Ø¯
            if abs(slope) < 0.001:
                trend_direction = 'stable'
            elif slope > 0:
                trend_direction = 'increasing'
            else:
                trend_direction = 'decreasing'
            
            # Ù‚ÙˆØ© Ø§Ù„Ø§ØªØ¬Ø§Ù‡
            trend_strength = abs(r_value)
            
            # Ø§Ù„ØªÙ‚Ù„Ø¨Ø§Øª
            volatility = np.std(sentiment_scores)
            if volatility > 0.3:
                trend_direction = 'volatile'
        else:
            trend_direction = 'stable'
            trend_strength = 0.0
            slope = 0.0
        
        # ÙƒØ´Ù Ø§Ù„Ù‚Ù…Ù… ÙˆØ§Ù„ÙˆØ¯ÙŠØ§Ù†
        peaks = self._find_peaks(smoothed_scores)
        valleys = self._find_valleys(smoothed_scores)
        
        peak_points = [trend_data[i] for i in peaks]
        valley_points = [trend_data[i] for i in valleys]
        
        # ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°
        anomalies = self._detect_anomalies(trend_data)
        
        # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·
        correlation_factors = self._calculate_correlations(trend_data)
        
        # ÙØªØ±Ø© Ø§Ù„Ø§ØªØ¬Ø§Ù‡
        if len(timestamps) > 1:
            trend_duration = timestamps[-1] - timestamps[0]
        else:
            trend_duration = timedelta(0)
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
        predictions = await self._generate_predictions(trend_data)
        
        # ÙØªØ±Ø© Ø§Ù„Ø«Ù‚Ø©
        if len(sentiment_scores) > 2:
            confidence_interval = stats.t.interval(
                0.95, len(sentiment_scores)-1,
                loc=np.mean(sentiment_scores),
                scale=stats.sem(sentiment_scores)
            )
        else:
            confidence_interval = (0.0, 0.0)
        
        return TrendAnalysis(
            trend_direction=trend_direction,
            trend_strength=trend_strength,
            trend_duration=trend_duration,
            peak_points=peak_points,
            valley_points=valley_points,
            anomalies=anomalies,
            correlation_factors=correlation_factors,
            predictions=predictions,
            confidence_interval=confidence_interval
        )
    
    def _smooth_data(self, data: List[float], window_size: int) -> List[float]:
        """ØªÙ†Ø¹ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ØªØ­Ø±Ùƒ"""
        if len(data) < window_size:
            return data
        
        smoothed = []
        for i in range(len(data)):
            start = max(0, i - window_size // 2)
            end = min(len(data), i + window_size // 2 + 1)
            smoothed.append(np.mean(data[start:end]))
        
        return smoothed
    
    def _find_peaks(self, data: List[float]) -> List[int]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ù…Ù… ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        if len(data) < 3:
            return []
        
        peaks, _ = find_peaks(data, prominence=self.peak_prominence)
        return peaks.tolist()
    
    def _find_valleys(self, data: List[float]) -> List[int]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¯ÙŠØ§Ù† ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        if len(data) < 3:
            return []
        
        # Ù‚Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¯ÙŠØ§Ù†
        inverted_data = [-x for x in data]
        valleys, _ = find_peaks(inverted_data, prominence=self.peak_prominence)
        return valleys.tolist()
    
    def _detect_anomalies(self, trend_data: List[TrendPoint]) -> List[TrendPoint]:
        """ÙƒØ´Ù Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø´Ø§Ø°Ø©"""
        if len(trend_data) < self.min_data_points:
            return []
        
        sentiment_scores = [p.sentiment_score for p in trend_data]
        mean_score = np.mean(sentiment_scores)
        std_score = np.std(sentiment_scores)
        
        anomalies = []
        for point in trend_data:
            if std_score > 0:
                z_score = abs(point.sentiment_score - mean_score) / std_score
                if z_score > self.anomaly_threshold:
                    anomalies.append(point)
        
        return anomalies
    
    def _calculate_correlations(self, trend_data: List[TrendPoint]) -> Dict[str, float]:
        """Ø­Ø³Ø§Ø¨ Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·"""
        correlations = {}
        
        if len(trend_data) < 3:
            return correlations
        
        try:
            # Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÙŠÙ† Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ø«Ù‚Ø©
            sentiment_scores = [p.sentiment_score for p in trend_data]
            confidence_scores = [p.confidence for p in trend_data]
            
            if len(set(confidence_scores)) > 1:  # ØªØ¬Ù†Ø¨ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ù…Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø«Ø§Ø¨ØªØ©
                corr_sentiment_confidence = np.corrcoef(sentiment_scores, confidence_scores)[0, 1]
                if not np.isnan(corr_sentiment_confidence):
                    correlations['sentiment_confidence'] = corr_sentiment_confidence
            
            # Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÙŠÙ† Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ø­Ø¬Ù…
            volumes = [p.volume for p in trend_data]
            if len(set(volumes)) > 1:
                corr_sentiment_volume = np.corrcoef(sentiment_scores, volumes)[0, 1]
                if not np.isnan(corr_sentiment_volume):
                    correlations['sentiment_volume'] = corr_sentiment_volume
            
            # Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§Ù„ÙˆÙ‚Øª (Ø§ØªØ¬Ø§Ù‡ Ø²Ù…Ù†ÙŠ)
            time_indices = list(range(len(sentiment_scores)))
            corr_sentiment_time = np.corrcoef(sentiment_scores, time_indices)[0, 1]
            if not np.isnan(corr_sentiment_time):
                correlations['sentiment_time'] = corr_sentiment_time
            
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø¨Ø¹Ø¶ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª: {str(e)}")
        
        return correlations
    
    async def _analyze_emotion_trends(self, trend_data: List[TrendPoint]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø¹ÙˆØ§Ø·Ù"""
        emotion_trends = {}
        
        for emotion in self.emotions:
            emotion_scores = []
            timestamps = []
            
            for point in trend_data:
                if emotion in point.emotion_scores:
                    emotion_scores.append(point.emotion_scores[emotion])
                    timestamps.append(point.timestamp)
            
            if len(emotion_scores) >= 3:
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
                if len(emotion_scores) > 1:
                    slope, _, r_value, _, _ = stats.linregress(
                        range(len(emotion_scores)), emotion_scores
                    )
                    
                    direction = 'stable'
                    if abs(slope) > 0.001:
                        direction = 'increasing' if slope > 0 else 'decreasing'
                    
                    strength = abs(r_value)
                else:
                    direction = 'stable'
                    strength = 0.0
                
                emotion_trends[emotion] = {
                    'direction': direction,
                    'strength': strength,
                    'average_intensity': np.mean(emotion_scores),
                    'peak_intensity': np.max(emotion_scores),
                    'volatility': np.std(emotion_scores),
                    'data_points': len(emotion_scores)
                }
        
        return emotion_trends
    
    async def _detect_patterns(self, trend_data: List[TrendPoint]) -> Dict[str, Any]:
        """ÙƒØ´Ù Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        patterns = {}
        
        if len(trend_data) < self.min_data_points:
            return patterns
        
        try:
            # Ù†Ù…Ø· Ø§Ù„Ø¯ÙˆØ±ÙŠØ© (Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„ÙŠÙˆÙ…ÙŠ/Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ÙŠ)
            patterns['cyclical'] = self._detect_cyclical_patterns(trend_data)
            
            # Ù†Ù…Ø· Ø§Ù„ØªØ¬Ù…ÙŠØ¹ (clustering)
            patterns['clustering'] = self._detect_clustering_patterns(trend_data)
            
            # Ù†Ù…Ø· Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ©
            patterns['seasonality'] = self._detect_seasonal_patterns(trend_data)
            
            # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¹ÙˆØ§Ø·Ù
            patterns['emotion_patterns'] = self._detect_emotion_patterns(trend_data)
            
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ÙƒØ´Ù Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ù†Ù…Ø§Ø·: {str(e)}")
        
        return patterns
    
    def _detect_cyclical_patterns(self, trend_data: List[TrendPoint]) -> Dict[str, Any]:
        """ÙƒØ´Ù Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¯ÙˆØ±ÙŠØ©"""
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„ÙŠÙˆÙ…ÙŠ
        hourly_sentiment = defaultdict(list)
        
        for point in trend_data:
            hour = point.timestamp.hour
            hourly_sentiment[hour].append(point.sentiment_score)
        
        # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù„ÙƒÙ„ Ø³Ø§Ø¹Ø©
        hourly_averages = {
            hour: np.mean(scores) 
            for hour, scores in hourly_sentiment.items()
            if len(scores) >= 2
        }
        
        # ÙƒØ´Ù Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø°Ø±ÙˆØ© ÙˆØ§Ù„Ø§Ù†Ø®ÙØ§Ø¶
        if len(hourly_averages) >= 6:  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªØ­Ù„ÙŠÙ„
            peak_hour = max(hourly_averages.items(), key=lambda x: x[1])
            valley_hour = min(hourly_averages.items(), key=lambda x: x[1])
            
            return {
                'has_daily_cycle': True,
                'peak_hour': peak_hour[0],
                'peak_sentiment': peak_hour[1],
                'valley_hour': valley_hour[0],
                'valley_sentiment': valley_hour[1],
                'cycle_strength': peak_hour[1] - valley_hour[1],
                'hourly_distribution': hourly_averages
            }
        
        return {'has_daily_cycle': False}
    
    def _detect_clustering_patterns(self, trend_data: List[TrendPoint]) -> Dict[str, Any]:
        """ÙƒØ´Ù Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ¬Ù…ÙŠØ¹"""
        if len(trend_data) < 10:
            return {'has_clusters': False}
        
        try:
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¬Ù…ÙŠØ¹
            features = []
            for point in trend_data:
                feature_vector = [
                    point.sentiment_score,
                    point.confidence,
                    point.timestamp.hour,  # Ø³Ø§Ø¹Ø© Ø§Ù„ÙŠÙˆÙ…
                    point.timestamp.weekday()  # ÙŠÙˆÙ… Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹
                ]
                
                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¹ÙˆØ§Ø·Ù Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
                for emotion in ['joy', 'sadness', 'anger']:
                    feature_vector.append(point.emotion_scores.get(emotion, 0.0))
                
                features.append(feature_vector)
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¬Ù…ÙŠØ¹
            scaler = StandardScaler()
            scaled_features = scaler.fit_transform(features)
            
            clustering = DBSCAN(eps=0.5, min_samples=3)
            cluster_labels = clustering.fit_predict(scaled_features)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            unique_clusters = set(cluster_labels)
            if -1 in unique_clusters:  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø´Ø§Ø°Ø©
                unique_clusters.remove(-1)
            
            if len(unique_clusters) >= 2:
                cluster_analysis = {}
                for cluster_id in unique_clusters:
                    cluster_points = [
                        trend_data[i] for i, label in enumerate(cluster_labels)
                        if label == cluster_id
                    ]
                    
                    if cluster_points:
                        avg_sentiment = np.mean([p.sentiment_score for p in cluster_points])
                        avg_confidence = np.mean([p.confidence for p in cluster_points])
                        
                        cluster_analysis[f'cluster_{cluster_id}'] = {
                            'size': len(cluster_points),
                            'avg_sentiment': avg_sentiment,
                            'avg_confidence': avg_confidence,
                            'time_span': {
                                'start': min(p.timestamp for p in cluster_points).isoformat(),
                                'end': max(p.timestamp for p in cluster_points).isoformat()
                            }
                        }
                
                return {
                    'has_clusters': True,
                    'num_clusters': len(unique_clusters),
                    'cluster_analysis': cluster_analysis,
                    'outliers': sum(1 for label in cluster_labels if label == -1)
                }
            
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¬Ù…ÙŠØ¹: {str(e)}")
        
        return {'has_clusters': False}
    
    def _detect_seasonal_patterns(self, trend_data: List[TrendPoint]) -> Dict[str, Any]:
        """ÙƒØ´Ù Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ©"""
        # ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø¨ ÙŠÙˆÙ… Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹
        weekday_sentiment = defaultdict(list)
        
        for point in trend_data:
            weekday = point.timestamp.weekday()
            weekday_sentiment[weekday].append(point.sentiment_score)
        
        weekday_analysis = {}
        weekday_names = ['Ø§Ù„Ø§Ø«Ù†ÙŠÙ†', 'Ø§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡', 'Ø§Ù„Ø£Ø±Ø¨Ø¹Ø§Ø¡', 'Ø§Ù„Ø®Ù…ÙŠØ³', 'Ø§Ù„Ø¬Ù…Ø¹Ø©', 'Ø§Ù„Ø³Ø¨Øª', 'Ø§Ù„Ø£Ø­Ø¯']
        
        for weekday, scores in weekday_sentiment.items():
            if len(scores) >= 2:
                weekday_analysis[weekday_names[weekday]] = {
                    'average_sentiment': np.mean(scores),
                    'sentiment_std': np.std(scores),
                    'sample_size': len(scores)
                }
        
        # ØªØ­Ø¯ÙŠØ¯ Ø£ÙØ¶Ù„ ÙˆØ£Ø³ÙˆØ£ Ø£ÙŠØ§Ù… Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹
        if len(weekday_analysis) >= 3:
            best_day = max(weekday_analysis.items(), key=lambda x: x[1]['average_sentiment'])
            worst_day = min(weekday_analysis.items(), key=lambda x: x[1]['average_sentiment'])
            
            return {
                'has_weekly_pattern': True,
                'best_day': best_day[0],
                'best_day_sentiment': best_day[1]['average_sentiment'],
                'worst_day': worst_day[0],
                'worst_day_sentiment': worst_day[1]['average_sentiment'],
                'weekday_analysis': weekday_analysis
            }
        
        return {'has_weekly_pattern': False}
    
    def _detect_emotion_patterns(self, trend_data: List[TrendPoint]) -> Dict[str, Any]:
        """ÙƒØ´Ù Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¹ÙˆØ§Ø·Ù"""
        emotion_correlations = {}
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÙŠÙ† Ø§Ù„Ø¹ÙˆØ§Ø·Ù Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        for emotion1 in self.emotions:
            for emotion2 in self.emotions:
                if emotion1 != emotion2:
                    scores1 = [p.emotion_scores.get(emotion1, 0.0) for p in trend_data]
                    scores2 = [p.emotion_scores.get(emotion2, 0.0) for p in trend_data]
                    
                    if len(set(scores1)) > 1 and len(set(scores2)) > 1:
                        correlation = np.corrcoef(scores1, scores2)[0, 1]
                        if not np.isnan(correlation) and abs(correlation) > 0.3:
                            emotion_correlations[f'{emotion1}_{emotion2}'] = correlation
        
        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¹ÙˆØ§Ø·Ù Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø©
        emotion_dominance = {}
        for emotion in self.emotions:
            scores = [p.emotion_scores.get(emotion, 0.0) for p in trend_data]
            if scores:
                emotion_dominance[emotion] = {
                    'average_intensity': np.mean(scores),
                    'max_intensity': np.max(scores),
                    'frequency': sum(1 for score in scores if score > 0.5) / len(scores)
                }
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¹ÙˆØ§Ø·Ù Ø­Ø³Ø¨ Ø§Ù„Ù‡ÙŠÙ…Ù†Ø©
        dominant_emotions = sorted(
            emotion_dominance.items(),
            key=lambda x: x[1]['average_intensity'],
            reverse=True
        )[:3]
        
        return {
            'emotion_correlations': emotion_correlations,
            'dominant_emotions': {name: data for name, data in dominant_emotions},
            'emotion_dominance_full': emotion_dominance
        }
    
    async def _generate_predictions(self, trend_data: List[TrendPoint]) -> Dict[str, Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª"""
        if len(trend_data) < 5:
            return {'error': 'insufficient_data_for_prediction'}
        
        try:
            sentiment_scores = [p.sentiment_score for p in trend_data]
            timestamps = [p.timestamp for p in trend_data]
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø§ØªØ¬Ø§Ù‡ (Linear Regression)
            time_indices = list(range(len(sentiment_scores)))
            slope, intercept, r_value, p_value, std_err = stats.linregress(
                time_indices, sentiment_scores
            )
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„Ù„Ù†Ù‚Ø§Ø· Ø§Ù„ØªØ§Ù„ÙŠØ©
            future_points = 5  # Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„Ù€ 5 Ù†Ù‚Ø§Ø· Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
            predictions = []
            
            for i in range(1, future_points + 1):
                future_index = len(sentiment_scores) + i
                predicted_sentiment = slope * future_index + intercept
                
                # ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ (Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…ØªÙˆØ³Ø· Ø§Ù„ÙØªØ±Ø§Øª)
                if len(timestamps) > 1:
                    avg_interval = (timestamps[-1] - timestamps[0]) / (len(timestamps) - 1)
                    future_time = timestamps[-1] + avg_interval * i
                else:
                    future_time = datetime.now() + timedelta(hours=i)
                
                predictions.append({
                    'timestamp': future_time.isoformat(),
                    'predicted_sentiment': predicted_sentiment,
                    'confidence': max(0.1, abs(r_value))  # Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø§ØªØ³Ø§Ù‚
                })
            
            # ØªØµÙ†ÙŠÙ Ø¬ÙˆØ¯Ø© Ø§Ù„ØªÙ†Ø¨Ø¤
            prediction_quality = 'poor'
            if abs(r_value) > 0.7:
                prediction_quality = 'excellent'
            elif abs(r_value) > 0.5:
                prediction_quality = 'good'
            elif abs(r_value) > 0.3:
                prediction_quality = 'moderate'
            
            return {
                'predictions': predictions,
                'model_stats': {
                    'slope': slope,
                    'intercept': intercept,
                    'r_squared': r_value ** 2,
                    'p_value': p_value,
                    'standard_error': std_err
                },
                'prediction_quality': prediction_quality,
                'confidence_level': abs(r_value)
            }
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª: {str(e)}")
            return {'error': str(e)}
    
    async def _identify_notable_events(self, trend_data: List[TrendPoint]) -> List[Dict[str, Any]]:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ø¨Ø§Ø±Ø²Ø©"""
        notable_events = []
        
        if len(trend_data) < 3:
            return notable_events
        
        sentiment_scores = [p.sentiment_score for p in trend_data]
        mean_sentiment = np.mean(sentiment_scores)
        std_sentiment = np.std(sentiment_scores)
        
        for i, point in enumerate(trend_data):
            event = None
            
            # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø­Ø¯Ø«
            if std_sentiment > 0:
                z_score = (point.sentiment_score - mean_sentiment) / std_sentiment
                
                if z_score > 2.0:
                    event = {
                        'type': 'sentiment_spike',
                        'description': 'Ø§Ø±ØªÙØ§Ø¹ Ø­Ø§Ø¯ ÙÙŠ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©',
                        'severity': 'high' if z_score > 3.0 else 'medium'
                    }
                elif z_score < -2.0:
                    event = {
                        'type': 'sentiment_drop',
                        'description': 'Ø§Ù†Ø®ÙØ§Ø¶ Ø­Ø§Ø¯ ÙÙŠ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±',
                        'severity': 'high' if z_score < -3.0 else 'medium'
                    }
            
            # Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ø¹ÙˆØ§Ø·Ù Ø§Ù„Ù‚ÙˆÙŠØ©
            max_emotion_score = max(point.emotion_scores.values()) if point.emotion_scores else 0
            if max_emotion_score > 0.8:
                dominant_emotion = max(point.emotion_scores.items(), key=lambda x: x[1])
                event = {
                    'type': 'strong_emotion',
                    'description': f'Ø¹Ø§Ø·ÙØ© Ù‚ÙˆÙŠØ©: {dominant_emotion[0]}',
                    'emotion': dominant_emotion[0],
                    'intensity': dominant_emotion[1],
                    'severity': 'high' if max_emotion_score > 0.9 else 'medium'
                }
            
            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø­Ø¯Ø« Ø¥Ø°Ø§ ÙˆÙØ¬Ø¯
            if event:
                event.update({
                    'timestamp': point.timestamp.isoformat(),
                    'sentiment_score': point.sentiment_score,
                    'confidence': point.confidence,
                    'category': point.category,
                    'z_score': (point.sentiment_score - mean_sentiment) / std_sentiment if std_sentiment > 0 else 0
                })
                notable_events.append(event)
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
        notable_events.sort(key=lambda x: abs(x.get('z_score', 0)), reverse=True)
        
        return notable_events[:10]  # Ø£Ù‡Ù… 10 Ø£Ø­Ø¯Ø§Ø«
    
    async def _generate_trend_visualizations(self, trend_data: List[TrendPoint],
                                           emotion_analysis: Dict[str, Any],
                                           category: Optional[str]) -> Dict[str, str]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØµÙˆØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª"""
        visualizations = {}
        
        try:
            timestamps = [p.timestamp for p in trend_data]
            sentiment_scores = [p.sentiment_score for p in trend_data]
            
            # Ø±Ø³Ù… Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
            fig_main = go.Figure()
            fig_main.add_trace(go.Scatter(
                x=timestamps,
                y=sentiment_scores,
                mode='lines+markers',
                name='Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±',
                line=dict(color='blue', width=2)
            ))
            
            # Ø¥Ø¶Ø§ÙØ© Ø®Ø· Ø§Ù„Ø§ØªØ¬Ø§Ù‡
            if len(sentiment_scores) > 1:
                x_numeric = list(range(len(sentiment_scores)))
                slope, intercept, _, _, _ = stats.linregress(x_numeric, sentiment_scores)
                trend_line = [slope * x + intercept for x in x_numeric]
                
                fig_main.add_trace(go.Scatter(
                    x=timestamps,
                    y=trend_line,
                    mode='lines',
                    name='Ø®Ø· Ø§Ù„Ø§ØªØ¬Ø§Ù‡',
                    line=dict(color='red', width=1, dash='dash')
                ))
            
            fig_main.update_layout(
                title=f'Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± - {category or "Ø¹Ø§Ù…"}',
                xaxis_title='Ø§Ù„ÙˆÙ‚Øª',
                yaxis_title='Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ø´Ø§Ø¹Ø±',
                hovermode='x unified'
            )
            
            visualizations['main_trend'] = json.dumps(fig_main, cls=PlotlyJSONEncoder)
            
            # Ø±Ø³Ù… Ø§Ù„Ø¹ÙˆØ§Ø·Ù Ø¥Ø°Ø§ ØªÙˆÙØ±Øª
            if emotion_analysis:
                fig_emotions = go.Figure()
                
                for emotion, data in emotion_analysis.items():
                    if 'average_intensity' in data:
                        emotion_scores = []
                        for point in trend_data:
                            emotion_scores.append(point.emotion_scores.get(emotion, 0.0))
                        
                        fig_emotions.add_trace(go.Scatter(
                            x=timestamps,
                            y=emotion_scores,
                            mode='lines',
                            name=emotion,
                            opacity=0.7
                        ))
                
                fig_emotions.update_layout(
                    title='Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø¹ÙˆØ§Ø·Ù',
                    xaxis_title='Ø§Ù„ÙˆÙ‚Øª',
                    yaxis_title='Ø´Ø¯Ø© Ø§Ù„Ø¹Ø§Ø·ÙØ©',
                    hovermode='x unified'
                )
                
                visualizations['emotion_trends'] = json.dumps(fig_emotions, cls=PlotlyJSONEncoder)
            
            # Ø±Ø³Ù… Ø§Ù„ØªÙˆØ²ÙŠØ¹
            fig_dist = px.histogram(
                x=sentiment_scores,
                nbins=20,
                title='ØªÙˆØ²ÙŠØ¹ Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ø´Ø§Ø¹Ø±'
            )
            fig_dist.update_layout(
                xaxis_title='Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ø´Ø§Ø¹Ø±',
                yaxis_title='Ø§Ù„ØªÙƒØ±Ø§Ø±'
            )
            
            visualizations['sentiment_distribution'] = json.dumps(fig_dist, cls=PlotlyJSONEncoder)
            
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ø¹Ø¶ Ø§Ù„ØªØµÙˆØ±Ø§Øª: {str(e)}")
        
        return visualizations
    
    async def get_trend_summary(self, category: Optional[str] = None) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ø®Øµ Ø³Ø±ÙŠØ¹ Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª"""
        try:
            # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø©
            recent_data = await self._get_recent_trend_data(hours=24)
            
            if not recent_data:
                return {
                    'status': 'no_data',
                    'message': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ§Ø­Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„'
                }
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            sentiment_scores = [p.sentiment_score for p in recent_data]
            
            current_sentiment = sentiment_scores[-1] if sentiment_scores else 0.0
            average_sentiment = np.mean(sentiment_scores)
            sentiment_change = current_sentiment - sentiment_scores[0] if len(sentiment_scores) > 1 else 0.0
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
            if abs(sentiment_change) < 0.05:
                trend_status = 'stable'
                trend_emoji = 'â¡ï¸'
            elif sentiment_change > 0:
                trend_status = 'improving'
                trend_emoji = 'ğŸ“ˆ'
            else:
                trend_status = 'declining'
                trend_emoji = 'ğŸ“‰'
            
            # Ø§Ù„Ø¹Ø§Ø·ÙØ© Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø©
            all_emotions = defaultdict(list)
            for point in recent_data:
                for emotion, score in point.emotion_scores.items():
                    all_emotions[emotion].append(score)
            
            dominant_emotion = None
            if all_emotions:
                avg_emotions = {emotion: np.mean(scores) for emotion, scores in all_emotions.items()}
                dominant_emotion = max(avg_emotions.items(), key=lambda x: x[1])
            
            return {
                'status': 'success',
                'summary': {
                    'current_sentiment': current_sentiment,
                    'average_sentiment': average_sentiment,
                    'sentiment_change_24h': sentiment_change,
                    'trend_status': trend_status,
                    'trend_emoji': trend_emoji,
                    'dominant_emotion': {
                        'emotion': dominant_emotion[0] if dominant_emotion else 'unknown',
                        'intensity': dominant_emotion[1] if dominant_emotion else 0.0
                    },
                    'data_points': len(recent_data),
                    'category': category,
                    'last_updated': recent_data[-1].timestamp.isoformat() if recent_data else None
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ø®Øµ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª: {str(e)}")
            return {
                'status': 'error',
                'message': f'ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}'
            }

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    import asyncio
    
    async def test_trend_analysis():
        # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„
        trend_service = TrendAnalysisService()
        
        # Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ù†Ù‚Ø§Ø· Ø§ØªØ¬Ø§Ù‡ Ù…ØªÙ†ÙˆØ¹Ø©
        sample_points = []
        base_time = datetime.now() - timedelta(hours=24)
        
        for i in range(48):  # 48 Ù†Ù‚Ø·Ø© Ø®Ù„Ø§Ù„ 24 Ø³Ø§Ø¹Ø©
            timestamp = base_time + timedelta(minutes=30 * i)
            
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø§ØªØ¬Ø§Ù‡ Ù…ØªØ²Ø§ÙŠØ¯ Ù…Ø¹ Ø¨Ø¹Ø¶ Ø§Ù„ØªÙ‚Ù„Ø¨Ø§Øª
            base_sentiment = 0.3 + (i * 0.01) + np.random.normal(0, 0.1)
            base_sentiment = max(-1, min(1, base_sentiment))  # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ø·Ø§Ù‚
            
            point = TrendPoint(
                timestamp=timestamp,
                sentiment_score=base_sentiment,
                emotion_scores={
                    'joy': max(0, base_sentiment + np.random.normal(0, 0.1)),
                    'sadness': max(0, -base_sentiment + np.random.normal(0, 0.1)),
                    'anger': max(0, np.random.normal(0, 0.05)),
                    'fear': max(0, np.random.normal(0, 0.05))
                },
                volume=np.random.randint(1, 10),
                confidence=np.random.uniform(0.7, 0.95),
                category='news'
            )
            sample_points.append(point)
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        analysis_result = await trend_service.analyze_trends(
            category='news',
            time_range='24h',
            include_emotions=True,
            min_samples=10
        )
        
        if analysis_result['success']:
            trend_data = analysis_result['data']['trend_analysis']
            print(f"ğŸ” Ø§ØªØ¬Ø§Ù‡ Ø§Ù„ØªØ±Ù†Ø¯: {trend_data['trend_direction']}")
            print(f"ğŸ’ª Ù‚ÙˆØ© Ø§Ù„Ø§ØªØ¬Ø§Ù‡: {trend_data['trend_strength']:.2f}")
            print(f"â° Ù…Ø¯Ø© Ø§Ù„Ø§ØªØ¬Ø§Ù‡: {trend_data['trend_duration']}")
            print(f"ğŸ“Š Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚Ù…Ø©: {len(trend_data['peak_points'])}")
            print(f"ğŸ“‰ Ù†Ù‚Ø§Ø· Ø§Ù„Ø§Ù†Ø®ÙØ§Ø¶: {len(trend_data['valley_points'])}")
            print(f"âš ï¸ Ù†Ù‚Ø§Ø· Ø´Ø§Ø°Ø©: {len(trend_data['anomalies'])}")
            
            # Ø¹Ø±Ø¶ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
            predictions = analysis_result['data']['predictions']
            if 'predictions' in predictions:
                print(f"\nğŸ”® Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª:")
                for pred in predictions['predictions'][:3]:
                    print(f"  {pred['timestamp']}: {pred['predicted_sentiment']:.2f}")
            
            # Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ø¨Ø§Ø±Ø²Ø©
            events = analysis_result['data']['notable_events']
            print(f"\nğŸ“Œ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ø¨Ø§Ø±Ø²Ø©: {len(events)}")
            for event in events[:3]:
                print(f"  {event['type']}: {event['description']}")
        else:
            print(f"âŒ ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {analysis_result.get('message', 'Ø®Ø·Ø£ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}")
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    asyncio.run(test_trend_analysis())
