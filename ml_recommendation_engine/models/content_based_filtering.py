# Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª - Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆØ§Ø¦ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
# Content-Based Filtering Models for Sabq AI Recommendation Engine

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel, pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import LatentDirichletAllocation, NMF
from sklearn.cluster import KMeans
import arabic_reshaper
import bidi.algorithm
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import ISRIStemmer
import gensim
from gensim.models import Word2Vec, Doc2Vec
from gensim.models.doc2vec import TaggedDocument
import logging
from typing import Dict, List, Tuple, Optional, Any, Union
from datetime import datetime, timedelta
import joblib
import pickle
from dataclasses import dataclass
import asyncio
from concurrent.futures import ThreadPoolExecutor
import multiprocessing as mp

# Ø¥Ø¹Ø¯Ø§Ø¯ NLTK Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class ContentFilteringConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆØ§Ø¦ÙŠØ©"""
    # BERT settings
    bert_model_name: str = "aubmindlab/bert-base-arabert"
    max_sequence_length: int = 512
    
    # TF-IDF settings
    max_features: int = 10000
    min_df: int = 2
    max_df: float = 0.95
    ngram_range: Tuple[int, int] = (1, 3)
    
    # Topic modeling
    n_topics: int = 50
    topic_model_type: str = "LDA"  # LDA, NMF, BERTopic
    
    # Similarity settings
    similarity_threshold: float = 0.1
    top_similar_articles: int = 100
    
    # Word2Vec settings
    word2vec_size: int = 200
    word2vec_window: int = 5
    word2vec_min_count: int = 2
    
    # Doc2Vec settings
    doc2vec_size: int = 300
    doc2vec_window: int = 5
    doc2vec_min_count: int = 2
    
    # Processing
    use_stemming: bool = True
    remove_diacritics: bool = True
    chunk_size: int = 1000


class ArabicTextProcessor:
    """
    Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
    Advanced Arabic Text Processor
    """
    
    def __init__(self, config: ContentFilteringConfig):
        self.config = config
        self.stemmer = ISRIStemmer()
        
        # Ù‚Ø§Ø¦Ù…Ø© ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        try:
            self.arabic_stopwords = set(stopwords.words('arabic'))
        except:
            self.arabic_stopwords = set()
        
        # Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª Ø¥ÙŠÙ‚Ø§Ù Ù…Ø®ØµØµØ© Ù„Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠ
        custom_stopwords = {
            'Ù‚Ø§Ù„', 'Ù‚Ø§Ù„Øª', 'Ø£Ø¶Ø§Ù', 'Ø£Ø¶Ø§ÙØª', 'Ø£ÙˆØ¶Ø­', 'Ø£ÙˆØ¶Ø­Øª', 'Ø°ÙƒØ±', 'Ø°ÙƒØ±Øª',
            'Ø£Ø´Ø§Ø±', 'Ø£Ø´Ø§Ø±Øª', 'Ø¨ÙŠÙ†', 'Ø¨ÙŠÙ†Øª', 'Ø£ÙƒØ¯', 'Ø£ÙƒØ¯Øª', 'Ù†ÙˆÙ‡', 'Ù†ÙˆÙ‡Øª',
            'Ù„ÙØª', 'Ù„ÙØªØª', 'Ø´Ø¯Ø¯', 'Ø´Ø¯Ø¯Øª', 'Ø£Ø¨Ø§Ù†', 'Ø£Ø¨Ø§Ù†Øª', 'Ø¨Ø­Ø³Ø¨', 'ÙˆÙÙ‚Ø§',
            'Ø­Ø³Ø¨', 'Ø·Ø¨Ù‚Ø§', 'ÙˆÙÙ‚', 'Ø¨Ù†Ø§Ø¡', 'Ø§Ø³ØªÙ†Ø§Ø¯Ø§', 'ØªØ¨Ø¹Ø§', 'Ø­ÙŠØ«', 'ÙƒÙ…Ø§',
            'Ø¥Ø°', 'Ø¹Ù†Ø¯', 'Ù„Ø¯Ù‰', 'ÙÙŠ', 'Ø¹Ù„Ù‰', 'Ø¥Ù„Ù‰', 'Ù…Ù†', 'Ø¹Ù†', 'Ù…Ø¹'
        }
        self.arabic_stopwords.update(custom_stopwords)
        
        # ØªØ¹Ø¨ÙŠØ±Ø§Øª Ù…Ù†ØªØ¸Ù…Ø© Ù„Ù„ØªÙ†Ø¸ÙŠÙ
        self.cleaning_patterns = [
            (r'[^\u0600-\u06FF\s]', ''),  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù ØºÙŠØ± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            (r'\s+', ' '),  # Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
            (r'[\u064B-\u0652]', ''),  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
            (r'[\u0640]', ''),  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ·ÙˆÙŠÙ„
        ]
    
    def clean_text(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
        if not text or not isinstance(text, str):
            return ""
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ø¨ÙŠØ±Ø§Øª Ø§Ù„Ù…Ù†ØªØ¸Ù…Ø©
        for pattern, replacement in self.cleaning_patterns:
            text = re.sub(pattern, replacement, text)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨ Ø°Ù„Ùƒ
        if self.config.remove_diacritics:
            text = self._remove_diacritics(text)
        
        return text.strip()
    
    def _remove_diacritics(self, text: str) -> str:
        """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ù…Ù† Ø§Ù„Ù†Øµ"""
        diacritics = r'[\u064B-\u0652\u0670\u0640]'
        return re.sub(diacritics, '', text)
    
    def tokenize(self, text: str) -> List[str]:
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ²"""
        text = self.clean_text(text)
        tokens = text.split()
        
        # Ø¥Ø²Ø§Ù„Ø© ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù
        tokens = [token for token in tokens if token not in self.arabic_stopwords]
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚ Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨ Ø°Ù„Ùƒ
        if self.config.use_stemming:
            tokens = [self.stemmer.stem(token) for token in tokens]
        
        return [token for token in tokens if len(token) > 1]
    
    def preprocess_article(self, article: Dict[str, str]) -> Dict[str, str]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù‚Ø§Ù„ ÙƒØ§Ù…Ù„"""
        processed = {}
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
        if 'title' in article:
            processed['title'] = self.clean_text(article['title'])
            processed['title_tokens'] = self.tokenize(article['title'])
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        if 'content' in article:
            processed['content'] = self.clean_text(article['content'])
            processed['content_tokens'] = self.tokenize(article['content'])
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ø®Øµ
        if 'summary' in article:
            processed['summary'] = self.clean_text(article['summary'])
            processed['summary_tokens'] = self.tokenize(article['summary'])
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        if 'tags' in article:
            if isinstance(article['tags'], str):
                tags = article['tags'].split(',')
            else:
                tags = article['tags']
            processed['tags'] = [self.clean_text(tag) for tag in tags]
        
        # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ
        all_text = []
        if 'title' in processed:
            all_text.append(processed['title'])
        if 'content' in processed:
            all_text.append(processed['content'])
        if 'summary' in processed:
            all_text.append(processed['summary'])
        
        processed['combined_text'] = ' '.join(all_text)
        processed['combined_tokens'] = self.tokenize(processed['combined_text'])
        
        return processed


class BERTContentExtractor:
    """
    Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BERT Ø§Ù„Ø¹Ø±Ø¨ÙŠ
    BERT-based Content Feature Extractor
    """
    
    def __init__(self, config: ContentFilteringConfig):
        self.config = config
        self.tokenizer = None
        self.model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self._load_model()
    
    def _load_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BERT Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
        try:
            logger.info(f"ğŸ¤– ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BERT: {self.config.bert_model_name}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.bert_model_name)
            self.model = AutoModel.from_pretrained(self.config.bert_model_name)
            self.model.to(self.device)
            self.model.eval()
            logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BERT Ø¨Ù†Ø¬Ø§Ø­")
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BERT: {str(e)}")
            raise
    
    def extract_embeddings(self, texts: List[str], batch_size: int = 8) -> np.ndarray:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ"""
        embeddings = []
        
        logger.info(f"ğŸ”„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ù„Ù€ {len(texts)} Ù†Øµ...")
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_embeddings = self._extract_batch_embeddings(batch_texts)
            embeddings.extend(batch_embeddings)
            
            if (i + batch_size) % 100 == 0:
                logger.info(f"ğŸ“Š ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {min(i + batch_size, len(texts))} / {len(texts)} Ù†Øµ")
        
        return np.array(embeddings)
    
    def _extract_batch_embeddings(self, texts: List[str]) -> List[np.ndarray]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ"""
        with torch.no_grad():
            # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†ØµÙˆØµ
            encoded = self.tokenizer(
                texts,
                padding=True,
                truncation=True,
                max_length=self.config.max_sequence_length,
                return_tensors='pt'
            ).to(self.device)
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
            outputs = self.model(**encoded)
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØªÙˆØ³Ø· Ø¢Ø®Ø± Ø·Ø¨Ù‚Ø© Ù…Ø®ÙÙŠØ©
            embeddings = outputs.last_hidden_state.mean(dim=1)
            
            return embeddings.cpu().numpy()
    
    def extract_sentence_embeddings(self, text: str) -> np.ndarray:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ¶Ù…ÙŠÙ† Ù„Ù†Øµ ÙˆØ§Ø­Ø¯"""
        return self.extract_embeddings([text])[0]
    
    def calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø¨ÙŠÙ† Ù†ØµÙŠÙ†"""
        emb1 = self.extract_sentence_embeddings(text1)
        emb2 = self.extract_sentence_embeddings(text2)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙƒÙˆØ³Ø§Ù†ÙŠ
        similarity = cosine_similarity([emb1], [emb2])[0][0]
        return float(similarity)


class TFIDFContentExtractor:
    """
    Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF
    TF-IDF based Content Feature Extractor
    """
    
    def __init__(self, config: ContentFilteringConfig):
        self.config = config
        self.text_processor = ArabicTextProcessor(config)
        self.vectorizer = None
        self.feature_matrix = None
        self.article_ids = []
    
    def _custom_tokenizer(self, text: str) -> List[str]:
        """Ù…ÙØ±Ù…Ø² Ù…Ø®ØµØµ Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
        return self.text_processor.tokenize(text)
    
    def fit_transform(self, articles_df: pd.DataFrame) -> np.ndarray:
        """ØªØ¯Ø±ÙŠØ¨ ÙˆØªØ­ÙˆÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª"""
        logger.info("ğŸ”„ Ø¨Ù†Ø§Ø¡ Ù…ØµÙÙˆÙØ© TF-IDF...")
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù†ØµÙˆØµ
        texts = []
        self.article_ids = []
        
        for _, article in articles_df.iterrows():
            # Ø¯Ù…Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰
            combined_text = f"{article.get('title', '')} {article.get('content', '')}"
            texts.append(combined_text)
            self.article_ids.append(article['id'])
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØ¬Ù…Ø¹ TF-IDF
        self.vectorizer = TfidfVectorizer(
            max_features=self.config.max_features,
            min_df=self.config.min_df,
            max_df=self.config.max_df,
            ngram_range=self.config.ngram_range,
            tokenizer=self._custom_tokenizer,
            lowercase=False,  # Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ø§ ØªØ­ØªØ§Ø¬ ØªØ­ÙˆÙŠÙ„ Ù„Ø£Ø­Ø±Ù ØµØºÙŠØ±Ø©
            stop_words=None   # Ù†ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù ÙÙŠ Ø§Ù„Ù…ÙØ±Ù…Ø²
        )
        
        # ØªØ¯Ø±ÙŠØ¨ ÙˆØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.feature_matrix = self.vectorizer.fit_transform(texts)
        
        logger.info(f"âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ù…ØµÙÙˆÙØ© TF-IDF: {self.feature_matrix.shape}")
        return self.feature_matrix
    
    def transform(self, texts: List[str]) -> np.ndarray:
        """ØªØ­ÙˆÙŠÙ„ Ù†ØµÙˆØµ Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨"""
        if self.vectorizer is None:
            raise ValueError("Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø¯Ø±Ø¨. Ø§Ø³ØªØ®Ø¯Ù… fit_transform Ø£ÙˆÙ„Ø§Ù‹.")
        
        return self.vectorizer.transform(texts)
    
    def get_similar_articles(self, article_id: str, n_similar: int = 10) -> List[Tuple[str, float]]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù…Ù‚Ø§Ù„ Ù…Ø¹ÙŠÙ†"""
        if article_id not in self.article_ids:
            return []
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¤Ø´Ø± Ø§Ù„Ù…Ù‚Ø§Ù„
        article_idx = self.article_ids.index(article_id)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙƒÙˆØ³Ø§Ù†ÙŠ
        article_vector = self.feature_matrix[article_idx:article_idx+1]
        similarities = cosine_similarity(article_vector, self.feature_matrix).flatten()
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        similar_indices = np.argsort(similarities)[::-1][1:n_similar+1]  # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ù…Ù‚Ø§Ù„ Ù†ÙØ³Ù‡
        
        similar_articles = []
        for idx in similar_indices:
            similar_id = self.article_ids[idx]
            similarity = similarities[idx]
            if similarity > self.config.similarity_threshold:
                similar_articles.append((similar_id, float(similarity)))
        
        return similar_articles
    
    def get_content_recommendations(self, user_articles: List[str], 
                                  n_recommendations: int = 10) -> List[Tuple[str, float]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        if not user_articles or self.feature_matrix is None:
            return []
        
        # Ø¨Ù†Ø§Ø¡ Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙƒÙ…ØªÙˆØ³Ø· Ù…Ù‚Ø§Ù„Ø§ØªÙ‡
        user_vectors = []
        for article_id in user_articles:
            if article_id in self.article_ids:
                article_idx = self.article_ids.index(article_id)
                user_vectors.append(self.feature_matrix[article_idx].toarray()[0])
        
        if not user_vectors:
            return []
        
        # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        user_profile = np.mean(user_vectors, axis=0)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
        similarities = cosine_similarity([user_profile], self.feature_matrix).flatten()
        
        # ØªØ±ØªÙŠØ¨ ÙˆØ§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø±Ø¦ÙŠØ©
        recommendations = []
        for i, similarity in enumerate(similarities):
            article_id = self.article_ids[i]
            if (article_id not in user_articles and 
                similarity > self.config.similarity_threshold):
                recommendations.append((article_id, float(similarity)))
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        recommendations.sort(key=lambda x: x[1], reverse=True)
        
        return recommendations[:n_recommendations]
    
    def get_feature_names(self) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ù…"""
        if self.vectorizer is None:
            return []
        return self.vectorizer.get_feature_names_out().tolist()
    
    def explain_similarity(self, article1_id: str, article2_id: str, 
                          top_features: int = 10) -> Dict[str, Any]:
        """Ø´Ø±Ø­ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ù…Ù‚Ø§Ù„ÙŠÙ†"""
        if (article1_id not in self.article_ids or 
            article2_id not in self.article_ids):
            return {}
        
        idx1 = self.article_ids.index(article1_id)
        idx2 = self.article_ids.index(article2_id)
        
        vector1 = self.feature_matrix[idx1].toarray()[0]
        vector2 = self.feature_matrix[idx2].toarray()[0]
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø³Ø§Ù‡Ù…Ø© ÙƒÙ„ Ù…Ø¹Ù„Ù… ÙÙŠ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        feature_contributions = vector1 * vector2
        feature_names = self.get_feature_names()
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø­Ø³Ø¨ Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø©
        top_indices = np.argsort(feature_contributions)[::-1][:top_features]
        
        explanations = []
        for idx in top_indices:
            if feature_contributions[idx] > 0:
                explanations.append({
                    'feature': feature_names[idx],
                    'contribution': float(feature_contributions[idx]),
                    'weight1': float(vector1[idx]),
                    'weight2': float(vector2[idx])
                })
        
        overall_similarity = cosine_similarity([vector1], [vector2])[0][0]
        
        return {
            'overall_similarity': float(overall_similarity),
            'top_contributing_features': explanations
        }


class TopicModelingEngine:
    """
    Ù…Ø­Ø±Ùƒ Ø§Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ÙŠØ©
    Topic Modeling Engine
    """
    
    def __init__(self, config: ContentFilteringConfig):
        self.config = config
        self.text_processor = ArabicTextProcessor(config)
        self.topic_model = None
        self.vectorizer = None
        self.article_topics = None
        self.topic_labels = []
    
    def train_lda_model(self, articles_df: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ LDA"""
        logger.info("ğŸ”„ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ LDA Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª...")
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù†ØµÙˆØµ
        texts = []
        for _, article in articles_df.iterrows():
            combined_text = f"{article.get('title', '')} {article.get('content', '')}"
            texts.append(combined_text)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØµÙÙˆÙØ© TF-IDF
        self.vectorizer = TfidfVectorizer(
            max_features=self.config.max_features,
            min_df=self.config.min_df,
            max_df=self.config.max_df,
            tokenizer=self.text_processor.tokenize,
            lowercase=False
        )
        
        text_features = self.vectorizer.fit_transform(texts)
        
        # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ LDA
        self.topic_model = LatentDirichletAllocation(
            n_components=self.config.n_topics,
            random_state=42,
            learning_method='batch',
            max_iter=100
        )
        
        self.topic_model.fit(text_features)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù„Ù„Ù…Ù‚Ø§Ù„Ø§Øª
        self.article_topics = self.topic_model.transform(text_features)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
        self._extract_topic_labels()
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
        perplexity = self.topic_model.perplexity(text_features)
        log_likelihood = self.topic_model.score(text_features)
        
        logger.info(f"âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ LDA - Perplexity: {perplexity:.2f}")
        
        return {
            'model_type': 'LDA',
            'n_topics': self.config.n_topics,
            'perplexity': perplexity,
            'log_likelihood': log_likelihood,
            'topic_labels': self.topic_labels
        }
    
    def train_nmf_model(self, articles_df: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ NMF Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª"""
        logger.info("ğŸ”„ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ NMF Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª...")
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù†ØµÙˆØµ (Ù…Ø´Ø§Ø¨Ù‡ Ù„Ù€ LDA)
        texts = []
        for _, article in articles_df.iterrows():
            combined_text = f"{article.get('title', '')} {article.get('content', '')}"
            texts.append(combined_text)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØµÙÙˆÙØ© TF-IDF
        self.vectorizer = TfidfVectorizer(
            max_features=self.config.max_features,
            min_df=self.config.min_df,
            max_df=self.config.max_df,
            tokenizer=self.text_processor.tokenize,
            lowercase=False
        )
        
        text_features = self.vectorizer.fit_transform(texts)
        
        # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ NMF
        self.topic_model = NMF(
            n_components=self.config.n_topics,
            random_state=42,
            init='nndsvd',
            max_iter=200
        )
        
        self.article_topics = self.topic_model.fit_transform(text_features)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
        self._extract_topic_labels()
        
        # Ø­Ø³Ø§Ø¨ Ø®Ø·Ø£ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø¨Ù†Ø§Ø¡
        reconstruction_error = self.topic_model.reconstruction_err_
        
        logger.info(f"âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ NMF - Reconstruction Error: {reconstruction_error:.2f}")
        
        return {
            'model_type': 'NMF',
            'n_topics': self.config.n_topics,
            'reconstruction_error': reconstruction_error,
            'topic_labels': self.topic_labels
        }
    
    def _extract_topic_labels(self, top_words: int = 5):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª"""
        feature_names = self.vectorizer.get_feature_names_out()
        
        self.topic_labels = []
        for topic_idx, topic in enumerate(self.topic_model.components_):
            top_features_ind = topic.argsort()[-top_words:][::-1]
            top_features = [feature_names[i] for i in top_features_ind]
            self.topic_labels.append(f"Ù…ÙˆØ¶ÙˆØ¹_{topic_idx}: {', '.join(top_features[:3])}")
    
    def get_article_topics(self, article_id: str, article_idx: int) -> List[Tuple[int, float]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ù‚Ø§Ù„ Ù…Ø¹ÙŠÙ†"""
        if self.article_topics is None or article_idx >= len(self.article_topics):
            return []
        
        topic_distribution = self.article_topics[article_idx]
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©
        topic_probs = [(i, prob) for i, prob in enumerate(topic_distribution)]
        topic_probs.sort(key=lambda x: x[1], reverse=True)
        
        # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø°Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© > 0.1
        return [(topic_id, prob) for topic_id, prob in topic_probs if prob > 0.1]
    
    def get_similar_articles_by_topic(self, article_idx: int, 
                                    n_similar: int = 10) -> List[Tuple[int, float]]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø´Ø§Ø¨Ù‡Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª"""
        if self.article_topics is None or article_idx >= len(self.article_topics):
            return []
        
        article_topics = self.article_topics[article_idx:article_idx+1]
        similarities = cosine_similarity(article_topics, self.article_topics).flatten()
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        similar_indices = np.argsort(similarities)[::-1][1:n_similar+1]
        
        return [(int(idx), float(similarities[idx])) for idx in similar_indices]
    
    def get_topic_trends(self, time_window: str = "daily") -> Dict[str, List]:
        """ØªØ­Ù„ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø¹Ø¨Ø± Ø§Ù„Ø²Ù…Ù†"""
        # ÙŠÙ…ÙƒÙ† ØªØ·ÙˆÙŠØ± Ù‡Ø°Ø§ Ù„ÙŠØ­Ù„Ù„ ØªØ·ÙˆØ± Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø¹Ø¨Ø± Ø§Ù„Ø²Ù…Ù†
        if self.article_topics is None:
            return {}
        
        # Ø­Ø³Ø§Ø¨ Ø´Ø¹Ø¨ÙŠØ© ÙƒÙ„ Ù…ÙˆØ¶ÙˆØ¹
        topic_popularity = np.mean(self.article_topics, axis=0)
        
        trends = {}
        for i, (topic_label, popularity) in enumerate(zip(self.topic_labels, topic_popularity)):
            trends[topic_label] = {
                'popularity_score': float(popularity),
                'rank': i + 1
            }
        
        return trends


class Word2VecContentModel:
    """
    Ù†Ù…ÙˆØ°Ø¬ Word2Vec Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
    Word2Vec Content Model for Arabic
    """
    
    def __init__(self, config: ContentFilteringConfig):
        self.config = config
        self.text_processor = ArabicTextProcessor(config)
        self.word2vec_model = None
        self.article_vectors = {}
    
    def train_word2vec(self, articles_df: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Word2Vec"""
        logger.info("ğŸ”„ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Word2Vec...")
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¬Ù…Ù„ Ù„Ù„ØªØ¯Ø±ÙŠØ¨
        sentences = []
        for _, article in articles_df.iterrows():
            combined_text = f"{article.get('title', '')} {article.get('content', '')}"
            tokens = self.text_processor.tokenize(combined_text)
            if len(tokens) > 5:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
                sentences.append(tokens)
        
        logger.info(f"ğŸ“ ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ {len(sentences)} Ø¬Ù…Ù„Ø©...")
        
        # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Word2Vec
        self.word2vec_model = Word2Vec(
            sentences=sentences,
            vector_size=self.config.word2vec_size,
            window=self.config.word2vec_window,
            min_count=self.config.word2vec_min_count,
            workers=mp.cpu_count(),
            epochs=30,
            sg=1  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Skip-gram
        )
        
        # Ø­Ø³Ø§Ø¨ Ù…ØªØ¬Ù‡Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
        self._calculate_article_vectors(articles_df)
        
        vocab_size = len(self.word2vec_model.wv.key_to_index)
        logger.info(f"âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ Word2Vec - Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª: {vocab_size}")
        
        return {
            'model_type': 'Word2Vec',
            'vocab_size': vocab_size,
            'vector_size': self.config.word2vec_size,
            'n_articles': len(self.article_vectors)
        }
    
    def _calculate_article_vectors(self, articles_df: pd.DataFrame):
        """Ø­Ø³Ø§Ø¨ Ù…ØªØ¬Ù‡Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª ÙƒÙ…ØªÙˆØ³Ø· ÙƒÙ„Ù…Ø§ØªÙ‡Ø§"""
        logger.info("ğŸ“Š Ø­Ø³Ø§Ø¨ Ù…ØªØ¬Ù‡Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª...")
        
        for _, article in articles_df.iterrows():
            combined_text = f"{article.get('title', '')} {article.get('content', '')}"
            tokens = self.text_processor.tokenize(combined_text)
            
            # Ø¬Ù…Ø¹ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            word_vectors = []
            for token in tokens:
                if token in self.word2vec_model.wv:
                    word_vectors.append(self.word2vec_model.wv[token])
            
            # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
            if word_vectors:
                article_vector = np.mean(word_vectors, axis=0)
                self.article_vectors[article['id']] = article_vector
    
    def get_similar_articles(self, article_id: str, n_similar: int = 10) -> List[Tuple[str, float]]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø´Ø§Ø¨Ù‡Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Word2Vec"""
        if article_id not in self.article_vectors:
            return []
        
        target_vector = self.article_vectors[article_id]
        similarities = []
        
        for other_id, other_vector in self.article_vectors.items():
            if other_id != article_id:
                similarity = cosine_similarity([target_vector], [other_vector])[0][0]
                similarities.append((other_id, float(similarity)))
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        return similarities[:n_similar]
    
    def find_similar_words(self, word: str, top_n: int = 10) -> List[Tuple[str, float]]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…Ø´Ø§Ø¨Ù‡Ø©"""
        if self.word2vec_model is None or word not in self.word2vec_model.wv:
            return []
        
        try:
            similar_words = self.word2vec_model.wv.most_similar(word, topn=top_n)
            return similar_words
        except:
            return []


class ContentBasedRecommender:
    """
    Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆØ§Ø¦ÙŠØ© Ø§Ù„Ù…ÙˆØ­Ø¯
    Unified Content-Based Recommendation System
    """
    
    def __init__(self, config: ContentFilteringConfig):
        self.config = config
        self.text_processor = ArabicTextProcessor(config)
        
        # Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        self.bert_extractor = None
        self.tfidf_extractor = None
        self.topic_model = None
        self.word2vec_model = None
        
        # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
        self.articles_df = None
        self.article_features = {}
        
        # Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        self.method_weights = {
            'bert': 0.4,
            'tfidf': 0.3,
            'topics': 0.2,
            'word2vec': 0.1
        }
    
    def train_all_models(self, articles_df: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        logger.info("ğŸ­ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆØ§Ø¦ÙŠØ©...")
        
        self.articles_df = articles_df.copy()
        results = {}
        
        try:
            # ØªØ¯Ø±ÙŠØ¨ BERT
            logger.info("1ï¸âƒ£ ØªØ¯Ø±ÙŠØ¨ Ù…Ø³ØªØ®Ø±Ø¬ BERT...")
            self.bert_extractor = BERTContentExtractor(self.config)
            results['bert'] = {'status': 'success', 'model_type': 'BERT'}
            
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ BERT: {str(e)}")
            results['bert'] = {'status': 'failed', 'error': str(e)}
        
        try:
            # ØªØ¯Ø±ÙŠØ¨ TF-IDF
            logger.info("2ï¸âƒ£ ØªØ¯Ø±ÙŠØ¨ Ù…Ø³ØªØ®Ø±Ø¬ TF-IDF...")
            self.tfidf_extractor = TFIDFContentExtractor(self.config)
            self.tfidf_extractor.fit_transform(articles_df)
            results['tfidf'] = {'status': 'success', 'model_type': 'TF-IDF'}
            
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ TF-IDF: {str(e)}")
            results['tfidf'] = {'status': 'failed', 'error': str(e)}
        
        try:
            # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
            logger.info("3ï¸âƒ£ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª...")
            self.topic_model = TopicModelingEngine(self.config)
            if self.config.topic_model_type.upper() == 'LDA':
                topic_result = self.topic_model.train_lda_model(articles_df)
            else:
                topic_result = self.topic_model.train_nmf_model(articles_df)
            results['topics'] = topic_result
            
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª: {str(e)}")
            results['topics'] = {'status': 'failed', 'error': str(e)}
        
        try:
            # ØªØ¯Ø±ÙŠØ¨ Word2Vec
            logger.info("4ï¸âƒ£ ØªØ¯Ø±ÙŠØ¨ Word2Vec...")
            self.word2vec_model = Word2VecContentModel(self.config)
            w2v_result = self.word2vec_model.train_word2vec(articles_df)
            results['word2vec'] = w2v_result
            
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Word2Vec: {str(e)}")
            results['word2vec'] = {'status': 'failed', 'error': str(e)}
        
        logger.info("ğŸ‰ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
        return results
    
    def get_content_recommendations(self, user_articles: List[str], 
                                  n_recommendations: int = 10,
                                  exclude_articles: Optional[List[str]] = None) -> List[Tuple[str, float]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Ù…Ø­ØªÙˆØ§Ø¦ÙŠØ© Ù…ÙˆØ­Ø¯Ø©"""
        if not user_articles:
            return self._get_popular_articles(n_recommendations)
        
        # Ø¬Ù…Ø¹ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ù…Ù† Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        all_recommendations = {}
        total_weight = 0
        
        # ØªÙˆØµÙŠØ§Øª TF-IDF
        if self.tfidf_extractor:
            try:
                tfidf_recs = self.tfidf_extractor.get_content_recommendations(
                    user_articles, n_recommendations * 2
                )
                self._merge_recommendations(all_recommendations, tfidf_recs, 
                                         self.method_weights['tfidf'])
                total_weight += self.method_weights['tfidf']
            except Exception as e:
                logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª TF-IDF: {str(e)}")
        
        # ØªÙˆØµÙŠØ§Øª Word2Vec
        if self.word2vec_model:
            try:
                w2v_recs = self._get_word2vec_recommendations(user_articles, n_recommendations * 2)
                self._merge_recommendations(all_recommendations, w2v_recs,
                                         self.method_weights['word2vec'])
                total_weight += self.method_weights['word2vec']
            except Exception as e:
                logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Word2Vec: {str(e)}")
        
        # ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
        if self.topic_model:
            try:
                topic_recs = self._get_topic_recommendations(user_articles, n_recommendations * 2)
                self._merge_recommendations(all_recommendations, topic_recs,
                                         self.method_weights['topics'])
                total_weight += self.method_weights['topics']
            except Exception as e:
                logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª: {str(e)}")
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Ù‚Ø§Ø·
        if total_weight > 0:
            for article_id in all_recommendations:
                all_recommendations[article_id] /= total_weight
        
        # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
        if exclude_articles:
            for article_id in exclude_articles:
                all_recommendations.pop(article_id, None)
        
        # ØªØ±ØªÙŠØ¨ ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø£ÙØ¶Ù„ Ø§Ù„ØªÙˆØµÙŠØ§Øª
        sorted_recommendations = sorted(
            all_recommendations.items(),
            key=lambda x: x[1],
            reverse=True
        )[:n_recommendations]
        
        return sorted_recommendations
    
    def _merge_recommendations(self, all_recs: Dict[str, float], 
                             new_recs: List[Tuple[str, float]], weight: float):
        """Ø¯Ù…Ø¬ ØªÙˆØµÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¹ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©"""
        for article_id, score in new_recs:
            if article_id not in all_recs:
                all_recs[article_id] = 0.0
            all_recs[article_id] += score * weight
    
    def _get_word2vec_recommendations(self, user_articles: List[str], 
                                    n_recommendations: int) -> List[Tuple[str, float]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Word2Vec"""
        if not self.word2vec_model or not self.word2vec_model.article_vectors:
            return []
        
        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„ÙƒÙ„ Ù…Ù‚Ø§Ù„ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…
        all_similar = {}
        
        for article_id in user_articles:
            similar_articles = self.word2vec_model.get_similar_articles(
                article_id, n_recommendations
            )
            
            for sim_id, similarity in similar_articles:
                if sim_id not in all_similar:
                    all_similar[sim_id] = 0.0
                all_similar[sim_id] += similarity
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Ù‚Ø§Ø·
        if user_articles:
            for article_id in all_similar:
                all_similar[article_id] /= len(user_articles)
        
        # ØªØ±ØªÙŠØ¨ ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        sorted_similar = sorted(all_similar.items(), key=lambda x: x[1], reverse=True)
        return sorted_similar[:n_recommendations]
    
    def _get_topic_recommendations(self, user_articles: List[str],
                                 n_recommendations: int) -> List[Tuple[str, float]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª"""
        if not self.topic_model or self.topic_model.article_topics is None:
            return []
        
        # Ø¨Ù†Ø§Ø¡ Ù…Ù„Ù Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ù‚Ø§Ù„Ø§ØªÙ‡
        user_topic_profile = np.zeros(self.config.n_topics)
        article_count = 0
        
        for article_id in user_articles:
            if article_id in self.tfidf_extractor.article_ids:
                article_idx = self.tfidf_extractor.article_ids.index(article_id)
                if article_idx < len(self.topic_model.article_topics):
                    user_topic_profile += self.topic_model.article_topics[article_idx]
                    article_count += 1
        
        if article_count == 0:
            return []
        
        # ØªØ·Ø¨ÙŠØ¹ Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        user_topic_profile /= article_count
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
        similarities = cosine_similarity([user_topic_profile], 
                                       self.topic_model.article_topics).flatten()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªÙˆØµÙŠØ§Øª
        recommendations = []
        for i, similarity in enumerate(similarities):
            if i < len(self.tfidf_extractor.article_ids):
                article_id = self.tfidf_extractor.article_ids[i]
                if article_id not in user_articles and similarity > 0.1:
                    recommendations.append((article_id, float(similarity)))
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        recommendations.sort(key=lambda x: x[1], reverse=True)
        return recommendations[:n_recommendations]
    
    def _get_popular_articles(self, n_articles: int) -> List[Tuple[str, float]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙƒØ«Ø± Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø´Ø¹Ø¨ÙŠØ© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ø¬Ø¯Ø¯"""
        if self.articles_df is None:
            return []
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª Ø£Ùˆ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª
        if 'views' in self.articles_df.columns:
            popular = self.articles_df.nlargest(n_articles, 'views')
        elif 'interactions_count' in self.articles_df.columns:
            popular = self.articles_df.nlargest(n_articles, 'interactions_count')
        else:
            # ØªØ±ØªÙŠØ¨ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¥Ø°Ø§ Ù„Ù… ØªØªÙˆÙØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø´Ø¹Ø¨ÙŠØ©
            popular = self.articles_df.sample(min(n_articles, len(self.articles_df)))
        
        return [(row['id'], 1.0) for _, row in popular.iterrows()]
    
    def explain_recommendation(self, user_articles: List[str], 
                             recommended_article: str) -> Dict[str, Any]:
        """Ø´Ø±Ø­ Ø³Ø¨Ø¨ Ø§Ù„ØªÙˆØµÙŠØ©"""
        explanation = {
            'recommended_article': recommended_article,
            'user_articles': user_articles,
            'explanation_methods': []
        }
        
        # Ø´Ø±Ø­ TF-IDF
        if self.tfidf_extractor and len(user_articles) > 0:
            for user_article in user_articles[:3]:  # Ø£ÙˆÙ„ 3 Ù…Ù‚Ø§Ù„Ø§Øª
                tfidf_explanation = self.tfidf_extractor.explain_similarity(
                    user_article, recommended_article
                )
                if tfidf_explanation:
                    explanation['explanation_methods'].append({
                        'method': 'TF-IDF',
                        'base_article': user_article,
                        'similarity': tfidf_explanation['overall_similarity'],
                        'key_features': tfidf_explanation['top_contributing_features'][:5]
                    })
        
        # Ø´Ø±Ø­ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
        if self.topic_model and recommended_article in self.tfidf_extractor.article_ids:
            rec_idx = self.tfidf_extractor.article_ids.index(recommended_article)
            rec_topics = self.topic_model.get_article_topics(recommended_article, rec_idx)
            
            explanation['topics'] = {
                'recommended_article_topics': rec_topics[:3],
                'topic_labels': [self.topic_model.topic_labels[t[0]] for t in rec_topics[:3]]
            }
        
        return explanation
    
    def save_models(self, base_path: str):
        """Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        logger.info(f"ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ {base_path}")
        
        # Ø­ÙØ¸ TF-IDF
        if self.tfidf_extractor:
            joblib.dump(self.tfidf_extractor, f"{base_path}_tfidf.pkl")
        
        # Ø­ÙØ¸ Topic Model
        if self.topic_model:
            joblib.dump(self.topic_model, f"{base_path}_topics.pkl")
        
        # Ø­ÙØ¸ Word2Vec
        if self.word2vec_model and self.word2vec_model.word2vec_model:
            self.word2vec_model.word2vec_model.save(f"{base_path}_word2vec.model")
            joblib.dump(self.word2vec_model.article_vectors, f"{base_path}_article_vectors.pkl")
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙƒÙˆÙŠÙ†
        joblib.dump({
            'config': self.config,
            'method_weights': self.method_weights,
            'article_count': len(self.articles_df) if self.articles_df is not None else 0
        }, f"{base_path}_config.pkl")
        
        logger.info("âœ… ØªÙ… Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
    
    def load_models(self, base_path: str):
        """ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        logger.info(f"ğŸ“‚ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† {base_path}")
        
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙƒÙˆÙŠÙ†
            config_data = joblib.load(f"{base_path}_config.pkl")
            self.config = config_data['config']
            self.method_weights = config_data['method_weights']
            
            # ØªØ­Ù…ÙŠÙ„ TF-IDF
            try:
                self.tfidf_extractor = joblib.load(f"{base_path}_tfidf.pkl")
            except:
                logger.warning("âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ TF-IDF")
            
            # ØªØ­Ù…ÙŠÙ„ Topic Model
            try:
                self.topic_model = joblib.load(f"{base_path}_topics.pkl")
            except:
                logger.warning("âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª")
            
            # ØªØ­Ù…ÙŠÙ„ Word2Vec
            try:
                self.word2vec_model = Word2VecContentModel(self.config)
                self.word2vec_model.word2vec_model = Word2Vec.load(f"{base_path}_word2vec.model")
                self.word2vec_model.article_vectors = joblib.load(f"{base_path}_article_vectors.pkl")
            except:
                logger.warning("âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Word2Vec")
            
            # ØªØ­Ù…ÙŠÙ„ BERT
            try:
                self.bert_extractor = BERTContentExtractor(self.config)
            except:
                logger.warning("âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BERT")
            
            logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {str(e)}")
            raise


# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙƒÙˆÙŠÙ†
    config = ContentFilteringConfig(
        n_topics=20,
        max_features=5000,
        topic_model_type="LDA"
    )
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©
    sample_articles = pd.DataFrame({
        'id': [f'article_{i}' for i in range(100)],
        'title': [f'Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ù‚Ø§Ù„ {i}' for i in range(100)],
        'content': [f'Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù‚Ø§Ù„ {i} Ù…Ø¹ Ù†Øµ Ø¹Ø±Ø¨ÙŠ Ù…ÙØµÙ„' for i in range(100)],
        'category': np.random.choice(['Ø³ÙŠØ§Ø³Ø©', 'Ø±ÙŠØ§Ø¶Ø©', 'Ø§Ù‚ØªØµØ§Ø¯', 'ØªÙ‚Ù†ÙŠØ©'], 100),
        'views': np.random.randint(100, 10000, 100)
    })
    
    # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    recommender = ContentBasedRecommender(config)
    results = recommender.train_all_models(sample_articles)
    
    print("ğŸ¯ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:", results)
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª
    user_articles = ['article_1', 'article_2', 'article_3']
    recommendations = recommender.get_content_recommendations(user_articles, n_recommendations=5)
    
    print("ğŸ“ Ø§Ù„ØªÙˆØµÙŠØ§Øª:", recommendations)
