# Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ† Ø§Ù„Ù…ØªÙƒÙŠÙ - Ø³Ø¨Ù‚ Ø§Ù„Ø°ÙƒÙŠØ©
# Adaptive Hybrid Recommendation System

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error, accuracy_score, precision_recall_fscore_support
from sklearn.model_selection import cross_val_score
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import logging
from typing import Dict, List, Tuple, Optional, Any, Union
from datetime import datetime, timedelta
from dataclasses import dataclass
import joblib
import pickle
import asyncio
from concurrent.futures import ThreadPoolExecutor
import threading
import time
from collections import defaultdict, deque
import json

from .collaborative_filtering import CollaborativeFilteringEnsemble, MatrixFactorizationModel, NeuralCollaborativeFiltering
from .content_based_filtering import ContentBasedRecommender, ContentFilteringConfig

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class HybridConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†"""
    # Meta-learning settings
    meta_learning_rate: float = 0.001
    meta_epochs: int = 100
    meta_batch_size: int = 256
    
    # Ensemble settings
    ensemble_methods: List[str] = None
    base_weights: Dict[str, float] = None
    weight_update_frequency: int = 100  # Ø¹Ø¯Ø¯ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª Ù‚Ø¨Ù„ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù†
    
    # Context awareness
    context_features: List[str] = None
    temporal_decay: float = 0.95  # ØªØ±Ø§Ø¬Ø¹ Ø²Ù…Ù†ÙŠ Ù„Ù„ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
    recency_window: int = 30  # Ø£ÙŠØ§Ù…
    
    # Cold start settings
    cold_start_threshold: int = 5  # Ø¹Ø¯Ø¯ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯
    new_item_threshold: int = 10  # Ø¹Ø¯Ø¯ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª Ù„Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯
    
    # Performance settings
    min_confidence: float = 0.1
    max_recommendations: int = 100
    cache_ttl: int = 300  # Ø«ÙˆØ§Ù†ÙŠ
    
    # A/B Testing
    enable_ab_testing: bool = True
    test_ratio: float = 0.1  # Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±

    def __post_init__(self):
        if self.ensemble_methods is None:
            self.ensemble_methods = ['collaborative', 'content', 'popularity', 'temporal']
        
        if self.base_weights is None:
            self.base_weights = {
                'collaborative': 0.4,
                'content': 0.3,
                'popularity': 0.2,
                'temporal': 0.1
            }
        
        if self.context_features is None:
            self.context_features = [
                'time_of_day', 'day_of_week', 'device_type', 
                'location', 'session_length', 'user_activity_level'
            ]


class UserProfileManager:
    """
    Ù…Ø¯ÙŠØ± Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
    Advanced User Profile Manager
    """
    
    def __init__(self, config: HybridConfig):
        self.config = config
        self.user_profiles = {}
        self.user_interactions = defaultdict(list)
        self.user_context_history = defaultdict(list)
        self.lock = threading.Lock()
    
    def update_user_profile(self, user_id: str, interaction_data: Dict[str, Any]):
        """ØªØ­Ø¯ÙŠØ« Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯"""
        with self.lock:
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯
            interaction_data['timestamp'] = datetime.now()
            self.user_interactions[user_id].append(interaction_data)
            
            # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø© ÙÙ‚Ø·
            cutoff_date = datetime.now() - timedelta(days=self.config.recency_window)
            self.user_interactions[user_id] = [
                interaction for interaction in self.user_interactions[user_id]
                if interaction['timestamp'] > cutoff_date
            ]
            
            # ØªØ­Ø¯ÙŠØ« Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            self._rebuild_user_profile(user_id)
    
    def _rebuild_user_profile(self, user_id: str):
        """Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        interactions = self.user_interactions[user_id]
        
        if not interactions:
            self.user_profiles[user_id] = self._get_default_profile()
            return
        
        profile = {
            'user_id': user_id,
            'total_interactions': len(interactions),
            'interaction_types': self._analyze_interaction_types(interactions),
            'category_preferences': self._analyze_category_preferences(interactions),
            'temporal_patterns': self._analyze_temporal_patterns(interactions),
            'engagement_level': self._calculate_engagement_level(interactions),
            'diversity_score': self._calculate_diversity_score(interactions),
            'recency_bias': self._calculate_recency_bias(interactions),
            'last_updated': datetime.now(),
            'profile_confidence': self._calculate_profile_confidence(interactions)
        }
        
        self.user_profiles[user_id] = profile
    
    def _analyze_interaction_types(self, interactions: List[Dict]) -> Dict[str, float]:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª"""
        type_counts = defaultdict(int)
        for interaction in interactions:
            type_counts[interaction.get('interaction_type', 'view')] += 1
        
        total = len(interactions)
        return {itype: count / total for itype, count in type_counts.items()}
    
    def _analyze_category_preferences(self, interactions: List[Dict]) -> Dict[str, float]:
        """ØªØ­Ù„ÙŠÙ„ ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„ÙØ¦Ø§Øª"""
        category_scores = defaultdict(float)
        total_weight = 0
        
        for interaction in interactions:
            category = interaction.get('category', 'other')
            weight = self._get_interaction_weight(interaction)
            category_scores[category] += weight
            total_weight += weight
        
        if total_weight > 0:
            return {cat: score / total_weight for cat, score in category_scores.items()}
        return {}
    
    def _analyze_temporal_patterns(self, interactions: List[Dict]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠØ©"""
        hours = []
        days = []
        
        for interaction in interactions:
            timestamp = interaction['timestamp']
            hours.append(timestamp.hour)
            days.append(timestamp.weekday())
        
        return {
            'preferred_hours': list(set(hours)),
            'active_days': list(set(days)),
            'peak_hour': max(set(hours), key=hours.count) if hours else 12,
            'peak_day': max(set(days), key=days.count) if days else 0
        }
    
    def _calculate_engagement_level(self, interactions: List[Dict]) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØ§Ø¹Ù„"""
        if not interactions:
            return 0.0
        
        engagement_weights = {
            'view': 1.0,
            'like': 3.0,
            'save': 4.0,
            'share': 5.0,
            'comment': 4.5
        }
        
        total_score = sum(
            engagement_weights.get(interaction.get('interaction_type', 'view'), 1.0)
            for interaction in interactions
        )
        
        return min(total_score / len(interactions), 5.0)
    
    def _calculate_diversity_score(self, interactions: List[Dict]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªÙ†ÙˆØ¹"""
        categories = [interaction.get('category', 'other') for interaction in interactions]
        unique_categories = len(set(categories))
        total_categories = len(categories)
        
        return unique_categories / max(total_categories, 1) if total_categories > 0 else 0.0
    
    def _calculate_recency_bias(self, interactions: List[Dict]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù†Ø­ÙŠØ§Ø² Ø§Ù„Ø­Ø¯Ø§Ø«Ø©"""
        if len(interactions) < 2:
            return 0.5
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ÙˆÙ‚Øª
        sorted_interactions = sorted(interactions, key=lambda x: x['timestamp'])
        
        # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„ÙØ¬ÙˆØ© Ø§Ù„Ø²Ù…Ù†ÙŠØ©
        time_gaps = []
        for i in range(1, len(sorted_interactions)):
            gap = (sorted_interactions[i]['timestamp'] - sorted_interactions[i-1]['timestamp']).total_seconds()
            time_gaps.append(gap)
        
        if not time_gaps:
            return 0.5
        
        avg_gap = np.mean(time_gaps)
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø§Ù†Ø­ÙŠØ§Ø² Ø¨ÙŠÙ† 0 Ùˆ 1
        return min(1.0, 3600 / max(avg_gap, 3600))  # Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø© ÙƒÙ…Ø±Ø¬Ø¹
    
    def _calculate_profile_confidence(self, interactions: List[Dict]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø«Ù‚Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø´Ø®ØµÙŠ"""
        if len(interactions) < self.config.cold_start_threshold:
            return 0.2
        
        # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø«Ù‚Ø©
        quantity_factor = min(len(interactions) / 50, 1.0)  # Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ø¯Ø¯
        diversity_factor = self._calculate_diversity_score(interactions)
        recency_factor = min(len([i for i in interactions 
                                if (datetime.now() - i['timestamp']).days <= 7]) / 10, 1.0)
        
        return (quantity_factor + diversity_factor + recency_factor) / 3
    
    def _get_interaction_weight(self, interaction: Dict) -> float:
        """Ø­Ø³Ø§Ø¨ ÙˆØ²Ù† Ø§Ù„ØªÙØ§Ø¹Ù„"""
        base_weights = {
            'view': 1.0,
            'like': 3.0,
            'save': 4.0,
            'share': 5.0,
            'comment': 4.5
        }
        
        base_weight = base_weights.get(interaction.get('interaction_type', 'view'), 1.0)
        
        # ØªØ·Ø¨ÙŠÙ‚ ØªØ±Ø§Ø¬Ø¹ Ø²Ù…Ù†ÙŠ
        days_ago = (datetime.now() - interaction['timestamp']).days
        time_decay = self.config.temporal_decay ** days_ago
        
        return base_weight * time_decay
    
    def _get_default_profile(self) -> Dict[str, Any]:
        """Ù…Ù„Ù Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ø¬Ø¯Ø¯"""
        return {
            'user_id': None,
            'total_interactions': 0,
            'interaction_types': {},
            'category_preferences': {},
            'temporal_patterns': {},
            'engagement_level': 0.0,
            'diversity_score': 0.0,
            'recency_bias': 0.5,
            'last_updated': datetime.now(),
            'profile_confidence': 0.0
        }
    
    def get_user_profile(self, user_id: str) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        if user_id not in self.user_profiles:
            return self._get_default_profile()
        return self.user_profiles[user_id].copy()
    
    def is_cold_start_user(self, user_id: str) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¬Ø¯ÙŠØ¯"""
        profile = self.get_user_profile(user_id)
        return profile['total_interactions'] < self.config.cold_start_threshold


class ContextualFeatureExtractor:
    """
    Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©
    Contextual Feature Extractor
    """
    
    def __init__(self, config: HybridConfig):
        self.config = config
        self.feature_history = defaultdict(list)
    
    def extract_context_features(self, user_id: str, request_context: Dict[str, Any]) -> np.ndarray:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©"""
        features = []
        
        # Ù…Ø¹Ø§Ù„Ù… Ø²Ù…Ù†ÙŠØ©
        now = datetime.now()
        features.extend([
            now.hour / 24.0,  # Ø³Ø§Ø¹Ø© Ø§Ù„ÙŠÙˆÙ…
            now.weekday() / 6.0,  # ÙŠÙˆÙ… Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹
            now.day / 31.0,  # ÙŠÙˆÙ… Ø§Ù„Ø´Ù‡Ø±
            (now.month - 1) / 11.0  # Ø§Ù„Ø´Ù‡Ø±
        ])
        
        # Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø¬Ù‡Ø§Ø²
        device_type = request_context.get('device_type', 'desktop')
        device_features = [
            1.0 if device_type == 'mobile' else 0.0,
            1.0 if device_type == 'tablet' else 0.0,
            1.0 if device_type == 'desktop' else 0.0
        ]
        features.extend(device_features)
        
        # Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ù…ÙˆÙ‚Ø¹ (Ù…Ø¨Ø³Ø·Ø©)
        location = request_context.get('location', {})
        features.extend([
            hash(location.get('country', 'unknown')) % 100 / 100.0,
            hash(location.get('city', 'unknown')) % 100 / 100.0
        ])
        
        # Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø¬Ù„Ø³Ø©
        features.extend([
            min(request_context.get('session_length', 0) / 3600, 1.0),  # Ø·ÙˆÙ„ Ø§Ù„Ø¬Ù„Ø³Ø© (Ø³Ø§Ø¹Ø§Øª)
            min(request_context.get('pages_visited', 0) / 20, 1.0),  # Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª
            request_context.get('is_returning_user', 0)
        ])
        
        # Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø­Ø¯ÙŠØ«
        recent_activity = self._get_recent_activity_features(user_id)
        features.extend(recent_activity)
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø«Ø§Ø¨Øª
        target_length = 20  # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        while len(features) < target_length:
            features.append(0.0)
        
        return np.array(features[:target_length])
    
    def _get_recent_activity_features(self, user_id: str) -> List[float]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø­Ø¯ÙŠØ«"""
        # ÙŠÙ…ÙƒÙ† ØªÙˆØ³ÙŠØ¹Ù‡Ø§ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø­Ø¯ÙŠØ«
        return [
            0.5,  # Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù†Ø´Ø§Ø· ÙÙŠ Ø¢Ø®Ø± Ø³Ø§Ø¹Ø©
            0.3,  # Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù†Ø´Ø§Ø· ÙÙŠ Ø¢Ø®Ø± ÙŠÙˆÙ…
            0.7,  # Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù†Ø´Ø§Ø· ÙÙŠ Ø¢Ø®Ø± Ø£Ø³Ø¨ÙˆØ¹
            0.1   # Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ù…Ù‚Ø§Ø±Ù† Ø¨Ø§Ù„Ù…ØªÙˆØ³Ø·
        ]


class AdaptiveWeightingModule:
    """
    ÙˆØ­Ø¯Ø© Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ù…ØªÙƒÙŠÙ
    Adaptive Weighting Module
    """
    
    def __init__(self, config: HybridConfig):
        self.config = config
        self.method_weights = config.base_weights.copy()
        self.performance_history = defaultdict(list)
        self.weight_model = None
        self.feature_extractor = ContextualFeatureExtractor(config)
        self._build_weight_model()
    
    def _build_weight_model(self):
        """Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆØ²ÙŠÙ† Ø§Ù„Ù…ØªÙƒÙŠÙ"""
        input_dim = 20  # Ø­Ø¬Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©
        n_methods = len(self.config.ensemble_methods)
        
        self.weight_model = keras.Sequential([
            layers.Dense(64, activation='relu', input_dim=input_dim),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(16, activation='relu'),
            layers.Dense(n_methods, activation='softmax')
        ])
        
        self.weight_model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
    
    def get_adaptive_weights(self, user_id: str, context: Dict[str, Any]) -> Dict[str, float]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ØªÙƒÙŠÙØ©"""
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©
        context_features = self.feature_extractor.extract_context_features(user_id, context)
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        if self.weight_model is not None:
            try:
                predicted_weights = self.weight_model.predict(
                    context_features.reshape(1, -1), 
                    verbose=0
                )[0]
                
                # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù‚Ø§Ù…ÙˆØ³
                weights = {}
                for i, method in enumerate(self.config.ensemble_methods):
                    weights[method] = float(predicted_weights[i])
                
                return weights
            except:
                logger.warning("âš ï¸ ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø£ÙˆØ²Ø§Ù†ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©")
        
        return self.method_weights.copy()
    
    def update_weights_from_feedback(self, user_id: str, context: Dict[str, Any],
                                   method_scores: Dict[str, float], 
                                   actual_feedback: float):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©"""
        # Ø­ÙØ¸ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù„ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø©
        for method, score in method_scores.items():
            self.performance_history[method].append({
                'predicted_score': score,
                'actual_feedback': actual_feedback,
                'context': context,
                'timestamp': datetime.now()
            })
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¥Ø°Ø§ ØªÙˆÙØ±Øª Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©
        if sum(len(history) for history in self.performance_history.values()) >= self.config.weight_update_frequency:
            self._retrain_weight_model()
    
    def _retrain_weight_model(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙˆØ²Ø§Ù†"""
        logger.info("ğŸ”„ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ØªÙƒÙŠÙØ©...")
        
        try:
            # ØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            X, y = self._prepare_weight_training_data()
            
            if len(X) > 50:  # ØªØ¯Ø±ÙŠØ¨ ÙÙ‚Ø· Ø¥Ø°Ø§ ØªÙˆÙØ±Øª Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©
                self.weight_model.fit(
                    X, y,
                    epochs=10,
                    batch_size=32,
                    verbose=0,
                    validation_split=0.2
                )
                logger.info("âœ… ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙˆØ²Ø§Ù†")
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙˆØ²Ø§Ù†: {str(e)}")
    
    def _prepare_weight_training_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """ØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙˆØ²Ø§Ù†"""
        X = []
        y = []
        
        # Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† ØªØ§Ø±ÙŠØ® Ø§Ù„Ø£Ø¯Ø§Ø¡
        for method, history in self.performance_history.items():
            for record in history[-100:]:  # Ø¢Ø®Ø± 100 Ø³Ø¬Ù„
                context_features = self.feature_extractor.extract_context_features(
                    'dummy_user', record['context']
                )
                X.append(context_features)
                
                # Ø¥Ù†Ø´Ø§Ø¡ ØªØ³Ù…ÙŠØ© Ø§Ù„Ù‡Ø¯Ù (Ø£ÙØ¶Ù„ Ø£Ø¯Ø§Ø¡ ÙŠØ­ØµÙ„ Ø¹Ù„Ù‰ ÙˆØ²Ù† Ø£Ø¹Ù„Ù‰)
                performance_score = abs(record['actual_feedback'] - record['predicted_score'])
                
                # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªÙˆØ²ÙŠØ¹ Ø£ÙˆØ²Ø§Ù†
                method_index = self.config.ensemble_methods.index(method)
                target_weights = np.zeros(len(self.config.ensemble_methods))
                target_weights[method_index] = 1.0 - performance_score  # ÙƒÙ„Ù…Ø§ Ù‚Ù„ Ø§Ù„Ø®Ø·Ø£ØŒ Ø²Ø§Ø¯ Ø§Ù„ÙˆØ²Ù†
                
                y.append(target_weights)
        
        return np.array(X), np.array(y)


class HybridRecommendationEngine:
    """
    Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ† Ø§Ù„Ù…ØªÙƒÙŠÙ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    Main Adaptive Hybrid Recommendation Engine
    """
    
    def __init__(self, config: HybridConfig):
        self.config = config
        self.collaborative_model = None
        self.content_model = None
        self.user_profile_manager = UserProfileManager(config)
        self.adaptive_weighting = AdaptiveWeightingModule(config)
        self.popularity_scores = {}
        self.recommendation_cache = {}
        self.performance_metrics = defaultdict(list)
        self.ab_test_groups = {}
        
        # Ù‚ÙÙ„ Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©
        self.lock = threading.Lock()
    
    def set_collaborative_model(self, model: CollaborativeFilteringEnsemble):
        """ØªØ¹ÙŠÙŠÙ† Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„ØªØ¹Ø§ÙˆÙ†ÙŠØ©"""
        self.collaborative_model = model
        logger.info("âœ… ØªÙ… ØªØ¹ÙŠÙŠÙ† Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„ØªØ¹Ø§ÙˆÙ†ÙŠØ©")
    
    def set_content_model(self, model: ContentBasedRecommender):
        """ØªØ¹ÙŠÙŠÙ† Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆØ§Ø¦ÙŠØ©"""
        self.content_model = model
        logger.info("âœ… ØªÙ… ØªØ¹ÙŠÙŠÙ† Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆØ§Ø¦ÙŠØ©")
    
    def update_popularity_scores(self, articles_df: pd.DataFrame):
        """ØªØ­Ø¯ÙŠØ« Ù†Ù‚Ø§Ø· Ø§Ù„Ø´Ø¹Ø¨ÙŠØ©"""
        logger.info("ğŸ“Š ØªØ­Ø¯ÙŠØ« Ù†Ù‚Ø§Ø· Ø§Ù„Ø´Ø¹Ø¨ÙŠØ©...")
        
        # Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ø´Ø¹Ø¨ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª
        for _, article in articles_df.iterrows():
            score = 0.0
            
            # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø´Ø¹Ø¨ÙŠØ©
            views = article.get('views', 0)
            likes = article.get('likes', 0)
            shares = article.get('shares', 0)
            comments = article.get('comments', 0)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ø±Ø¬Ø­Ø©
            score = (
                views * 1.0 +
                likes * 3.0 +
                shares * 5.0 +
                comments * 4.0
            )
            
            # ØªØ·Ø¨ÙŠÙ‚ ØªØ±Ø§Ø¬Ø¹ Ø²Ù…Ù†ÙŠ
            if 'created_at' in article:
                days_old = (datetime.now() - pd.to_datetime(article['created_at'])).days
                time_decay = self.config.temporal_decay ** days_old
                score *= time_decay
            
            self.popularity_scores[article['id']] = score
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Ù‚Ø§Ø·
        if self.popularity_scores:
            max_score = max(self.popularity_scores.values())
            if max_score > 0:
                for article_id in self.popularity_scores:
                    self.popularity_scores[article_id] /= max_score
        
        logger.info(f"âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ù†Ù‚Ø§Ø· Ø§Ù„Ø´Ø¹Ø¨ÙŠØ© Ù„Ù€ {len(self.popularity_scores)} Ù…Ù‚Ø§Ù„")
    
    def get_hybrid_recommendations(self, user_id: str, context: Dict[str, Any],
                                 n_recommendations: int = 10,
                                 exclude_articles: Optional[List[str]] = None) -> List[Tuple[str, float, Dict[str, Any]]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø©"""
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        cache_key = f"{user_id}_{hash(str(context))}_{n_recommendations}"
        if cache_key in self.recommendation_cache:
            cached_result, timestamp = self.recommendation_cache[cache_key]
            if (datetime.now() - timestamp).seconds < self.config.cache_ttl:
                return cached_result
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        user_profile = self.user_profile_manager.get_user_profile(user_id)
        is_cold_start = self.user_profile_manager.is_cold_start_user(user_id)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ØªÙƒÙŠÙØ©
        adaptive_weights = self.adaptive_weighting.get_adaptive_weights(user_id, context)
        
        # Ø¬Ù…Ø¹ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ù…Ù† Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        all_recommendations = defaultdict(lambda: {'score': 0.0, 'methods': {}, 'confidence': 0.0})
        total_weight = 0.0
        
        # Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„ØªØ¹Ø§ÙˆÙ†ÙŠØ©
        if self.collaborative_model and not is_cold_start:
            try:
                collab_recs = self._get_collaborative_recommendations(
                    user_id, n_recommendations * 2, exclude_articles
                )
                weight = adaptive_weights.get('collaborative', 0.0)
                self._merge_recommendations(all_recommendations, collab_recs, 
                                         weight, 'collaborative')
                total_weight += weight
            except Exception as e:
                logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª ØªØ¹Ø§ÙˆÙ†ÙŠØ©: {str(e)}")
        
        # Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…Ø­ØªÙˆØ§Ø¦ÙŠØ©
        if self.content_model:
            try:
                content_recs = self._get_content_recommendations(
                    user_id, user_profile, n_recommendations * 2, exclude_articles
                )
                weight = adaptive_weights.get('content', 0.0)
                self._merge_recommendations(all_recommendations, content_recs,
                                         weight, 'content')
                total_weight += weight
            except Exception as e:
                logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Ù…Ø­ØªÙˆØ§Ø¦ÙŠØ©: {str(e)}")
        
        # ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø´Ø¹Ø¨ÙŠØ©
        try:
            popularity_recs = self._get_popularity_recommendations(
                n_recommendations * 2, exclude_articles
            )
            weight = adaptive_weights.get('popularity', 0.0)
            self._merge_recommendations(all_recommendations, popularity_recs,
                                     weight, 'popularity')
            total_weight += weight
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø´Ø¹Ø¨ÙŠØ©: {str(e)}")
        
        # ØªÙˆØµÙŠØ§Øª Ø²Ù…Ù†ÙŠØ© (Ø§Ù„Ø­Ø¯Ø§Ø«Ø©)
        try:
            temporal_recs = self._get_temporal_recommendations(
                context, n_recommendations * 2, exclude_articles
            )
            weight = adaptive_weights.get('temporal', 0.0)
            self._merge_recommendations(all_recommendations, temporal_recs,
                                     weight, 'temporal')
            total_weight += weight
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Ø²Ù…Ù†ÙŠØ©: {str(e)}")
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Ù‚Ø§Ø· ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø©
        final_recommendations = []
        for article_id, rec_data in all_recommendations.items():
            if total_weight > 0:
                normalized_score = rec_data['score'] / total_weight
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø©
                confidence = len(rec_data['methods']) / len(self.config.ensemble_methods)
                
                # Ø¥Ø¶Ø§ÙØ© Ø¹ÙˆØ§Ù…Ù„ Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ø«Ù‚Ø©
                confidence *= user_profile.get('profile_confidence', 0.5)
                
                if normalized_score >= self.config.min_confidence:
                    final_recommendations.append((
                        article_id,
                        normalized_score,
                        {
                            'methods': rec_data['methods'],
                            'confidence': confidence,
                            'weights_used': adaptive_weights
                        }
                    ))
        
        # ØªØ±ØªÙŠØ¨ ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        final_recommendations.sort(key=lambda x: x[1], reverse=True)
        final_recommendations = final_recommendations[:n_recommendations]
        
        # ØªØ·Ø¨ÙŠÙ‚ ØªÙ†ÙˆÙŠØ¹ Ø¥Ø¶Ø§ÙÙŠ
        final_recommendations = self._apply_diversification(
            final_recommendations, user_profile
        )
        
        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        self.recommendation_cache[cache_key] = (final_recommendations, datetime.now())
        
        logger.info(f"ğŸ¯ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(final_recommendations)} ØªÙˆØµÙŠØ© Ù‡Ø¬ÙŠÙ†Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id}")
        
        return final_recommendations
    
    def _get_collaborative_recommendations(self, user_id: str, n_recs: int,
                                        exclude_articles: Optional[List[str]]) -> List[Tuple[str, float]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª ØªØ¹Ø§ÙˆÙ†ÙŠØ©"""
        if not self.collaborative_model:
            return []
        
        seen_items = exclude_articles or []
        return self.collaborative_model.get_ensemble_recommendations(
            user_id=user_id,
            n_recommendations=n_recs,
            exclude_seen=True,
            seen_items=seen_items
        )
    
    def _get_content_recommendations(self, user_id: str, user_profile: Dict[str, Any],
                                   n_recs: int, exclude_articles: Optional[List[str]]) -> List[Tuple[str, float]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Ù…Ø­ØªÙˆØ§Ø¦ÙŠØ©"""
        if not self.content_model:
            return []
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙˆØµÙŠØ§Øª
        user_articles = []
        if 'liked_articles' in user_profile:
            user_articles = user_profile['liked_articles']
        
        return self.content_model.get_content_recommendations(
            user_articles=user_articles,
            n_recommendations=n_recs,
            exclude_articles=exclude_articles
        )
    
    def _get_popularity_recommendations(self, n_recs: int,
                                     exclude_articles: Optional[List[str]]) -> List[Tuple[str, float]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø´Ø¹Ø¨ÙŠØ©"""
        exclude_set = set(exclude_articles or [])
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø´Ø¹Ø¨ÙŠØ©
        popular_articles = [
            (article_id, score) for article_id, score in self.popularity_scores.items()
            if article_id not in exclude_set
        ]
        
        popular_articles.sort(key=lambda x: x[1], reverse=True)
        return popular_articles[:n_recs]
    
    def _get_temporal_recommendations(self, context: Dict[str, Any], n_recs: int,
                                    exclude_articles: Optional[List[str]]) -> List[Tuple[str, float]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Ø²Ù…Ù†ÙŠØ© (Ø­Ø¯Ø§Ø«Ø©)"""
        exclude_set = set(exclude_articles or [])
        
        # ØªÙØ¶ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚
        current_hour = datetime.now().hour
        
        # ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† Ù‡Ø°Ø§ Ù„ÙŠØ£Ø®Ø° ÙÙŠ Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø± Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ù†Ø´Ø± ÙˆØ§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª
        temporal_scores = {}
        for article_id, popularity_score in self.popularity_scores.items():
            if article_id not in exclude_set:
                # ØªØ·Ø¨ÙŠÙ‚ Ø¹Ø§Ù…Ù„ Ø²Ù…Ù†ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ
                temporal_factor = 1.0
                if 6 <= current_hour <= 12:  # ØµØ¨Ø§Ø­Ø§Ù‹ - ØªÙØ¶ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
                    temporal_factor = 1.2
                elif 18 <= current_hour <= 23:  # Ù…Ø³Ø§Ø¡Ù‹ - ØªÙØ¶ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ±ÙÙŠÙ‡ÙŠ
                    temporal_factor = 1.1
                
                temporal_scores[article_id] = popularity_score * temporal_factor
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠØ©
        temporal_recs = sorted(temporal_scores.items(), key=lambda x: x[1], reverse=True)
        return temporal_recs[:n_recs]
    
    def _merge_recommendations(self, all_recs: Dict, new_recs: List[Tuple[str, float]],
                             weight: float, method_name: str):
        """Ø¯Ù…Ø¬ ØªÙˆØµÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¹ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©"""
        for article_id, score in new_recs:
            all_recs[article_id]['score'] += score * weight
            all_recs[article_id]['methods'][method_name] = score * weight
    
    def _apply_diversification(self, recommendations: List[Tuple[str, float, Dict]],
                             user_profile: Dict[str, Any]) -> List[Tuple[str, float, Dict]]:
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ†ÙˆÙŠØ¹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØµÙŠØ§Øª"""
        if len(recommendations) <= 3:
            return recommendations
        
        # ØªÙ†ÙˆÙŠØ¹ Ø¨Ø³ÙŠØ· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙØ¦Ø§Øª (Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªØ§Ø­Ø©)
        diversified = []
        used_categories = set()
        
        # Ø¥Ø¶Ø§ÙØ© Ø£ÙØ¶Ù„ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø£ÙˆÙ„Ø§Ù‹
        for rec in recommendations[:3]:
            diversified.append(rec)
            # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ù…Ù†Ø·Ù‚ Ù„ØªØªØ¨Ø¹ Ø§Ù„ÙØ¦Ø§Øª
        
        # Ø¥Ø¶Ø§ÙØ© Ø¨Ø§Ù‚ÙŠ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„ØªÙ†ÙˆÙŠØ¹
        for rec in recommendations[3:]:
            # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ù…Ù†Ø·Ù‚ ØªÙ†ÙˆÙŠØ¹ Ø£ÙƒØ«Ø± ØªØ·ÙˆØ±Ø§Ù‹ Ù‡Ù†Ø§
            diversified.append(rec)
            if len(diversified) >= len(recommendations):
                break
        
        return diversified
    
    def record_user_feedback(self, user_id: str, article_id: str, 
                           feedback_type: str, feedback_value: float,
                           context: Dict[str, Any]):
        """ØªØ³Ø¬ÙŠÙ„ ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        # ØªØ­Ø¯ÙŠØ« Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        interaction_data = {
            'article_id': article_id,
            'interaction_type': feedback_type,
            'rating': feedback_value,
            'context': context
        }
        
        self.user_profile_manager.update_user_profile(user_id, interaction_data)
        
        # ØªØ­Ø¯ÙŠØ« Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        # (ÙŠØªØ·Ù„Ø¨ ØªØªØ¨Ø¹ Ø£ÙŠ Ø·Ø±ÙŠÙ‚Ø© Ø£ÙˆØµØª Ø¨Ù‡Ø°Ø§ Ø§Ù„Ù…Ù‚Ø§Ù„)
        
        logger.info(f"ğŸ“ ØªÙ… ØªØ³Ø¬ÙŠÙ„ ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø©: {user_id} -> {article_id} ({feedback_type}: {feedback_value})")
    
    def get_explanation(self, user_id: str, article_id: str, 
                       recommendation_data: Dict[str, Any]) -> Dict[str, Any]:
        """Ø´Ø±Ø­ Ø³Ø¨Ø¨ Ø§Ù„ØªÙˆØµÙŠØ©"""
        explanation = {
            'article_id': article_id,
            'user_id': user_id,
            'overall_confidence': recommendation_data.get('confidence', 0.0),
            'methods_contribution': recommendation_data.get('methods', {}),
            'weights_used': recommendation_data.get('weights_used', {}),
            'user_profile_factors': [],
            'contextual_factors': []
        }
        
        # Ø¥Ø¶Ø§ÙØ© Ø¹ÙˆØ§Ù…Ù„ Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        user_profile = self.user_profile_manager.get_user_profile(user_id)
        if user_profile['category_preferences']:
            explanation['user_profile_factors'].append({
                'factor': 'category_preferences',
                'value': user_profile['category_preferences']
            })
        
        # Ø¥Ø¶Ø§ÙØ© Ø¹ÙˆØ§Ù…Ù„ Ø³ÙŠØ§Ù‚ÙŠØ©
        explanation['contextual_factors'].append({
            'factor': 'recommendation_time',
            'value': datetime.now().isoformat()
        })
        
        return explanation
    
    def optimize_weights(self, validation_interactions: List[Dict[str, Any]]):
        """ØªØ­Ø³ÙŠÙ† Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©"""
        logger.info("ğŸ¯ ØªØ­Ø³ÙŠÙ† Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†...")
        
        # ÙŠÙ…ÙƒÙ† ØªØ·Ø¨ÙŠÙ‚ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© ØªØ­Ø³ÙŠÙ† Ù…ØªÙ‚Ø¯Ù…Ø© Ù‡Ù†Ø§
        # Ù„Ù„Ø¨Ø³Ø§Ø·Ø©ØŒ Ø³Ù†Ø³ØªØ®Ø¯Ù… ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø¨Ø³ÙŠØ·
        
        method_performance = defaultdict(list)
        
        for interaction in validation_interactions:
            user_id = interaction['user_id']
            article_id = interaction['article_id']
            actual_rating = interaction.get('rating', 1.0)
            
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„ØªÙˆØµÙŠØ§Øª Ù…Ù† ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø©
            context = interaction.get('context', {})
            
            try:
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Ù…Ù† ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø© Ù…Ù†ÙØµÙ„Ø©
                if self.collaborative_model:
                    collab_recs = self._get_collaborative_recommendations(user_id, 20, [])
                    collab_score = next((score for aid, score in collab_recs if aid == article_id), 0.0)
                    method_performance['collaborative'].append(abs(actual_rating - collab_score))
                
                if self.content_model:
                    user_profile = self.user_profile_manager.get_user_profile(user_id)
                    content_recs = self._get_content_recommendations(user_id, user_profile, 20, [])
                    content_score = next((score for aid, score in content_recs if aid == article_id), 0.0)
                    method_performance['content'].append(abs(actual_rating - content_score))
                
                # Ù†ÙØ³ Ø§Ù„Ø´ÙŠØ¡ Ù„Ù„Ø·Ø±Ù‚ Ø§Ù„Ø£Ø®Ø±Ù‰...
                
            except Exception as e:
                logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡: {str(e)}")
                continue
        
        # Ø­Ø³Ø§Ø¨ Ø£ÙˆØ²Ø§Ù† Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡
        new_weights = {}
        total_inverse_error = 0
        
        for method, errors in method_performance.items():
            if errors:
                avg_error = np.mean(errors)
                inverse_error = 1.0 / (1.0 + avg_error)
                new_weights[method] = inverse_error
                total_inverse_error += inverse_error
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        if total_inverse_error > 0:
            for method in new_weights:
                new_weights[method] /= total_inverse_error
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            self.config.base_weights.update(new_weights)
            logger.info(f"âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù†: {new_weights}")
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        return {
            'total_recommendations': sum(len(metrics) for metrics in self.performance_metrics.values()),
            'cache_hit_rate': len(self.recommendation_cache) / max(1, len(self.performance_metrics)),
            'active_users': len(self.user_profile_manager.user_profiles),
            'method_weights': self.config.base_weights.copy(),
            'last_update': datetime.now().isoformat()
        }
    
    def save_model(self, filepath: str):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†"""
        logger.info(f"ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ† ÙÙŠ {filepath}")
        
        model_data = {
            'config': self.config,
            'user_profiles': dict(self.user_profile_manager.user_profiles),
            'popularity_scores': self.popularity_scores,
            'method_weights': self.config.base_weights,
            'performance_metrics': dict(self.performance_metrics),
            'save_timestamp': datetime.now().isoformat()
        }
        
        with open(f"{filepath}_hybrid_system.pkl", 'wb') as f:
            pickle.dump(model_data, f)
        
        # Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ØªÙƒÙŠÙØ©
        if self.adaptive_weighting.weight_model:
            self.adaptive_weighting.weight_model.save(f"{filepath}_weight_model.h5")
        
        logger.info("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†")
    
    def load_model(self, filepath: str):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†"""
        logger.info(f"ğŸ“‚ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ† Ù…Ù† {filepath}")
        
        try:
            with open(f"{filepath}_hybrid_system.pkl", 'rb') as f:
                model_data = pickle.load(f)
            
            self.config = model_data['config']
            self.user_profile_manager.user_profiles = model_data['user_profiles']
            self.popularity_scores = model_data['popularity_scores']
            self.performance_metrics = defaultdict(list, model_data['performance_metrics'])
            
            # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ØªÙƒÙŠÙØ©
            try:
                self.adaptive_weighting.weight_model = keras.models.load_model(
                    f"{filepath}_weight_model.h5"
                )
            except:
                logger.warning("âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ØªÙƒÙŠÙØ©")
            
            logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†")
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†: {str(e)}")
            raise


# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙƒÙˆÙŠÙ†
    config = HybridConfig(
        ensemble_methods=['collaborative', 'content', 'popularity', 'temporal'],
        base_weights={
            'collaborative': 0.4,
            'content': 0.3,
            'popularity': 0.2,
            'temporal': 0.1
        }
    )
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†
    hybrid_engine = HybridRecommendationEngine(config)
    
    # Ù…Ø­Ø§ÙƒØ§Ø© ØªØ­Ø¯ÙŠØ« Ù†Ù‚Ø§Ø· Ø§Ù„Ø´Ø¹Ø¨ÙŠØ©
    sample_articles = pd.DataFrame({
        'id': [f'article_{i}' for i in range(100)],
        'views': np.random.randint(100, 10000, 100),
        'likes': np.random.randint(0, 500, 100),
        'shares': np.random.randint(0, 100, 100),
        'comments': np.random.randint(0, 50, 100),
        'created_at': pd.date_range('2024-01-01', periods=100, freq='H')
    })
    
    hybrid_engine.update_popularity_scores(sample_articles)
    
    # Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª
    context = {
        'device_type': 'mobile',
        'location': {'country': 'SA', 'city': 'Riyadh'},
        'session_length': 1800,
        'is_returning_user': True
    }
    
    recommendations = hybrid_engine.get_hybrid_recommendations(
        user_id='user_123',
        context=context,
        n_recommendations=5
    )
    
    print("ğŸ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø©:")
    for article_id, score, metadata in recommendations:
        print(f"  {article_id}: {score:.3f} (Ø«Ù‚Ø©: {metadata['confidence']:.3f})")
    
    # ØªØ³Ø¬ÙŠÙ„ ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø©
    hybrid_engine.record_user_feedback(
        user_id='user_123',
        article_id='article_1',
        feedback_type='like',
        feedback_value=1.0,
        context=context
    )
