# Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„ØªÙƒÙŠÙÙŠ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø²Ø§Ø¬ ÙˆØ§Ù„Ø³ÙŠØ§Ù‚ - Ø³Ø¨Ù‚ Ø§Ù„Ø°ÙƒÙŠØ©
# Adaptive Contextual and Mood-Based Recommendation Engine

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestRegressor
import logging
from typing import Dict, List, Tuple, Optional, Any, Union
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from collections import defaultdict, deque
from enum import Enum
import json
import joblib
import pickle
import asyncio
from concurrent.futures import ThreadPoolExecutor
import threading
import time
import math
import random

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MoodState(Enum):
    """Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø²Ø§Ø¬"""
    POSITIVE = "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"
    NEUTRAL = "Ù…Ø­Ø§ÙŠØ¯"
    NEGATIVE = "Ø³Ù„Ø¨ÙŠ"
    CURIOUS = "ÙØ¶ÙˆÙ„ÙŠ"
    RELAXED = "Ù…Ø³ØªØ±Ø®ÙŠ"
    ENERGETIC = "Ù†Ø´ÙŠØ·"
    FOCUSED = "Ù…Ø±ÙƒØ²"
    EXPLORATORY = "Ø§Ø³ØªÙƒØ´Ø§ÙÙŠ"
    NOSTALGIC = "Ø­Ù†ÙŠÙ†"
    ANALYTICAL = "ØªØ­Ù„ÙŠÙ„ÙŠ"

class ContextType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚"""
    TEMPORAL = "Ø²Ù…Ù†ÙŠ"          # Ø§Ù„ÙˆÙ‚Øª Ù…Ù† Ø§Ù„ÙŠÙˆÙ…ØŒ Ø§Ù„ÙŠÙˆÙ… Ù…Ù† Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹
    ENVIRONMENTAL = "Ø¨ÙŠØ¦ÙŠ"      # Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ Ø§Ù„Ø·Ù‚Ø³ØŒ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡
    SOCIAL = "Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ"         # Ù…Ø¹ Ø£ØµØ¯Ù‚Ø§Ø¡ØŒ ÙˆØ­ÙŠØ¯ØŒ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø©
    ACTIVITY = "Ù†Ø´Ø§Ø·"          # Ø§Ù„Ø¹Ù…Ù„ØŒ Ø§Ù„Ø±Ø§Ø­Ø©ØŒ Ø§Ù„Ø³ÙØ±
    DEVICE = "Ø¬Ù‡Ø§Ø²"           # Ù…ÙˆØ¨Ø§ÙŠÙ„ØŒ Ø­Ø§Ø³ÙˆØ¨ØŒ ØªØ§Ø¨Ù„Øª
    BEHAVIORAL = "Ø³Ù„ÙˆÙƒÙŠ"       # Ù†Ù…Ø· Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø­Ø§Ù„ÙŠ

@dataclass
class ContextualFeatures:
    """Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø³ÙŠØ§Ù‚"""
    # Temporal features
    hour_of_day: float = 0.0
    day_of_week: float = 0.0
    is_weekend: bool = False
    is_holiday: bool = False
    time_since_last_visit: float = 0.0
    
    # Environmental features
    location_type: str = "home"  # home, work, public, travel
    weather_condition: str = "clear"  # clear, rain, snow, cloudy
    ambient_light: float = 1.0  # 0.0 (dark) to 1.0 (bright)
    noise_level: float = 0.5  # 0.0 (silent) to 1.0 (loud)
    
    # Social features
    social_context: str = "alone"  # alone, with_friends, with_family, in_group
    social_activity: bool = False
    
    # Activity features
    current_activity: str = "browsing"  # working, relaxing, commuting, browsing
    multitasking: bool = False
    urgency_level: float = 0.5  # 0.0 (no rush) to 1.0 (urgent)
    
    # Device features
    device_type: str = "mobile"  # mobile, tablet, desktop
    connection_type: str = "wifi"  # wifi, mobile, slow
    battery_level: float = 1.0  # 0.0 to 1.0
    screen_size: str = "medium"  # small, medium, large
    
    # Behavioral features
    session_length: float = 0.0  # minutes
    pages_visited: int = 0
    scroll_speed: float = 1.0  # relative speed
    interaction_rate: float = 0.5  # interactions per minute
    
    # Mood indicators
    reading_speed: float = 1.0  # relative to user's average
    click_precision: float = 1.0  # accuracy of clicks
    time_between_actions: float = 1.0  # relative timing
    content_engagement: float = 0.5  # depth of engagement

@dataclass
class MoodIndicators:
    """Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø²Ø§Ø¬"""
    # Behavioral indicators
    activity_level: float = 0.5  # 0.0 (passive) to 1.0 (active)
    exploration_tendency: float = 0.5  # tendency to explore new content
    focus_level: float = 0.5  # ability to focus on content
    patience_level: float = 0.5  # tolerance for long content
    
    # Interaction patterns
    interaction_frequency: float = 0.5
    content_completion_rate: float = 0.5
    sharing_propensity: float = 0.5
    commenting_activity: float = 0.5
    
    # Content preferences (mood-based)
    positive_content_preference: float = 0.5
    complex_content_tolerance: float = 0.5
    visual_content_preference: float = 0.5
    interactive_content_preference: float = 0.5
    
    # Temporal patterns
    consistency_with_routine: float = 0.5
    spontaneity_level: float = 0.5

@dataclass
class ContextualConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ"""
    # Mood detection
    mood_detection_window: int = 10  # minutes
    mood_smoothing_factor: float = 0.7
    min_interactions_for_mood: int = 3
    
    # Context analysis
    context_update_frequency: int = 30  # seconds
    context_history_length: int = 100
    context_similarity_threshold: float = 0.8
    
    # Adaptation parameters
    adaptation_learning_rate: float = 0.01
    context_weight_decay: float = 0.95
    mood_weight_decay: float = 0.9
    
    # Recommendation tuning
    context_influence_factor: float = 0.3
    mood_influence_factor: float = 0.4
    base_recommendation_weight: float = 0.3
    
    # Content matching
    mood_content_mapping: Dict[str, List[str]] = field(default_factory=dict)
    context_content_preferences: Dict[str, Dict[str, float]] = field(default_factory=dict)
    
    def __post_init__(self):
        if not self.mood_content_mapping:
            self.mood_content_mapping = {
                MoodState.POSITIVE.value: ["Ø¥ÙŠØ¬Ø§Ø¨ÙŠ", "Ù…Ù„Ù‡Ù…", "Ù†Ø¬Ø§Ø­", "Ø£Ù…Ù„", "ÙØ±Ø­"],
                MoodState.NEGATIVE.value: ["Ø¯Ø¹Ù…", "Ø­Ù„ÙˆÙ„", "ØªØ­ÙÙŠØ²", "ØªØ·ÙˆÙŠØ±"],
                MoodState.CURIOUS.value: ["Ø¹Ù„Ù…ÙŠ", "ØªÙ‚Ù†ÙŠ", "Ø§ÙƒØªØ´Ø§Ù", "Ù…Ø¹Ø±ÙØ©"],
                MoodState.RELAXED.value: ["ØªØ±ÙÙŠÙ‡", "ÙÙ†", "Ø·Ø¨ÙŠØ¹Ø©", "Ù‡Ø¯ÙˆØ¡"],
                MoodState.ENERGETIC.value: ["Ø±ÙŠØ§Ø¶Ø©", "Ø­Ù…Ø§Ø³", "Ù†Ø´Ø§Ø·", "ØªØ­Ø¯ÙŠ"],
                MoodState.FOCUSED.value: ["ØªØ­Ù„ÙŠÙ„", "Ø¹Ù…Ù‚", "Ø¯Ø±Ø§Ø³Ø©", "ØªØ±ÙƒÙŠØ²"],
                MoodState.EXPLORATORY.value: ["Ø¬Ø¯ÙŠØ¯", "Ù…ØªÙ†ÙˆØ¹", "ØºØ±ÙŠØ¨", "Ù…Ø«ÙŠØ±"],
                MoodState.NOSTALGIC.value: ["ØªØ§Ø±ÙŠØ®", "Ø°ÙƒØ±ÙŠØ§Øª", "ØªØ±Ø§Ø«", "Ù…Ø§Ø¶ÙŠ"],
                MoodState.ANALYTICAL.value: ["Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª", "Ø¨Ø­Ø«", "ØªØ­Ù„ÙŠÙ„", "Ø¯Ø±Ø§Ø³Ø©"]
            }
        
        if not self.context_content_preferences:
            self.context_content_preferences = {
                "morning": {"news": 0.8, "analysis": 0.6, "light": 0.4},
                "afternoon": {"work": 0.7, "professional": 0.8, "quick": 0.6},
                "evening": {"entertainment": 0.8, "deep": 0.7, "visual": 0.6},
                "night": {"light": 0.8, "short": 0.7, "relaxing": 0.8},
                "weekend": {"entertainment": 0.9, "deep": 0.8, "visual": 0.7},
                "work": {"professional": 0.9, "quick": 0.8, "relevant": 0.7},
                "mobile": {"short": 0.8, "visual": 0.7, "quick": 0.9},
                "desktop": {"long": 0.7, "detailed": 0.8, "complex": 0.6}
            }


class MoodDetector:
    """
    ÙƒØ§Ø´Ù Ø§Ù„Ù…Ø²Ø§Ø¬ Ù…Ù† Ø§Ù„Ø³Ù„ÙˆÙƒ
    Mood Detector from Behavior
    """
    
    def __init__(self, config: ContextualConfig):
        self.config = config
        self.interaction_history = deque(maxlen=config.context_history_length)
        self.mood_history = deque(maxlen=50)
        self.current_mood = MoodState.NEUTRAL
        self.mood_confidence = 0.5
        
    def detect_mood(self, recent_interactions: List[Dict[str, Any]], 
                   contextual_features: ContextualFeatures) -> Tuple[MoodState, float]:
        """ÙƒØ´Ù Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠ"""
        
        if len(recent_interactions) < self.config.min_interactions_for_mood:
            return self.current_mood, self.mood_confidence
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø²Ø§Ø¬
        mood_indicators = self._extract_mood_indicators(recent_interactions, contextual_features)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø²Ø§Ø¬ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª
        mood_scores = self._calculate_mood_scores(mood_indicators)
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹
        detected_mood = max(mood_scores, key=mood_scores.get)
        confidence = mood_scores[detected_mood]
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ†Ø¹ÙŠÙ… Ø§Ù„Ø²Ù…Ù†ÙŠ
        if self.mood_history:
            previous_mood_weight = self.config.mood_smoothing_factor
            current_mood_weight = 1 - previous_mood_weight
            
            # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø±ÙŠØ©
            if detected_mood == self.current_mood:
                confidence = previous_mood_weight * self.mood_confidence + current_mood_weight * confidence
            else:
                confidence = current_mood_weight * confidence
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªØ§Ø±ÙŠØ®
        self.mood_history.append({
            'mood': detected_mood,
            'confidence': confidence,
            'timestamp': datetime.now(),
            'indicators': mood_indicators
        })
        
        self.current_mood = detected_mood
        self.mood_confidence = confidence
        
        logger.info(f"ğŸ­ ÙƒØ´Ù Ù…Ø²Ø§Ø¬: {detected_mood.value} (Ø«Ù‚Ø©: {confidence:.2f})")
        
        return detected_mood, confidence
    
    def _extract_mood_indicators(self, interactions: List[Dict[str, Any]], 
                                context: ContextualFeatures) -> MoodIndicators:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø²Ø§Ø¬"""
        
        indicators = MoodIndicators()
        
        if not interactions:
            return indicators
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù†Ø´Ø§Ø·
        interaction_count = len(interactions)
        time_span = (datetime.now() - pd.to_datetime(interactions[0]['timestamp'])).total_seconds() / 60
        activity_rate = interaction_count / max(time_span, 1)
        indicators.activity_level = min(activity_rate / 5, 1.0)  # ØªØ·Ø¨ÙŠØ¹ Ù„Ù€ 5 ØªÙØ§Ø¹Ù„Ø§Øª/Ø¯Ù‚ÙŠÙ‚Ø©
        
        # ØªØ­Ù„ÙŠÙ„ Ù†Ù…Ø· Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù
        unique_articles = len(set(interaction['article_id'] for interaction in interactions))
        exploration_rate = unique_articles / interaction_count
        indicators.exploration_tendency = exploration_rate
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ±ÙƒÙŠØ²
        reading_times = [interaction.get('reading_time', 0) for interaction in interactions]
        avg_reading_time = np.mean(reading_times) if reading_times else 0
        indicators.focus_level = min(avg_reading_time / 300, 1.0)  # ØªØ·Ø¨ÙŠØ¹ Ù„Ù€ 5 Ø¯Ù‚Ø§Ø¦Ù‚
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµØ¨Ø±
        completion_rates = [interaction.get('read_percentage', 50) for interaction in interactions]
        avg_completion = np.mean(completion_rates) / 100
        indicators.patience_level = avg_completion
        
        # ØªØ­Ù„ÙŠÙ„ ØªÙƒØ±Ø§Ø± Ø§Ù„ØªÙØ§Ø¹Ù„
        interaction_types = [interaction.get('interaction_type', 'view') for interaction in interactions]
        active_interactions = sum(1 for t in interaction_types if t in ['like', 'save', 'share', 'comment'])
        indicators.interaction_frequency = active_interactions / interaction_count
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ø¯Ù„ Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        indicators.content_completion_rate = avg_completion
        
        # ØªØ­Ù„ÙŠÙ„ Ù…ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ©
        sharing_interactions = sum(1 for t in interaction_types if t == 'share')
        indicators.sharing_propensity = sharing_interactions / interaction_count
        
        # ØªØ­Ù„ÙŠÙ„ Ù†Ø´Ø§Ø· Ø§Ù„ØªØ¹Ù„ÙŠÙ‚
        comment_interactions = sum(1 for t in interaction_types if t == 'comment')
        indicators.commenting_activity = comment_interactions / interaction_count
        
        # ØªØ­Ù„ÙŠÙ„ ØªÙØ¶ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠ (ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø³Ø·)
        # ÙŠÙ…ÙƒÙ† ØªØ·ÙˆÙŠØ±Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ØªÙØ§Ø¹Ù„ Ù…Ø¹Ù‡
        positive_keywords = ['Ù†Ø¬Ø­', 'Ø£Ù…Ù„', 'ÙØ±Ø­', 'Ø¥Ù†Ø¬Ø§Ø²', 'ØªÙ‚Ø¯Ù…']
        positive_content_count = 0
        for interaction in interactions:
            title = interaction.get('title', '').lower()
            if any(keyword in title for keyword in positive_keywords):
                positive_content_count += 1
        indicators.positive_content_preference = positive_content_count / interaction_count
        
        # ØªØ­Ù„ÙŠÙ„ ØªØ­Ù…Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø¹Ù‚Ø¯
        long_articles = sum(1 for interaction in interactions 
                          if interaction.get('content_length', 0) > 1000)
        indicators.complex_content_tolerance = long_articles / interaction_count
        
        # ØªØ­Ù„ÙŠÙ„ ØªÙØ¶ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¨ØµØ±ÙŠ
        visual_interactions = sum(1 for interaction in interactions 
                                if interaction.get('has_images', False) or 
                                   interaction.get('has_video', False))
        indicators.visual_content_preference = visual_interactions / interaction_count
        
        # ØªØ­Ù„ÙŠÙ„ ØªÙØ¶ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ
        interactive_content = sum(1 for interaction in interactions 
                                if interaction.get('interaction_type') in ['like', 'save', 'share', 'comment'])
        indicators.interactive_content_preference = interactive_content / interaction_count
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«Ø¨Ø§Øª Ù…Ø¹ Ø§Ù„Ø±ÙˆØªÙŠÙ†
        current_hour = datetime.now().hour
        user_typical_hours = [pd.to_datetime(interaction['timestamp']).hour 
                             for interaction in interactions]
        hour_consistency = sum(1 for hour in user_typical_hours 
                              if abs(hour - current_hour) <= 2) / len(user_typical_hours)
        indicators.consistency_with_routine = hour_consistency
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¹ÙÙˆÙŠØ©
        time_gaps = []
        sorted_interactions = sorted(interactions, key=lambda x: x['timestamp'])
        for i in range(1, len(sorted_interactions)):
            gap = (pd.to_datetime(sorted_interactions[i]['timestamp']) - 
                  pd.to_datetime(sorted_interactions[i-1]['timestamp'])).total_seconds()
            time_gaps.append(gap)
        
        if time_gaps:
            gap_variance = np.var(time_gaps)
            indicators.spontaneity_level = min(gap_variance / 3600, 1.0)  # ØªØ·Ø¨ÙŠØ¹ Ù„Ù„ØªØ¨Ø§ÙŠÙ† Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ
        
        return indicators
    
    def _calculate_mood_scores(self, indicators: MoodIndicators) -> Dict[MoodState, float]:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©"""
        
        scores = {}
        
        # Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠ
        scores[MoodState.POSITIVE] = (
            indicators.positive_content_preference * 0.4 +
            indicators.sharing_propensity * 0.3 +
            indicators.interaction_frequency * 0.3
        )
        
        # Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ø³Ù„Ø¨ÙŠ
        scores[MoodState.NEGATIVE] = (
            (1 - indicators.positive_content_preference) * 0.4 +
            (1 - indicators.interaction_frequency) * 0.3 +
            (1 - indicators.activity_level) * 0.3
        )
        
        # Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„ÙØ¶ÙˆÙ„ÙŠ
        scores[MoodState.CURIOUS] = (
            indicators.exploration_tendency * 0.5 +
            indicators.complex_content_tolerance * 0.3 +
            indicators.focus_level * 0.2
        )
        
        # Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ù…Ø³ØªØ±Ø®ÙŠ
        scores[MoodState.RELAXED] = (
            indicators.visual_content_preference * 0.4 +
            (1 - indicators.activity_level) * 0.3 +
            indicators.patience_level * 0.3
        )
        
        # Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ù†Ø´ÙŠØ·
        scores[MoodState.ENERGETIC] = (
            indicators.activity_level * 0.5 +
            indicators.interaction_frequency * 0.3 +
            indicators.spontaneity_level * 0.2
        )
        
        # Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ù…Ø±ÙƒØ²
        scores[MoodState.FOCUSED] = (
            indicators.focus_level * 0.5 +
            indicators.content_completion_rate * 0.3 +
            indicators.consistency_with_routine * 0.2
        )
        
        # Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§ÙÙŠ
        scores[MoodState.EXPLORATORY] = (
            indicators.exploration_tendency * 0.6 +
            indicators.spontaneity_level * 0.4
        )
        
        # Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ø­Ù†ÙŠÙ†ÙŠ
        scores[MoodState.NOSTALGIC] = (
            (1 - indicators.exploration_tendency) * 0.4 +
            indicators.consistency_with_routine * 0.3 +
            indicators.patience_level * 0.3
        )
        
        # Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ÙŠ
        scores[MoodState.ANALYTICAL] = (
            indicators.complex_content_tolerance * 0.4 +
            indicators.focus_level * 0.3 +
            (1 - indicators.visual_content_preference) * 0.3
        )
        
        # Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ù…Ø­Ø§ÙŠØ¯ (Ø®Ø· Ø§Ù„Ø£Ø³Ø§Ø³)
        baseline_score = 0.3
        scores[MoodState.NEUTRAL] = baseline_score
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Ù‚Ø§Ø·
        total_score = sum(scores.values())
        if total_score > 0:
            scores = {mood: score / total_score for mood, score in scores.items()}
        
        return scores
    
    def get_mood_explanation(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙØ³ÙŠØ± Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠ"""
        if not self.mood_history:
            return {}
        
        latest_mood_data = self.mood_history[-1]
        indicators = latest_mood_data['indicators']
        
        explanation = {
            'current_mood': self.current_mood.value,
            'confidence': self.mood_confidence,
            'key_indicators': [],
            'mood_trends': self._analyze_mood_trends()
        }
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        indicator_scores = {
            'Ø§Ù„Ù†Ø´Ø§Ø·': indicators.activity_level,
            'Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù': indicators.exploration_tendency,
            'Ø§Ù„ØªØ±ÙƒÙŠØ²': indicators.focus_level,
            'Ø§Ù„ØµØ¨Ø±': indicators.patience_level,
            'Ø§Ù„ØªÙØ§Ø¹Ù„': indicators.interaction_frequency,
            'Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠ': indicators.positive_content_preference
        }
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙˆØ£Ø®Ø° Ø§Ù„Ø£Ø¹Ù„Ù‰
        sorted_indicators = sorted(indicator_scores.items(), key=lambda x: x[1], reverse=True)
        explanation['key_indicators'] = sorted_indicators[:3]
        
        return explanation
    
    def _analyze_mood_trends(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø²Ø§Ø¬"""
        if len(self.mood_history) < 3:
            return {'trend': 'insufficient_data'}
        
        recent_moods = [entry['mood'] for entry in list(self.mood_history)[-5:]]
        mood_changes = len(set(recent_moods))
        
        confidence_trend = [entry['confidence'] for entry in list(self.mood_history)[-5:]]
        avg_confidence = np.mean(confidence_trend)
        confidence_stability = 1 - np.std(confidence_trend)
        
        return {
            'stability': 'stable' if mood_changes <= 2 else 'variable',
            'average_confidence': avg_confidence,
            'confidence_stability': confidence_stability,
            'dominant_recent_mood': max(set(recent_moods), key=recent_moods.count).value
        }


class ContextAnalyzer:
    """
    Ù…Ø­Ù„Ù„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¨ÙŠØ¦ÙŠ ÙˆØ§Ù„Ø³Ù„ÙˆÙƒÙŠ
    Environmental and Behavioral Context Analyzer
    """
    
    def __init__(self, config: ContextualConfig):
        self.config = config
        self.context_history = deque(maxlen=config.context_history_length)
        self.context_patterns = {}
        
    def analyze_context(self, user_data: Dict[str, Any], 
                       session_data: Dict[str, Any]) -> ContextualFeatures:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø­Ø§Ù„ÙŠ"""
        
        features = ContextualFeatures()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø²Ù…Ù†ÙŠØ©
        now = datetime.now()
        features.hour_of_day = now.hour / 24.0
        features.day_of_week = now.weekday() / 6.0
        features.is_weekend = now.weekday() >= 5
        features.is_holiday = self._is_holiday(now)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª Ù…Ù†Ø° Ø¢Ø®Ø± Ø²ÙŠØ§Ø±Ø©
        last_visit = user_data.get('last_visit')
        if last_visit:
            time_diff = (now - pd.to_datetime(last_visit)).total_seconds() / 3600  # Ø¨Ø§Ù„Ø³Ø§Ø¹Ø§Øª
            features.time_since_last_visit = min(time_diff / 24, 1.0)  # ØªØ·Ø¨ÙŠØ¹ Ù„ÙŠÙˆÙ… ÙˆØ§Ø­Ø¯
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø¨ÙŠØ¦ÙŠØ©
        features.location_type = session_data.get('location_type', 'home')
        features.weather_condition = session_data.get('weather', 'clear')
        features.ambient_light = session_data.get('ambient_light', 1.0)
        features.noise_level = session_data.get('noise_level', 0.5)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©
        features.social_context = session_data.get('social_context', 'alone')
        features.social_activity = session_data.get('social_activity', False)
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ù†Ø´Ø§Ø·
        features.current_activity = session_data.get('current_activity', 'browsing')
        features.multitasking = session_data.get('multitasking', False)
        features.urgency_level = session_data.get('urgency_level', 0.5)
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø¬Ù‡Ø§Ø²
        features.device_type = session_data.get('device_type', 'mobile')
        features.connection_type = session_data.get('connection_type', 'wifi')
        features.battery_level = session_data.get('battery_level', 1.0)
        features.screen_size = session_data.get('screen_size', 'medium')
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©
        features.session_length = session_data.get('session_length', 0.0)
        features.pages_visited = session_data.get('pages_visited', 0)
        features.scroll_speed = session_data.get('scroll_speed', 1.0)
        features.interaction_rate = session_data.get('interaction_rate', 0.5)
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©
        features.reading_speed = session_data.get('reading_speed', 1.0)
        features.click_precision = session_data.get('click_precision', 1.0)
        features.time_between_actions = session_data.get('time_between_actions', 1.0)
        features.content_engagement = session_data.get('content_engagement', 0.5)
        
        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®
        self.context_history.append({
            'features': features,
            'timestamp': now
        })
        
        return features
    
    def _is_holiday(self, date: datetime) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØªØ§Ø±ÙŠØ® Ø¹Ø·Ù„Ø©"""
        # ØªÙ†ÙÙŠØ° Ù…Ø¨Ø³Ø· - ÙŠÙ…ÙƒÙ† ØªØ·ÙˆÙŠØ±Ù‡ Ù„ÙŠØ´Ù…Ù„ Ø§Ù„Ø¹Ø·Ù„ Ø§Ù„ÙØ¹Ù„ÙŠØ©
        # ÙÙŠ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ù…Ø«Ù„ Ø§Ù„Ø¹Ø·Ù„ Ø§Ù„Ø¯ÙŠÙ†ÙŠØ© ÙˆØ§Ù„ÙˆØ·Ù†ÙŠØ©
        return False
    
    def detect_context_patterns(self) -> Dict[str, Any]:
        """ÙƒØ´Ù Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³ÙŠØ§Ù‚"""
        if len(self.context_history) < 10:
            return {}
        
        patterns = {}
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠØ©
        hourly_usage = defaultdict(int)
        device_usage = defaultdict(int)
        activity_patterns = defaultdict(list)
        
        for entry in self.context_history:
            features = entry['features']
            timestamp = entry['timestamp']
            
            hour = timestamp.hour
            hourly_usage[hour] += 1
            device_usage[features.device_type] += 1
            activity_patterns[features.current_activity].append(hour)
        
        # Ø£ÙƒØ«Ø± Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ù†Ø´Ø§Ø·Ø§Ù‹
        peak_hours = sorted(hourly_usage.items(), key=lambda x: x[1], reverse=True)[:3]
        patterns['peak_hours'] = [hour for hour, count in peak_hours]
        
        # Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…ÙØ¶Ù„
        preferred_device = max(device_usage, key=device_usage.get)
        patterns['preferred_device'] = preferred_device
        
        # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù†Ø´Ø§Ø·
        for activity, hours in activity_patterns.items():
            if len(hours) >= 3:
                patterns[f'{activity}_pattern'] = {
                    'average_hour': np.mean(hours),
                    'hour_variance': np.var(hours),
                    'frequency': len(hours)
                }
        
        return patterns
    
    def predict_context_change(self) -> Dict[str, float]:
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨ØªØºÙŠÙŠØ± Ø§Ù„Ø³ÙŠØ§Ù‚"""
        if len(self.context_history) < 5:
            return {}
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø³Ø· Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
        recent_contexts = list(self.context_history)[-5:]
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø¬Ù‡Ø§Ø²
        devices = [entry['features'].device_type for entry in recent_contexts]
        device_stability = len(set(devices)) == 1
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ù†Ø´Ø§Ø·
        activities = [entry['features'].current_activity for entry in recent_contexts]
        activity_stability = len(set(activities)) == 1
        
        # ØªØ­Ù„ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡ Ø·ÙˆÙ„ Ø§Ù„Ø¬Ù„Ø³Ø©
        session_lengths = [entry['features'].session_length for entry in recent_contexts]
        session_trend = np.polyfit(range(len(session_lengths)), session_lengths, 1)[0]
        
        predictions = {
            'device_change_probability': 0.1 if device_stability else 0.7,
            'activity_change_probability': 0.2 if activity_stability else 0.6,
            'session_end_probability': max(0, min(session_trend * 0.1, 1.0))
        }
        
        return predictions


class ContextualContentMatcher:
    """
    Ù…Ø·Ø§Ø¨Ù‚ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ
    Contextual Content Matcher
    """
    
    def __init__(self, config: ContextualConfig):
        self.config = config
        self.content_context_scores = {}
        
    def calculate_content_context_fit(self, content: Dict[str, Any], 
                                    context: ContextualFeatures,
                                    mood: MoodState) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø¯Ù‰ Ù…Ù„Ø§Ø¡Ù…Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„Ø³ÙŠØ§Ù‚ ÙˆØ§Ù„Ù…Ø²Ø§Ø¬"""
        
        fit_score = 0.0
        
        # Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø²Ø§Ø¬
        mood_score = self._calculate_mood_content_match(content, mood)
        fit_score += mood_score * self.config.mood_influence_factor
        
        # Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø²Ù…Ù†ÙŠ
        temporal_score = self._calculate_temporal_content_match(content, context)
        fit_score += temporal_score * 0.2
        
        # Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¬Ù‡Ø§Ø²
        device_score = self._calculate_device_content_match(content, context)
        fit_score += device_score * 0.15
        
        # Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù†Ø´Ø§Ø·
        activity_score = self._calculate_activity_content_match(content, context)
        fit_score += activity_score * 0.15
        
        # Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¨ÙŠØ¦Ø©
        environment_score = self._calculate_environment_content_match(content, context)
        fit_score += environment_score * 0.1
        
        return min(fit_score, 1.0)
    
    def _calculate_mood_content_match(self, content: Dict[str, Any], mood: MoodState) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„Ù…Ø²Ø§Ø¬"""
        
        content_text = f"{content.get('title', '')} {content.get('description', '')}"
        content_category = content.get('category', '').lower()
        content_tags = content.get('tags', [])
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        mood_keywords = self.config.mood_content_mapping.get(mood.value, [])
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        keyword_matches = 0
        for keyword in mood_keywords:
            if keyword.lower() in content_text.lower():
                keyword_matches += 1
            if keyword.lower() in content_category:
                keyword_matches += 1
            if any(keyword.lower() in tag.lower() for tag in content_tags):
                keyword_matches += 1
        
        keyword_score = min(keyword_matches / len(mood_keywords), 1.0) if mood_keywords else 0.5
        
        # Ù…Ø·Ø§Ø¨Ù‚Ø© Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù…Ø²Ø§Ø¬
        mood_specific_score = self._get_mood_specific_score(content, mood)
        
        return (keyword_score * 0.6 + mood_specific_score * 0.4)
    
    def _get_mood_specific_score(self, content: Dict[str, Any], mood: MoodState) -> float:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø®Ø§ØµØ© Ø¨ÙƒÙ„ Ù…Ø²Ø§Ø¬"""
        
        content_length = content.get('content_length', 500)
        has_images = content.get('has_images', False)
        complexity_level = content.get('complexity_level', 0.5)
        reading_time = content.get('estimated_reading_time', 5)
        
        if mood == MoodState.RELAXED:
            # Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ù…Ø³ØªØ±Ø®ÙŠ ÙŠÙØ¶Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¨ØµØ±ÙŠ ÙˆØ§Ù„Ù‚ØµÙŠØ±
            return (
                (0.8 if has_images else 0.3) * 0.4 +
                (0.9 if reading_time <= 3 else 0.5) * 0.3 +
                (0.8 if complexity_level <= 0.5 else 0.4) * 0.3
            )
        
        elif mood == MoodState.FOCUSED:
            # Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ù…Ø±ÙƒØ² ÙŠÙØ¶Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ù…ÙŠÙ‚ ÙˆØ§Ù„Ø·ÙˆÙŠÙ„
            return (
                (0.9 if reading_time >= 10 else 0.5) * 0.4 +
                (0.8 if complexity_level >= 0.7 else 0.4) * 0.4 +
                (0.7 if not has_images else 0.5) * 0.2
            )
        
        elif mood == MoodState.ENERGETIC:
            # Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ù†Ø´ÙŠØ· ÙŠÙØ¶Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø³Ø±ÙŠØ¹ ÙˆØ§Ù„ØªÙØ§Ø¹Ù„ÙŠ
            return (
                (0.9 if reading_time <= 5 else 0.4) * 0.5 +
                (0.8 if has_images else 0.4) * 0.3 +
                (0.7 if content.get('interactive', False) else 0.5) * 0.2
            )
        
        elif mood == MoodState.CURIOUS:
            # Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„ÙØ¶ÙˆÙ„ÙŠ ÙŠÙØ¶Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ØªÙ†ÙˆØ¹ ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙŠ
            return (
                (0.9 if complexity_level >= 0.6 else 0.5) * 0.4 +
                (0.8 if 'Ø¹Ù„Ù…ÙŠ' in content.get('category', '') else 0.5) * 0.3 +
                (0.7 if content.get('has_data', False) else 0.5) * 0.3
            )
        
        elif mood == MoodState.ANALYTICAL:
            # Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ÙŠ ÙŠÙØ¶Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª
            return (
                (0.9 if content.get('has_charts', False) else 0.4) * 0.4 +
                (0.8 if 'ØªØ­Ù„ÙŠÙ„' in content.get('title', '') else 0.5) * 0.3 +
                (0.8 if complexity_level >= 0.8 else 0.5) * 0.3
            )
        
        else:
            return 0.5  # Ù†Ù‚Ø§Ø· Ù…Ø­Ø§ÙŠØ¯Ø© Ù„Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ù…Ø­Ø§ÙŠØ¯
    
    def _calculate_temporal_content_match(self, content: Dict[str, Any], 
                                        context: ContextualFeatures) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„ÙˆÙ‚Øª"""
        
        hour = context.hour_of_day * 24
        
        # ØªÙØ¶ÙŠÙ„Ø§Øª Ø²Ù…Ù†ÙŠØ© Ù„Ù„Ù…Ø­ØªÙˆÙ‰
        if 6 <= hour <= 11:  # ØµØ¨Ø§Ø­
            preferences = self.config.context_content_preferences.get('morning', {})
        elif 12 <= hour <= 17:  # Ø¨Ø¹Ø¯ Ø§Ù„Ø¸Ù‡Ø±
            preferences = self.config.context_content_preferences.get('afternoon', {})
        elif 18 <= hour <= 22:  # Ù…Ø³Ø§Ø¡
            preferences = self.config.context_content_preferences.get('evening', {})
        else:  # Ù„ÙŠÙ„
            preferences = self.config.context_content_preferences.get('night', {})
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ·Ø§Ø¨Ù‚
        match_score = 0.5  # Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        
        content_category = content.get('category', '').lower()
        content_type = content.get('type', '').lower()
        
        for pref_type, pref_score in preferences.items():
            if pref_type in content_category or pref_type in content_type:
                match_score = max(match_score, pref_score)
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ø·Ù„Ø© Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹
        if context.is_weekend:
            weekend_prefs = self.config.context_content_preferences.get('weekend', {})
            for pref_type, pref_score in weekend_prefs.items():
                if pref_type in content_category or pref_type in content_type:
                    match_score = max(match_score, pref_score)
        
        return match_score
    
    def _calculate_device_content_match(self, content: Dict[str, Any], 
                                      context: ContextualFeatures) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„Ø¬Ù‡Ø§Ø²"""
        
        device_prefs = self.config.context_content_preferences.get(context.device_type, {})
        
        content_length = content.get('content_length', 500)
        has_images = content.get('has_images', False)
        is_interactive = content.get('interactive', False)
        
        match_score = 0.5
        
        if context.device_type == 'mobile':
            # Ø§Ù„Ù…ÙˆØ¨Ø§ÙŠÙ„ ÙŠÙØ¶Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù‚ØµÙŠØ± ÙˆØ§Ù„Ø¨ØµØ±ÙŠ
            if content_length <= 800:
                match_score += 0.3
            if has_images:
                match_score += 0.2
            if content.get('mobile_optimized', False):
                match_score += 0.2
        
        elif context.device_type == 'desktop':
            # Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨ ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø·ÙˆÙŠÙ„ ÙˆØ§Ù„Ù…Ø¹Ù‚Ø¯
            if content_length >= 1000:
                match_score += 0.2
            if content.get('has_charts', False):
                match_score += 0.2
            if is_interactive:
                match_score += 0.1
        
        elif context.device_type == 'tablet':
            # Ø§Ù„ØªØ§Ø¨Ù„Øª Ù…ØªÙˆØ³Ø· Ø¨ÙŠÙ† Ø§Ù„Ù…ÙˆØ¨Ø§ÙŠÙ„ ÙˆØ³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨
            if 600 <= content_length <= 1500:
                match_score += 0.2
            if has_images:
                match_score += 0.15
        
        return min(match_score, 1.0)
    
    def _calculate_activity_content_match(self, content: Dict[str, Any], 
                                        context: ContextualFeatures) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø­Ø§Ù„ÙŠ"""
        
        activity = context.current_activity
        
        if activity == 'working':
            # Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¹Ù…Ù„: Ù…Ø­ØªÙˆÙ‰ Ù…Ù‡Ù†ÙŠ ÙˆØ³Ø±ÙŠØ¹
            if 'Ù…Ù‡Ù†ÙŠ' in content.get('category', '') or 'Ø¹Ù…Ù„' in content.get('tags', []):
                return 0.9
            if content.get('estimated_reading_time', 10) <= 5:
                return 0.7
            return 0.4
        
        elif activity == 'commuting':
            # Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ù‚Ù„: Ù…Ø­ØªÙˆÙ‰ Ù‚ØµÙŠØ± ÙˆØ³Ù‡Ù„ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©
            if content.get('estimated_reading_time', 10) <= 3:
                return 0.9
            if content.get('has_audio', False):
                return 0.8
            return 0.3
        
        elif activity == 'relaxing':
            # Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ±Ø®Ø§Ø¡: Ù…Ø­ØªÙˆÙ‰ ØªØ±ÙÙŠÙ‡ÙŠ ÙˆÙ…Ø±ÙŠØ­
            if 'ØªØ±ÙÙŠÙ‡' in content.get('category', ''):
                return 0.9
            if content.get('has_images', False):
                return 0.7
            return 0.5
        
        else:
            return 0.5  # Ù†Ù‚Ø§Ø· Ù…Ø­Ø§ÙŠØ¯Ø© Ù„Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„Ø£Ø®Ø±Ù‰
    
    def _calculate_environment_content_match(self, content: Dict[str, Any], 
                                           context: ContextualFeatures) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„Ø¨ÙŠØ¦Ø©"""
        
        match_score = 0.5
        
        # ØªØ£Ø«ÙŠØ± Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø©
        if context.ambient_light < 0.3:  # Ø¥Ø¶Ø§Ø¡Ø© Ù…Ù†Ø®ÙØ¶Ø©
            # ØªÙØ¶ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙˆØªÙŠ Ø£Ùˆ Ù‚ØµÙŠØ± Ø§Ù„Ù…Ø¯Ù‰
            if content.get('has_audio', False):
                match_score += 0.2
            if content.get('estimated_reading_time', 10) <= 3:
                match_score += 0.1
        
        # ØªØ£Ø«ÙŠØ± Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡
        if context.noise_level > 0.7:  # Ø¨ÙŠØ¦Ø© ØµØ§Ø®Ø¨Ø©
            # ØªÙØ¶ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¨ØµØ±ÙŠ
            if content.get('has_images', False):
                match_score += 0.2
            if content.get('has_video', False):
                match_score += 0.1
        
        # ØªØ£Ø«ÙŠØ± Ù†ÙˆØ¹ Ø§Ù„Ù…ÙˆÙ‚Ø¹
        if context.location_type == 'public':
            # ÙÙŠ Ø§Ù„Ø£Ù…Ø§ÙƒÙ† Ø§Ù„Ø¹Ø§Ù…Ø©: Ù…Ø­ØªÙˆÙ‰ Ø£Ù‚Ù„ Ø­Ø³Ø§Ø³ÙŠØ© ÙˆØ£Ù‚ØµØ±
            if content.get('estimated_reading_time', 10) <= 5:
                match_score += 0.2
            if 'Ø¹Ø§Ù…' not in content.get('sensitivity_level', 'Ø¹Ø§Ù…'):
                match_score -= 0.2
        
        return min(max(match_score, 0.0), 1.0)


class ContextualRecommendationEngine:
    """
    Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    Main Contextual Recommendation Engine
    """
    
    def __init__(self, config: ContextualConfig):
        self.config = config
        self.mood_detector = MoodDetector(config)
        self.context_analyzer = ContextAnalyzer(config)
        self.content_matcher = ContextualContentMatcher(config)
        self.adaptation_history = deque(maxlen=1000)
        
    def get_contextual_recommendations(self, user_id: str, 
                                     base_recommendations: List[Tuple[str, float]],
                                     user_data: Dict[str, Any],
                                     session_data: Dict[str, Any],
                                     recent_interactions: List[Dict[str, Any]],
                                     content_database: List[Dict[str, Any]]) -> List[Tuple[str, float, Dict[str, Any]]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Ù…ÙƒÙŠÙØ© Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØ§Ù„Ù…Ø²Ø§Ø¬"""
        
        logger.info(f"ğŸ¯ Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆØµÙŠØ§Øª Ø³ÙŠØ§Ù‚ÙŠØ© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id}")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø­Ø§Ù„ÙŠ
        current_context = self.context_analyzer.analyze_context(user_data, session_data)
        
        # ÙƒØ´Ù Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠ
        current_mood, mood_confidence = self.mood_detector.detect_mood(
            recent_interactions, current_context
        )
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙƒÙŠÙ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØµÙŠØ§Øª
        contextual_recommendations = []
        
        for article_id, base_score in base_recommendations:
            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            content_info = next(
                (content for content in content_database if content['id'] == article_id),
                {}
            )
            
            if not content_info:
                continue
            
            # Ø­Ø³Ø§Ø¨ Ù…Ø¯Ù‰ Ø§Ù„Ù…Ù„Ø§Ø¡Ù…Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©
            context_fit = self.content_matcher.calculate_content_context_fit(
                content_info, current_context, current_mood
            )
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ÙƒÙŠÙØ©
            adapted_score = self._calculate_adapted_score(
                base_score, context_fit, mood_confidence
            )
            
            # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³ÙŠØ§Ù‚
            context_info = {
                'mood': current_mood.value,
                'mood_confidence': mood_confidence,
                'context_fit': context_fit,
                'adaptation_reason': self._generate_adaptation_reason(
                    current_mood, current_context, context_fit
                ),
                'original_score': base_score,
                'context_boost': adapted_score - base_score
            }
            
            contextual_recommendations.append((article_id, adapted_score, context_info))
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…ÙƒÙŠÙØ©
        contextual_recommendations.sort(key=lambda x: x[1], reverse=True)
        
        # ØªØ·Ø¨ÙŠÙ‚ ØªÙ†ÙˆÙŠØ¹ Ø¥Ø¶Ø§ÙÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚
        diversified_recommendations = self._apply_contextual_diversification(
            contextual_recommendations, current_mood, current_context
        )
        
        # Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙƒÙŠÙ Ù„Ù„ØªØ¹Ù„Ù…
        self._record_adaptation(user_id, current_mood, current_context, 
                              diversified_recommendations)
        
        logger.info(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(diversified_recommendations)} ØªÙˆØµÙŠØ© Ø³ÙŠØ§Ù‚ÙŠØ©")
        
        return diversified_recommendations
    
    def _calculate_adapted_score(self, base_score: float, context_fit: float, 
                               mood_confidence: float) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ÙƒÙŠÙØ©"""
        
        # Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„ØªÙˆØµÙŠØ©
        base_weight = self.config.base_recommendation_weight
        
        # ÙˆØ²Ù† Ø§Ù„ØªØ£Ø«ÙŠØ± Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ (ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø«Ù‚Ø© ÙƒØ´Ù Ø§Ù„Ù…Ø²Ø§Ø¬)
        context_weight = self.config.context_influence_factor * mood_confidence
        
        # Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ÙƒÙŠÙØ©
        adapted_score = (
            base_score * base_weight +
            context_fit * context_weight +
            base_score * context_fit * (1 - base_weight - context_weight)
        )
        
        return min(adapted_score, 1.0)
    
    def _generate_adaptation_reason(self, mood: MoodState, 
                                  context: ContextualFeatures,
                                  context_fit: float) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø³Ø¨Ø¨ Ø§Ù„ØªÙƒÙŠÙ"""
        
        reasons = []
        
        # Ø³Ø¨Ø¨ Ø§Ù„Ù…Ø²Ø§Ø¬
        if context_fit > 0.7:
            reasons.append(f"ÙŠÙ†Ø§Ø³Ø¨ Ù…Ø²Ø§Ø¬Ùƒ {mood.value}")
        
        # Ø³Ø¨Ø¨ Ø§Ù„ÙˆÙ‚Øª
        hour = context.hour_of_day * 24
        if 6 <= hour <= 11:
            reasons.append("Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµØ¨Ø§Ø­ÙŠØ©")
        elif 18 <= hour <= 22:
            reasons.append("Ù…Ø«Ø§Ù„ÙŠ Ù„Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø³Ø§Ø¦ÙŠØ©")
        
        # Ø³Ø¨Ø¨ Ø§Ù„Ø¬Ù‡Ø§Ø²
        if context.device_type == 'mobile' and context_fit > 0.6:
            reasons.append("Ù…Ø­Ø³Ù† Ù„Ù„Ù‡Ø§ØªÙ Ø§Ù„Ù…Ø­Ù…ÙˆÙ„")
        
        # Ø³Ø¨Ø¨ Ø§Ù„Ù†Ø´Ø§Ø·
        if context.current_activity == 'relaxing':
            reasons.append("Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø§Ø³ØªØ±Ø®Ø§Ø¡")
        elif context.current_activity == 'working':
            reasons.append("Ù…ÙÙŠØ¯ Ù„Ù„Ø¹Ù…Ù„")
        
        return '; '.join(reasons) if reasons else "ØªÙˆØµÙŠØ© Ø¹Ø§Ù…Ø©"
    
    def _apply_contextual_diversification(self, recommendations: List[Tuple[str, float, Dict]],
                                        mood: MoodState, 
                                        context: ContextualFeatures) -> List[Tuple[str, float, Dict]]:
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ†ÙˆÙŠØ¹ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ"""
        
        if len(recommendations) <= 3:
            return recommendations
        
        diversified = []
        used_categories = set()
        mood_matched_count = 0
        
        for article_id, score, context_info in recommendations:
            # ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ Ø¥Ø¶Ø§ÙØ© Ù‡Ø°Ù‡ Ø§Ù„ØªÙˆØµÙŠØ©
            should_add = True
            
            # ØªÙ†ÙˆÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª
            category = context_info.get('category', 'general')
            if category in used_categories and len(used_categories) >= 3:
                # ØªÙ‚Ù„ÙŠÙ„ Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
                score *= 0.8
            
            # ØªÙˆØ§Ø²Ù† Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù…Ø²Ø§Ø¬
            if context_info.get('context_fit', 0) > 0.7:
                mood_matched_count += 1
                if mood_matched_count > len(recommendations) * 0.7:
                    # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù…Ø²Ø§Ø¬ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ù‡ÙŠÙ…Ù†Ø©
                    score *= 0.9
            
            if should_add:
                diversified.append((article_id, score, context_info))
                used_categories.add(category)
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†ÙˆÙŠØ¹
        diversified.sort(key=lambda x: x[1], reverse=True)
        
        return diversified
    
    def _record_adaptation(self, user_id: str, mood: MoodState, 
                         context: ContextualFeatures,
                         recommendations: List[Tuple[str, float, Dict]]):
        """ØªØ³Ø¬ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙƒÙŠÙ Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ"""
        
        adaptation_record = {
            'user_id': user_id,
            'timestamp': datetime.now(),
            'mood': mood.value,
            'context': {
                'hour_of_day': context.hour_of_day,
                'day_of_week': context.day_of_week,
                'device_type': context.device_type,
                'activity': context.current_activity,
                'location_type': context.location_type
            },
            'recommendations_count': len(recommendations),
            'average_context_fit': np.mean([rec[2]['context_fit'] for rec in recommendations]),
            'mood_confidence': recommendations[0][2]['mood_confidence'] if recommendations else 0
        }
        
        self.adaptation_history.append(adaptation_record)
    
    def learn_from_feedback(self, user_id: str, article_id: str, 
                          feedback_type: str, feedback_value: float,
                          context_at_recommendation: Dict[str, Any]):
        """Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©"""
        
        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„ØªÙƒÙŠÙ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„
        recent_adaptations = [
            record for record in self.adaptation_history
            if record['user_id'] == user_id and 
               (datetime.now() - record['timestamp']).total_seconds() < 3600  # Ø¢Ø®Ø± Ø³Ø§Ø¹Ø©
        ]
        
        if not recent_adaptations:
            return
        
        # ØªØ­Ø¯ÙŠØ« Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØªÙƒÙŠÙ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        adaptation_success = self._evaluate_adaptation_success(
            feedback_type, feedback_value
        )
        
        # ØªØ¹Ø¯ÙŠÙ„ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªÙƒÙŠÙ
        if adaptation_success > 0.7:
            # ØªÙ‚ÙˆÙŠØ© Ø§Ù„ØªÙƒÙŠÙ Ø§Ù„Ù†Ø§Ø¬Ø­
            self.config.mood_influence_factor = min(
                self.config.mood_influence_factor * 1.05, 0.8
            )
        elif adaptation_success < 0.3:
            # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªÙƒÙŠÙ ØºÙŠØ± Ø§Ù„Ù†Ø§Ø¬Ø­
            self.config.mood_influence_factor = max(
                self.config.mood_influence_factor * 0.95, 0.1
            )
        
        logger.info(f"ğŸ“š ØªÙ… Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©: {feedback_type} = {feedback_value}")
    
    def _evaluate_adaptation_success(self, feedback_type: str, feedback_value: float) -> float:
        """ØªÙ‚ÙŠÙŠÙ… Ù†Ø¬Ø§Ø­ Ø§Ù„ØªÙƒÙŠÙ"""
        
        success_scores = {
            'view': 0.3,
            'like': 0.8,
            'save': 0.9,
            'share': 1.0,
            'comment': 0.9,
            'long_read': 0.7,  # Ù‚Ø±Ø§Ø¡Ø© Ø·ÙˆÙŠÙ„Ø©
            'quick_exit': 0.1  # Ø®Ø±ÙˆØ¬ Ø³Ø±ÙŠØ¹
        }
        
        base_success = success_scores.get(feedback_type, 0.5)
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©
        if feedback_value > 0:
            return min(base_success + feedback_value * 0.3, 1.0)
        else:
            return max(base_success + feedback_value * 0.3, 0.0)
    
    def get_adaptation_insights(self, user_id: str) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¤Ù‰ Ø§Ù„ØªÙƒÙŠÙ"""
        
        user_adaptations = [
            record for record in self.adaptation_history
            if record['user_id'] == user_id
        ]
        
        if not user_adaptations:
            return {}
        
        # ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø²Ø§Ø¬
        mood_distribution = defaultdict(int)
        context_patterns = defaultdict(list)
        
        for record in user_adaptations:
            mood_distribution[record['mood']] += 1
            context_patterns['hours'].append(record['context']['hour_of_day'])
            context_patterns['devices'].append(record['context']['device_type'])
        
        # Ø£ÙƒØ«Ø± Ø§Ù„Ù…Ø²Ø§Ø¬Ø§Øª Ø´ÙŠÙˆØ¹Ø§Ù‹
        dominant_mood = max(mood_distribution, key=mood_distribution.get)
        
        # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        if context_patterns['hours']:
            peak_hour = max(set(context_patterns['hours']), 
                          key=context_patterns['hours'].count)
        else:
            peak_hour = 12
        
        preferred_device = max(set(context_patterns['devices']), 
                             key=context_patterns['devices'].count) if context_patterns['devices'] else 'mobile'
        
        return {
            'dominant_mood': dominant_mood,
            'mood_distribution': dict(mood_distribution),
            'peak_usage_hour': peak_hour,
            'preferred_device': preferred_device,
            'adaptation_frequency': len(user_adaptations),
            'average_context_fit': np.mean([r['average_context_fit'] for r in user_adaptations])
        }
    
    def save_model(self, filepath: str):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ"""
        logger.info(f"ğŸ’¾ Ø­ÙØ¸ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ ÙÙŠ {filepath}")
        
        save_data = {
            'config': self.config,
            'adaptation_history': list(self.adaptation_history),
            'mood_detector_state': {
                'mood_history': list(self.mood_detector.mood_history),
                'current_mood': self.mood_detector.current_mood.value,
                'mood_confidence': self.mood_detector.mood_confidence
            },
            'context_patterns': self.context_analyzer.context_patterns,
            'save_timestamp': datetime.now().isoformat()
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(save_data, f)
        
        logger.info("âœ… ØªÙ… Ø­ÙØ¸ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ")
    
    def load_model(self, filepath: str):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ"""
        logger.info(f"ğŸ“‚ ØªØ­Ù…ÙŠÙ„ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ Ù…Ù† {filepath}")
        
        try:
            with open(filepath, 'rb') as f:
                save_data = pickle.load(f)
            
            self.config = save_data['config']
            self.adaptation_history = deque(save_data['adaptation_history'], 
                                          maxlen=1000)
            
            # Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø­Ø§Ù„Ø© ÙƒØ§Ø´Ù Ø§Ù„Ù…Ø²Ø§Ø¬
            mood_state = save_data['mood_detector_state']
            self.mood_detector.mood_history = deque(mood_state['mood_history'], 
                                                  maxlen=50)
            self.mood_detector.current_mood = MoodState(mood_state['current_mood'])
            self.mood_detector.mood_confidence = mood_state['mood_confidence']
            
            # Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³ÙŠØ§Ù‚
            self.context_analyzer.context_patterns = save_data['context_patterns']
            
            logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ")
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ: {str(e)}")
            raise


# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙƒÙˆÙŠÙ†
    config = ContextualConfig(
        mood_detection_window=10,
        context_update_frequency=30,
        mood_influence_factor=0.4
    )
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ
    contextual_engine = ContextualRecommendationEngine(config)
    
    # Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©
    user_data = {
        'user_id': 'user_123',
        'last_visit': '2024-01-15 10:00:00'
    }
    
    session_data = {
        'device_type': 'mobile',
        'location_type': 'home',
        'current_activity': 'relaxing',
        'session_length': 15.0,
        'connection_type': 'wifi'
    }
    
    recent_interactions = [
        {
            'article_id': 'article_1',
            'interaction_type': 'view',
            'reading_time': 120,
            'read_percentage': 80,
            'timestamp': '2024-01-15 14:30:00'
        },
        {
            'article_id': 'article_2',
            'interaction_type': 'like',
            'reading_time': 300,
            'read_percentage': 95,
            'timestamp': '2024-01-15 14:35:00'
        }
    ]
    
    base_recommendations = [
        ('article_3', 0.8),
        ('article_4', 0.7),
        ('article_5', 0.6)
    ]
    
    content_database = [
        {
            'id': 'article_3',
            'title': 'Ù…Ù‚Ø§Ù„ ØªØ±ÙÙŠÙ‡ÙŠ Ø®ÙÙŠÙ',
            'category': 'ØªØ±ÙÙŠÙ‡',
            'content_length': 400,
            'has_images': True,
            'estimated_reading_time': 3
        },
        {
            'id': 'article_4',
            'title': 'ØªØ­Ù„ÙŠÙ„ Ø§Ù‚ØªØµØ§Ø¯ÙŠ Ù…Ø¹Ù…Ù‚',
            'category': 'Ø§Ù‚ØªØµØ§Ø¯',
            'content_length': 1200,
            'has_images': False,
            'estimated_reading_time': 8
        },
        {
            'id': 'article_5',
            'title': 'Ø£Ø®Ø¨Ø§Ø± ØªÙ‚Ù†ÙŠØ© Ø³Ø±ÙŠØ¹Ø©',
            'category': 'ØªÙ‚Ù†ÙŠØ©',
            'content_length': 600,
            'has_images': True,
            'estimated_reading_time': 4
        }
    ]
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Ø³ÙŠØ§Ù‚ÙŠØ©
    contextual_recs = contextual_engine.get_contextual_recommendations(
        user_id='user_123',
        base_recommendations=base_recommendations,
        user_data=user_data,
        session_data=session_data,
        recent_interactions=recent_interactions,
        content_database=content_database
    )
    
    print("ğŸ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©:")
    for article_id, score, context_info in contextual_recs:
        print(f"  {article_id}: {score:.3f}")
        print(f"    Ø§Ù„Ù…Ø²Ø§Ø¬: {context_info['mood']}")
        print(f"    Ø§Ù„Ø³Ø¨Ø¨: {context_info['adaptation_reason']}")
        print(f"    Ø§Ù„ØªØ­Ø³Ù†: +{context_info['context_boost']:.3f}")
        print()
    
    # Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©
    contextual_engine.learn_from_feedback(
        user_id='user_123',
        article_id='article_3',
        feedback_type='like',
        feedback_value=1.0,
        context_at_recommendation={}
    )
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¤Ù‰ Ø§Ù„ØªÙƒÙŠÙ
    insights = contextual_engine.get_adaptation_insights('user_123')
    print("ğŸ“Š Ø±Ø¤Ù‰ Ø§Ù„ØªÙƒÙŠÙ:", insights)
