# Ù†Ø¸Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - Ø³Ø¨Ù‚ Ø§Ù„Ø°ÙƒÙŠØ©
# Advanced User Interest Analysis System

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.decomposition import PCA, LatentDirichletAllocation, NMF
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
import networkx as nx
from scipy.stats import entropy
from scipy.spatial.distance import pdist, squareform
import logging
from typing import Dict, List, Tuple, Optional, Any, Union
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from collections import defaultdict, Counter
import joblib
import pickle
import json
import re
from enum import Enum
import math

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class InterestType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª"""
    TOPICAL = "Ù…ÙˆØ¶ÙˆØ¹ÙŠ"           # Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø¹ÙŠÙ†
    TEMPORAL = "Ø²Ù…Ù†ÙŠ"            # Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙÙŠ ÙˆÙ‚Øª Ù…Ø¹ÙŠÙ†
    SOCIAL = "Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ"           # Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø´Ø§Ø¦Ø¹
    PERSONAL = "Ø´Ø®ØµÙŠ"           # Ø§Ù‡ØªÙ…Ø§Ù… Ø´Ø®ØµÙŠ ÙØ±ÙŠØ¯
    TRENDING = "Ø±Ø§Ø¦Ø¬"            # Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø©
    SEASONAL = "Ù…ÙˆØ³Ù…ÙŠ"          # Ø§Ù‡ØªÙ…Ø§Ù… Ù…ÙˆØ³Ù…ÙŠ
    EXPLORATORY = "Ø§Ø³ØªÙƒØ´Ø§ÙÙŠ"    # Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ø§Ù„ØªÙ†ÙˆÙŠØ¹ ÙˆØ§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù

@dataclass
class InterestProfile:
    """Ù…Ù„Ù Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
    user_id: str
    interests: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    interest_evolution: List[Dict[str, Any]] = field(default_factory=list)
    personality_traits: Dict[str, float] = field(default_factory=dict)
    behavior_patterns: Dict[str, Any] = field(default_factory=dict)
    engagement_preferences: Dict[str, float] = field(default_factory=dict)
    temporal_preferences: Dict[str, Any] = field(default_factory=dict)
    diversity_score: float = 0.0
    curiosity_score: float = 0.0
    consistency_score: float = 0.0
    last_updated: datetime = field(default_factory=datetime.now)
    confidence_score: float = 0.0

@dataclass
class InterestAnalysisConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù†Ø¸Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª"""
    # Clustering settings
    n_clusters: int = 10
    clustering_algorithm: str = "kmeans"  # kmeans, dbscan, hierarchical
    min_cluster_size: int = 5
    
    # Temporal analysis
    temporal_window_days: int = 30
    trend_detection_threshold: float = 0.1
    seasonality_periods: List[int] = field(default_factory=lambda: [7, 30, 90, 365])
    
    # Interest evolution
    interest_decay_rate: float = 0.95
    min_interest_strength: float = 0.1
    max_interests_per_user: int = 20
    
    # Personality analysis
    personality_dimensions: List[str] = field(default_factory=lambda: [
        'openness', 'conscientiousness', 'extraversion', 
        'agreeableness', 'neuroticism', 'curiosity'
    ])
    
    # Behavior patterns
    min_pattern_support: float = 0.1
    max_pattern_length: int = 5
    
    # Feature extraction
    tfidf_max_features: int = 1000
    embedding_dim: int = 128
    
    # Similarity thresholds
    similarity_threshold: float = 0.3
    interest_merge_threshold: float = 0.8


class TemporalInterestAnalyzer:
    """
    Ù…Ø­Ù„Ù„ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©
    Temporal Interest Analyzer
    """
    
    def __init__(self, config: InterestAnalysisConfig):
        self.config = config
        self.temporal_patterns = {}
        self.seasonal_trends = {}
        
    def analyze_temporal_patterns(self, user_interactions: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª"""
        logger.info(f"ğŸ“… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ù„Ù€ {len(user_interactions)} ØªÙØ§Ø¹Ù„...")
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆÙ‚Øª Ø¥Ù„Ù‰ Ù…Ø¹Ø§Ù„Ù… Ø²Ù…Ù†ÙŠØ©
        user_interactions['hour'] = pd.to_datetime(user_interactions['created_at']).dt.hour
        user_interactions['day_of_week'] = pd.to_datetime(user_interactions['created_at']).dt.dayofweek
        user_interactions['month'] = pd.to_datetime(user_interactions['created_at']).dt.month
        user_interactions['week_of_year'] = pd.to_datetime(user_interactions['created_at']).dt.isocalendar().week
        
        patterns = {
            'hourly_distribution': self._analyze_hourly_patterns(user_interactions),
            'daily_distribution': self._analyze_daily_patterns(user_interactions),
            'weekly_trends': self._analyze_weekly_trends(user_interactions),
            'monthly_seasonality': self._analyze_monthly_patterns(user_interactions),
            'reading_sessions': self._analyze_reading_sessions(user_interactions),
            'content_timing_preferences': self._analyze_content_timing(user_interactions)
        }
        
        # ØªØ­Ø¯ÙŠØ¯ Ù†Ù…Ø· Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
        patterns['primary_usage_pattern'] = self._identify_primary_pattern(patterns)
        patterns['activity_regularity'] = self._calculate_activity_regularity(user_interactions)
        
        return patterns
    
    def _analyze_hourly_patterns(self, interactions: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³Ø§Ø¹Ø§Øª"""
        hourly_activity = interactions.groupby('hour').size()
        
        # ØªØ­Ø¯ÙŠØ¯ Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø°Ø±ÙˆØ©
        peak_hours = hourly_activity.nlargest(3).index.tolist()
        
        # ØªØµÙ†ÙŠÙ Ù†Ù…Ø· Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        if any(hour in [6, 7, 8, 9] for hour in peak_hours):
            pattern_type = "ØµØ¨Ø§Ø­ÙŠ"
        elif any(hour in [12, 13, 14] for hour in peak_hours):
            pattern_type = "ÙˆÙ‚Øª Ø§Ù„ØºØ¯Ø§Ø¡"
        elif any(hour in [18, 19, 20, 21] for hour in peak_hours):
            pattern_type = "Ù…Ø³Ø§Ø¦ÙŠ"
        elif any(hour in [22, 23, 0, 1] for hour in peak_hours):
            pattern_type = "Ù„ÙŠÙ„ÙŠ"
        else:
            pattern_type = "Ù…ØªÙ†ÙˆØ¹"
        
        return {
            'distribution': hourly_activity.to_dict(),
            'peak_hours': peak_hours,
            'pattern_type': pattern_type,
            'consistency': self._calculate_temporal_consistency(hourly_activity)
        }
    
    def _analyze_daily_patterns(self, interactions: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£ÙŠØ§Ù…"""
        daily_activity = interactions.groupby('day_of_week').size()
        
        # Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£ÙŠØ§Ù… Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        day_names = {0: 'Ø§Ù„Ø¥Ø«Ù†ÙŠÙ†', 1: 'Ø§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡', 2: 'Ø§Ù„Ø£Ø±Ø¨Ø¹Ø§Ø¡', 3: 'Ø§Ù„Ø®Ù…ÙŠØ³', 
                    4: 'Ø§Ù„Ø¬Ù…Ø¹Ø©', 5: 'Ø§Ù„Ø³Ø¨Øª', 6: 'Ø§Ù„Ø£Ø­Ø¯'}
        
        # ØªØ­Ø¯ÙŠØ¯ Ù†Ù…Ø· Ø£ÙŠØ§Ù… Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ Ù…Ù‚Ø§Ø¨Ù„ Ø¹Ø·Ù„Ø© Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹
        weekday_activity = daily_activity[0:5].sum()
        weekend_activity = daily_activity[5:7].sum()
        
        if weekend_activity > weekday_activity:
            pattern_type = "Ø¹Ø·Ù„Ø© Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹"
        elif weekday_activity > weekend_activity * 2:
            pattern_type = "Ø£ÙŠØ§Ù… Ø§Ù„Ø¹Ù…Ù„"
        else:
            pattern_type = "Ù…ØªÙˆØ§Ø²Ù†"
        
        return {
            'distribution': {day_names[day]: count for day, count in daily_activity.items()},
            'weekday_vs_weekend': {
                'weekdays': weekday_activity,
                'weekends': weekend_activity,
                'ratio': weekday_activity / max(weekend_activity, 1)
            },
            'pattern_type': pattern_type,
            'most_active_day': day_names[daily_activity.idxmax()]
        }
    
    def _analyze_weekly_trends(self, interactions: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ÙŠØ©"""
        weekly_activity = interactions.groupby('week_of_year').size()
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¹Ø§Ù…
        weeks = list(weekly_activity.index)
        activities = list(weekly_activity.values)
        
        if len(weeks) > 1:
            # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ù„Ù„Ø§ØªØ¬Ø§Ù‡
            correlation = np.corrcoef(weeks, activities)[0, 1]
            
            if correlation > 0.3:
                trend = "Ù…ØªØ²Ø§ÙŠØ¯"
            elif correlation < -0.3:
                trend = "Ù…ØªÙ†Ø§Ù‚Øµ"
            else:
                trend = "Ù…Ø³ØªÙ‚Ø±"
        else:
            trend = "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
        
        return {
            'weekly_distribution': weekly_activity.to_dict(),
            'trend': trend,
            'average_weekly_activity': weekly_activity.mean(),
            'activity_variance': weekly_activity.var()
        }
    
    def _analyze_monthly_patterns(self, interactions: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø´Ù‡Ø±ÙŠØ©"""
        monthly_activity = interactions.groupby('month').size()
        
        # Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø´Ù‡Ø± Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        month_names = {
            1: 'ÙŠÙ†Ø§ÙŠØ±', 2: 'ÙØ¨Ø±Ø§ÙŠØ±', 3: 'Ù…Ø§Ø±Ø³', 4: 'Ø£Ø¨Ø±ÙŠÙ„',
            5: 'Ù…Ø§ÙŠÙˆ', 6: 'ÙŠÙˆÙ†ÙŠÙˆ', 7: 'ÙŠÙˆÙ„ÙŠÙˆ', 8: 'Ø£ØºØ³Ø·Ø³',
            9: 'Ø³Ø¨ØªÙ…Ø¨Ø±', 10: 'Ø£ÙƒØªÙˆØ¨Ø±', 11: 'Ù†ÙˆÙÙ…Ø¨Ø±', 12: 'Ø¯ÙŠØ³Ù…Ø¨Ø±'
        }
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ©
        if len(monthly_activity) >= 3:
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ© Ø§Ù„Ø¨Ø³ÙŠØ·
            q1_activity = monthly_activity.get([1, 2, 3], pd.Series()).sum()  # Ø§Ù„Ø±Ø¨Ø¹ Ø§Ù„Ø£ÙˆÙ„
            q2_activity = monthly_activity.get([4, 5, 6], pd.Series()).sum()  # Ø§Ù„Ø±Ø¨Ø¹ Ø§Ù„Ø«Ø§Ù†ÙŠ
            q3_activity = monthly_activity.get([7, 8, 9], pd.Series()).sum()  # Ø§Ù„Ø±Ø¨Ø¹ Ø§Ù„Ø«Ø§Ù„Ø«
            q4_activity = monthly_activity.get([10, 11, 12], pd.Series()).sum()  # Ø§Ù„Ø±Ø¨Ø¹ Ø§Ù„Ø±Ø§Ø¨Ø¹
            
            seasonal_pattern = {
                'Ø§Ù„Ø´ØªØ§Ø¡': q1_activity,
                'Ø§Ù„Ø±Ø¨ÙŠØ¹': q2_activity,
                'Ø§Ù„ØµÙŠÙ': q3_activity,
                'Ø§Ù„Ø®Ø±ÙŠÙ': q4_activity
            }
            
            peak_season = max(seasonal_pattern, key=seasonal_pattern.get)
        else:
            seasonal_pattern = {}
            peak_season = "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
        
        return {
            'monthly_distribution': {month_names.get(month, str(month)): count 
                                   for month, count in monthly_activity.items()},
            'seasonal_pattern': seasonal_pattern,
            'peak_season': peak_season,
            'seasonality_strength': np.std(list(seasonal_pattern.values())) if seasonal_pattern else 0
        }
    
    def _analyze_reading_sessions(self, interactions: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©"""
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª ÙÙŠ Ø¬Ù„Ø³Ø§Øª (ÙØ¬ÙˆØ© Ø²Ù…Ù†ÙŠØ© > 30 Ø¯Ù‚ÙŠÙ‚Ø© = Ø¬Ù„Ø³Ø© Ø¬Ø¯ÙŠØ¯Ø©)
        interactions_sorted = interactions.sort_values('created_at')
        interactions_sorted['time_diff'] = pd.to_datetime(interactions_sorted['created_at']).diff()
        
        # ØªØ­Ø¯ÙŠØ¯ Ø¨Ø¯Ø§ÙŠØ§Øª Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        session_breaks = interactions_sorted['time_diff'] > timedelta(minutes=30)
        interactions_sorted['session_id'] = session_breaks.cumsum()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù„Ø³Ø§Øª
        session_stats = interactions_sorted.groupby('session_id').agg({
            'created_at': ['min', 'max', 'count'],
            'reading_time': 'sum'
        }).reset_index()
        
        session_stats.columns = ['session_id', 'start_time', 'end_time', 'articles_count', 'total_reading_time']
        session_stats['session_duration'] = (
            pd.to_datetime(session_stats['end_time']) - 
            pd.to_datetime(session_stats['start_time'])
        ).dt.total_seconds() / 60  # Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚
        
        return {
            'total_sessions': len(session_stats),
            'avg_session_duration': session_stats['session_duration'].mean(),
            'avg_articles_per_session': session_stats['articles_count'].mean(),
            'avg_reading_time_per_session': session_stats['total_reading_time'].mean(),
            'session_patterns': {
                'short_sessions': len(session_stats[session_stats['session_duration'] < 5]),
                'medium_sessions': len(session_stats[(session_stats['session_duration'] >= 5) & 
                                                   (session_stats['session_duration'] <= 30)]),
                'long_sessions': len(session_stats[session_stats['session_duration'] > 30])
            }
        }
    
    def _analyze_content_timing(self, interactions: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ ØªÙØ¶ÙŠÙ„Ø§Øª ØªÙˆÙ‚ÙŠØª Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        content_timing = {}
        
        # ØªØ­Ù„ÙŠÙ„ ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø­Ø³Ø¨ Ø§Ù„ÙˆÙ‚Øª
        if 'category' in interactions.columns:
            for category in interactions['category'].unique():
                category_data = interactions[interactions['category'] == category]
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„ÙƒÙ„ ÙØ¦Ø©
                hourly_dist = category_data.groupby('hour').size()
                peak_hour = hourly_dist.idxmax() if len(hourly_dist) > 0 else 0
                
                content_timing[category] = {
                    'peak_hour': peak_hour,
                    'total_interactions': len(category_data),
                    'hourly_distribution': hourly_dist.to_dict()
                }
        
        return content_timing
    
    def _identify_primary_pattern(self, patterns: Dict[str, Any]) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"""
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø³Ø· Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        hourly_pattern = patterns['hourly_distribution']['pattern_type']
        daily_pattern = patterns['daily_distribution']['pattern_type']
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
        if hourly_pattern == "ØµØ¨Ø§Ø­ÙŠ" and daily_pattern == "Ø£ÙŠØ§Ù… Ø§Ù„Ø¹Ù…Ù„":
            return "Ù…Ø­ØªØ±Ù Ù…Ù†Ø¸Ù…"
        elif hourly_pattern == "Ù…Ø³Ø§Ø¦ÙŠ" and daily_pattern == "Ù…ØªÙˆØ§Ø²Ù†":
            return "Ù‚Ø§Ø±Ø¦ Ù…Ø³Ø§Ø¦ÙŠ"
        elif daily_pattern == "Ø¹Ø·Ù„Ø© Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹":
            return "Ù‚Ø§Ø±Ø¦ ÙˆÙ‚Øª Ø§Ù„ÙØ±Ø§Øº"
        elif hourly_pattern == "Ù„ÙŠÙ„ÙŠ":
            return "Ù‚Ø§Ø±Ø¦ Ù„ÙŠÙ„ÙŠ"
        else:
            return "Ù…ØªÙ†ÙˆØ¹ Ø§Ù„Ø£ÙˆÙ‚Ø§Øª"
    
    def _calculate_activity_regularity(self, interactions: pd.DataFrame) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù†ØªØ¸Ø§Ù… Ø§Ù„Ù†Ø´Ø§Ø·"""
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¨Ø§ÙŠÙ† ÙÙŠ Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„ÙŠÙˆÙ…ÙŠ
        daily_counts = interactions.groupby(pd.to_datetime(interactions['created_at']).dt.date).size()
        
        if len(daily_counts) > 1:
            # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„ØªØ¨Ø§ÙŠÙ† (Ø£Ù‚Ù„ ØªØ¨Ø§ÙŠÙ† = Ø£ÙƒØ«Ø± Ø§Ù†ØªØ¸Ø§Ù…Ø§Ù‹)
            cv = daily_counts.std() / daily_counts.mean() if daily_counts.mean() > 0 else 1
            regularity = max(0, 1 - cv)  # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù†Ù‚Ø§Ø· Ù…Ù† 0 Ø¥Ù„Ù‰ 1
        else:
            regularity = 0.5  # Ù‚ÙŠÙ…Ø© Ù…ØªÙˆØ³Ø·Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø¯ÙˆØ¯Ø©
        
        return regularity
    
    def _calculate_temporal_consistency(self, activity_series: pd.Series) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§ØªØ³Ø§Ù‚ Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠ"""
        if len(activity_series) == 0:
            return 0.0
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù†Ø³Ø¨ÙŠ
        total_activity = activity_series.sum()
        if total_activity == 0:
            return 0.0
        
        distribution = activity_series / total_activity
        
        # Ø­Ø³Ø§Ø¨ Ø¥Ù†ØªØ±ÙˆØ¨ÙŠØ§ Ø§Ù„ØªÙˆØ²ÙŠØ¹ (Ø£Ù‚Ù„ Ø¥Ù†ØªØ±ÙˆØ¨ÙŠØ§ = Ø£ÙƒØ«Ø± ØªØ±ÙƒØ²Ø§Ù‹)
        entropy_val = entropy(distribution + 1e-10)  # Ø¥Ø¶Ø§ÙØ© Ù‚ÙŠÙ…Ø© ØµØºÙŠØ±Ø© Ù„ØªØ¬Ù†Ø¨ log(0)
        max_entropy = np.log(len(distribution))
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù†Ù‚Ø§Ø· Ø§ØªØ³Ø§Ù‚ (1 = Ù…ØªØ³Ù‚ Ø¬Ø¯Ø§Ù‹ØŒ 0 = Ù…ØªÙ†Ø§Ø«Ø± Ø¬Ø¯Ø§Ù‹)
        consistency = 1 - (entropy_val / max_entropy) if max_entropy > 0 else 0
        
        return consistency


class TopicalInterestExtractor:
    """
    Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ÙŠØ©
    Topical Interest Extractor
    """
    
    def __init__(self, config: InterestAnalysisConfig):
        self.config = config
        self.topic_models = {}
        self.interest_embeddings = {}
        
    def extract_topical_interests(self, user_content: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ÙŠØ©"""
        logger.info(f"ğŸ“š Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ÙŠØ© Ù…Ù† {len(user_content)} Ù…Ø­ØªÙˆÙ‰...")
        
        if not user_content:
            return {}
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù†ØµÙˆØµ
        texts = []
        for content in user_content:
            combined_text = f"{content.get('title', '')} {content.get('content', '')}"
            texts.append(combined_text)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ù†ØµÙŠØ©
        tfidf_features = self._extract_tfidf_features(texts)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
        topics = self._perform_topic_modeling(texts)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        keywords = self._extract_keywords(texts)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙØ¦Ø§Øª
        categories = self._analyze_categories(user_content)
        
        # Ø­Ø³Ø§Ø¨ Ù‚ÙˆØ© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù„ÙƒÙ„ Ù…ÙˆØ¶ÙˆØ¹
        interest_strengths = self._calculate_interest_strengths(user_content, topics)
        
        return {
            'topics': topics,
            'keywords': keywords,
            'categories': categories,
            'interest_strengths': interest_strengths,
            'tfidf_features': tfidf_features,
            'content_diversity': self._calculate_content_diversity(texts)
        }
    
    def _extract_tfidf_features(self, texts: List[str]) -> Dict[str, float]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø§Ù„Ù… TF-IDF"""
        if not texts:
            return {}
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØ¬Ù…Ø¹ TF-IDF
        vectorizer = TfidfVectorizer(
            max_features=self.config.tfidf_max_features,
            stop_words=None,  # Ø³Ù†ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ø§Ø­Ù‚Ø§Ù‹
            ngram_range=(1, 2)
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform(texts)
            feature_names = vectorizer.get_feature_names_out()
            
            # Ø­Ø³Ø§Ø¨ Ø£Ù‡Ù…ÙŠØ© ÙƒÙ„ Ù…Ø¹Ù„Ù…
            feature_scores = np.mean(tfidf_matrix.toarray(), axis=0)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ù…Ø¹ Ù†Ù‚Ø§Ø·Ù‡Ø§
            features_dict = dict(zip(feature_names, feature_scores))
            
            # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
            sorted_features = dict(sorted(features_dict.items(), 
                                        key=lambda x: x[1], reverse=True)[:50])
            
            return sorted_features
            
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø§Ù„Ù… TF-IDF: {str(e)}")
            return {}
    
    def _perform_topic_modeling(self, texts: List[str]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª"""
        if len(texts) < 3:
            return {}
        
        try:
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            vectorizer = TfidfVectorizer(
                max_features=1000,
                min_df=2,
                max_df=0.8,
                ngram_range=(1, 2)
            )
            
            tfidf_matrix = vectorizer.fit_transform(texts)
            feature_names = vectorizer.get_feature_names_out()
            
            # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ LDA
            n_topics = min(self.config.n_clusters, len(texts) // 2)
            lda_model = LatentDirichletAllocation(
                n_components=n_topics,
                random_state=42,
                max_iter=100
            )
            
            doc_topic_matrix = lda_model.fit_transform(tfidf_matrix)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
            topics = {}
            for topic_idx, topic in enumerate(lda_model.components_):
                top_words_idx = topic.argsort()[-10:][::-1]
                top_words = [feature_names[i] for i in top_words_idx]
                topic_strength = np.mean(doc_topic_matrix[:, topic_idx])
                
                topics[f"Ù…ÙˆØ¶ÙˆØ¹_{topic_idx}"] = {
                    'keywords': top_words,
                    'strength': float(topic_strength),
                    'description': ', '.join(top_words[:3])
                }
            
            return topics
            
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª: {str(e)}")
            return {}
    
    def _extract_keywords(self, texts: List[str]) -> Dict[str, float]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        if not texts:
            return {}
        
        # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ
        combined_text = ' '.join(texts)
        
        # ØªÙ†Ø¸ÙŠÙ ÙˆØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ
        words = re.findall(r'\b\w+\b', combined_text.lower())
        
        # Ø­Ø³Ø§Ø¨ ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        word_counts = Counter(words)
        
        # ØªØµÙÙŠØ© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© ÙˆØ§Ù„Ø´Ø§Ø¦Ø¹Ø©
        filtered_words = {
            word: count for word, count in word_counts.items()
            if len(word) > 2 and count > 1
        }
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Ù‚Ø§Ø·
        if filtered_words:
            max_count = max(filtered_words.values())
            normalized_keywords = {
                word: count / max_count 
                for word, count in filtered_words.items()
            }
            
            # Ø£Ù‡Ù… 20 ÙƒÙ„Ù…Ø©
            top_keywords = dict(sorted(normalized_keywords.items(), 
                                     key=lambda x: x[1], reverse=True)[:20])
            
            return top_keywords
        
        return {}
    
    def _analyze_categories(self, user_content: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙØ¦Ø§Øª"""
        categories = {}
        
        # Ø¬Ù…Ø¹ Ø§Ù„ÙØ¦Ø§Øª ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        category_counts = Counter()
        for content in user_content:
            category = content.get('category', 'Ø£Ø®Ø±Ù‰')
            interaction_type = content.get('interaction_type', 'view')
            
            # ÙˆØ²Ù† Ø§Ù„ØªÙØ§Ø¹Ù„ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
            weight = {
                'view': 1.0,
                'like': 3.0,
                'save': 4.0,
                'share': 5.0,
                'comment': 4.5
            }.get(interaction_type, 1.0)
            
            category_counts[category] += weight
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Ù‚Ø§Ø·
        if category_counts:
            total_weight = sum(category_counts.values())
            
            for category, weight in category_counts.items():
                preference_score = weight / total_weight
                
                categories[category] = {
                    'preference_score': preference_score,
                    'total_interactions': weight,
                    'relative_interest': 'Ø¹Ø§Ù„ÙŠ' if preference_score > 0.3 else 
                                       'Ù…ØªÙˆØ³Ø·' if preference_score > 0.1 else 'Ù…Ù†Ø®ÙØ¶'
                }
        
        return categories
    
    def _calculate_interest_strengths(self, user_content: List[Dict[str, Any]], 
                                    topics: Dict[str, Any]) -> Dict[str, float]:
        """Ø­Ø³Ø§Ø¨ Ù‚ÙˆØ© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù„ÙƒÙ„ Ù…ÙˆØ¶ÙˆØ¹"""
        interest_strengths = {}
        
        # Ø­Ø³Ø§Ø¨ Ù‚ÙˆØ© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª
        for topic_name, topic_data in topics.items():
            total_strength = 0.0
            topic_keywords = topic_data.get('keywords', [])
            
            for content in user_content:
                content_text = f"{content.get('title', '')} {content.get('content', '')}"
                
                # Ø­Ø³Ø§Ø¨ ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
                keyword_matches = sum(1 for keyword in topic_keywords 
                                    if keyword.lower() in content_text.lower())
                
                if keyword_matches > 0:
                    # ÙˆØ²Ù† Ø§Ù„ØªÙØ§Ø¹Ù„
                    interaction_weight = {
                        'view': 1.0, 'like': 3.0, 'save': 4.0, 
                        'share': 5.0, 'comment': 4.5
                    }.get(content.get('interaction_type', 'view'), 1.0)
                    
                    # Ù‚ÙˆØ© Ø§Ù„ØªØ·Ø§Ø¨Ù‚
                    match_strength = keyword_matches / len(topic_keywords)
                    
                    total_strength += match_strength * interaction_weight
            
            interest_strengths[topic_name] = total_strength
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù‚ÙˆØ©
        if interest_strengths:
            max_strength = max(interest_strengths.values())
            if max_strength > 0:
                interest_strengths = {
                    topic: strength / max_strength 
                    for topic, strength in interest_strengths.items()
                }
        
        return interest_strengths
    
    def _calculate_content_diversity(self, texts: List[str]) -> float:
        """Ø­Ø³Ø§Ø¨ ØªÙ†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        if len(texts) < 2:
            return 0.0
        
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØµÙÙˆÙØ© TF-IDF
            vectorizer = TfidfVectorizer(max_features=500)
            tfidf_matrix = vectorizer.fit_transform(texts)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„Ù†ØµÙˆØµ
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ´Ø§Ø¨Ù‡ (Ø£Ù‚Ù„ ØªØ´Ø§Ø¨Ù‡ = Ø£ÙƒØ«Ø± ØªÙ†ÙˆØ¹Ø§Ù‹)
            avg_similarity = np.mean(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)])
            
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù†Ù‚Ø§Ø· ØªÙ†ÙˆØ¹
            diversity_score = 1 - avg_similarity
            
            return max(0.0, min(1.0, diversity_score))
            
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø­Ø³Ø§Ø¨ ØªÙ†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {str(e)}")
            return 0.5


class PersonalityAnalyzer:
    """
    Ù…Ø­Ù„Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ© ÙˆØ§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©
    Personality and Behavioral Traits Analyzer
    """
    
    def __init__(self, config: InterestAnalysisConfig):
        self.config = config
        self.personality_indicators = {
            'openness': ['ØªÙ†ÙˆØ¹', 'Ø§Ø³ØªÙƒØ´Ø§Ù', 'ÙØ¶ÙˆÙ„', 'Ø¥Ø¨Ø¯Ø§Ø¹', 'Ø¬Ø¯ÙŠØ¯'],
            'conscientiousness': ['Ù…Ù†ØªØ¸Ù…', 'Ù…Ø³ØªÙ…Ø±', 'Ø¯Ù‚ÙŠÙ‚', 'Ù…Ø³Ø¤ÙˆÙ„'],
            'extraversion': ['ØªÙØ§Ø¹Ù„', 'Ù…Ø´Ø§Ø±ÙƒØ©', 'Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ', 'Ù†Ø´Ø·'],
            'agreeableness': ['ØªØ¹Ø§ÙˆÙ†', 'Ù…Ø³Ø§Ø¹Ø¯Ø©', 'ÙˆØ¯', 'ØªÙÙ‡Ù…'],
            'neuroticism': ['Ù‚Ù„Ù‚', 'ØªÙˆØªØ±', 'Ø³Ù„Ø¨ÙŠ', 'Ø¹Ø§Ø·ÙÙŠ'],
            'curiosity': ['Ù„Ù…Ø§Ø°Ø§', 'ÙƒÙŠÙ', 'Ù…Ø¹Ø±ÙØ©', 'ØªØ¹Ù„Ù…', 'ÙÙ‡Ù…']
        }
    
    def analyze_personality(self, user_data: Dict[str, Any]) -> Dict[str, float]:
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø®ØµÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        logger.info("ğŸ§  ØªØ­Ù„ÙŠÙ„ Ø´Ø®ØµÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…...")
        
        personality_scores = {}
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù…Ø§Øª Ù…Ù† Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙØ§Ø¹Ù„
        interaction_patterns = user_data.get('behavior_patterns', {})
        content_preferences = user_data.get('topical_interests', {})
        temporal_patterns = user_data.get('temporal_patterns', {})
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ù†ÙØªØ§Ø­ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        personality_scores['openness'] = self._analyze_openness(
            content_preferences, interaction_patterns
        )
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¶Ù…ÙŠØ± ÙˆØ§Ù„Ù†Ø¸Ø§Ù…
        personality_scores['conscientiousness'] = self._analyze_conscientiousness(
            temporal_patterns, interaction_patterns
        )
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ù†Ø¨Ø³Ø§Ø·
        personality_scores['extraversion'] = self._analyze_extraversion(
            interaction_patterns
        )
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø¨ÙˆÙ„ÙŠØ©
        personality_scores['agreeableness'] = self._analyze_agreeableness(
            interaction_patterns, content_preferences
        )
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹ØµØ§Ø¨ÙŠØ©
        personality_scores['neuroticism'] = self._analyze_neuroticism(
            interaction_patterns, temporal_patterns
        )
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙØ¶ÙˆÙ„
        personality_scores['curiosity'] = self._analyze_curiosity(
            content_preferences, interaction_patterns
        )
        
        return personality_scores
    
    def _analyze_openness(self, content_prefs: Dict, interaction_patterns: Dict) -> float:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ù†ÙØªØ§Ø­ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø§Ø±Ø¨"""
        openness_score = 0.0
        
        # ØªÙ†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content_diversity = content_prefs.get('content_diversity', 0.0)
        openness_score += content_diversity * 0.4
        
        # Ø§Ø³ØªÙƒØ´Ø§Ù ÙØ¦Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©
        categories = content_prefs.get('categories', {})
        category_count = len(categories)
        category_diversity = min(category_count / 10, 1.0)  # ØªØ·Ø¨ÙŠØ¹ Ù„Ù€ 10 ÙØ¦Ø§Øª
        openness_score += category_diversity * 0.3
        
        # ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ù…Ø­ØªÙˆÙ‰ Ù…ØªÙ†ÙˆØ¹
        if 'exploration_rate' in interaction_patterns:
            exploration_rate = interaction_patterns['exploration_rate']
            openness_score += exploration_rate * 0.3
        
        return min(openness_score, 1.0)
    
    def _analyze_conscientiousness(self, temporal_patterns: Dict, 
                                 interaction_patterns: Dict) -> float:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¶Ù…ÙŠØ± ÙˆØ§Ù„Ù†Ø¸Ø§Ù…"""
        conscientiousness_score = 0.0
        
        # Ø§Ù†ØªØ¸Ø§Ù… Ø§Ù„Ù†Ø´Ø§Ø·
        if 'activity_regularity' in temporal_patterns:
            regularity = temporal_patterns['activity_regularity']
            conscientiousness_score += regularity * 0.5
        
        # Ø§ØªØ³Ø§Ù‚ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©
        if 'hourly_distribution' in temporal_patterns:
            consistency = temporal_patterns['hourly_distribution'].get('consistency', 0.0)
            conscientiousness_score += consistency * 0.3
        
        # Ù…Ø¹Ø¯Ù„ Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©
        if 'completion_rate' in interaction_patterns:
            completion_rate = interaction_patterns['completion_rate']
            conscientiousness_score += completion_rate * 0.2
        
        return min(conscientiousness_score, 1.0)
    
    def _analyze_extraversion(self, interaction_patterns: Dict) -> float:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ù†Ø¨Ø³Ø§Ø·"""
        extraversion_score = 0.0
        
        # Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ©
        sharing_rate = interaction_patterns.get('sharing_rate', 0.0)
        extraversion_score += sharing_rate * 0.4
        
        # Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª
        commenting_rate = interaction_patterns.get('commenting_rate', 0.0)
        extraversion_score += commenting_rate * 0.4
        
        # ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ
        social_content_preference = interaction_patterns.get('social_content_preference', 0.0)
        extraversion_score += social_content_preference * 0.2
        
        return min(extraversion_score, 1.0)
    
    def _analyze_agreeableness(self, interaction_patterns: Dict, 
                             content_prefs: Dict) -> float:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø¨ÙˆÙ„ÙŠØ©"""
        agreeableness_score = 0.0
        
        # ØªÙØ¶ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠ
        positive_content_rate = interaction_patterns.get('positive_content_rate', 0.5)
        agreeableness_score += positive_content_rate * 0.4
        
        # Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¥Ø¹Ø¬Ø§Ø¨ Ù…Ù‚Ø§Ø¨Ù„ Ø¹Ø¯Ù… Ø§Ù„Ø¥Ø¹Ø¬Ø§Ø¨
        like_rate = interaction_patterns.get('like_rate', 0.0)
        agreeableness_score += like_rate * 0.3
        
        # ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø«ÙŠØ± Ù„Ù„Ø¬Ø¯Ù„
        controversial_avoidance = 1.0 - interaction_patterns.get('controversial_content_rate', 0.0)
        agreeableness_score += controversial_avoidance * 0.3
        
        return min(agreeableness_score, 1.0)
    
    def _analyze_neuroticism(self, interaction_patterns: Dict, 
                           temporal_patterns: Dict) -> float:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹ØµØ§Ø¨ÙŠØ©"""
        neuroticism_score = 0.0
        
        # Ø¹Ø¯Ù… Ø§Ù†ØªØ¸Ø§Ù… Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù†Ø´Ø§Ø·
        if 'activity_regularity' in temporal_patterns:
            irregularity = 1.0 - temporal_patterns['activity_regularity']
            neuroticism_score += irregularity * 0.3
        
        # ØªÙØ¶ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø³Ù„Ø¨ÙŠ Ø£Ùˆ Ø§Ù„Ù…Ù‚Ù„Ù‚
        negative_content_rate = interaction_patterns.get('negative_content_rate', 0.0)
        neuroticism_score += negative_content_rate * 0.4
        
        # ØªÙ‚Ù„Ø¨Ø§Øª ÙÙŠ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        usage_variability = interaction_patterns.get('usage_variability', 0.0)
        neuroticism_score += usage_variability * 0.3
        
        return min(neuroticism_score, 1.0)
    
    def _analyze_curiosity(self, content_prefs: Dict, interaction_patterns: Dict) -> float:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙØ¶ÙˆÙ„"""
        curiosity_score = 0.0
        
        # ØªÙ†ÙˆØ¹ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
        topic_diversity = len(content_prefs.get('topics', {})) / 10  # ØªØ·Ø¨ÙŠØ¹ Ù„Ù€ 10 Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
        curiosity_score += min(topic_diversity, 1.0) * 0.4
        
        # Ù…Ø¹Ø¯Ù„ Ø§Ø³ØªÙƒØ´Ø§Ù Ù…Ø­ØªÙˆÙ‰ Ø¬Ø¯ÙŠØ¯
        exploration_rate = interaction_patterns.get('new_content_exploration_rate', 0.0)
        curiosity_score += exploration_rate * 0.4
        
        # Ø¹Ù…Ù‚ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©
        reading_depth = interaction_patterns.get('average_reading_depth', 0.0)
        curiosity_score += reading_depth * 0.2
        
        return min(curiosity_score, 1.0)


class UserInterestAnalysisEngine:
    """
    Ù…Ø­Ø±Ùƒ ØªØ­Ù„ÙŠÙ„ Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    Main User Interest Analysis Engine
    """
    
    def __init__(self, config: InterestAnalysisConfig):
        self.config = config
        self.temporal_analyzer = TemporalInterestAnalyzer(config)
        self.topical_extractor = TopicalInterestExtractor(config)
        self.personality_analyzer = PersonalityAnalyzer(config)
        self.user_profiles = {}
        
    def analyze_user_interests(self, user_id: str, user_interactions: pd.DataFrame,
                             user_content: List[Dict[str, Any]]) -> InterestProfile:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø´Ø§Ù…Ù„"""
        logger.info(f"ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id}...")
        
        # Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ
        temporal_patterns = self.temporal_analyzer.analyze_temporal_patterns(user_interactions)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ÙŠØ©
        topical_interests = self.topical_extractor.extract_topical_interests(user_content)
        
        # ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³Ù„ÙˆÙƒ
        behavior_patterns = self._analyze_behavior_patterns(user_interactions)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø®ØµÙŠØ©
        user_data = {
            'behavior_patterns': behavior_patterns,
            'topical_interests': topical_interests,
            'temporal_patterns': temporal_patterns
        }
        personality_traits = self.personality_analyzer.analyze_personality(user_data)
        
        # Ø­Ø³Ø§Ø¨ ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„ØªÙØ§Ø¹Ù„
        engagement_preferences = self._calculate_engagement_preferences(user_interactions)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        diversity_score = self._calculate_diversity_score(topical_interests, behavior_patterns)
        curiosity_score = personality_traits.get('curiosity', 0.0)
        consistency_score = temporal_patterns.get('activity_regularity', 0.0)
        
        # Ø¨Ù†Ø§Ø¡ Ù…Ù„Ù Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª
        interest_profile = InterestProfile(
            user_id=user_id,
            interests=self._build_interests_dict(topical_interests, temporal_patterns),
            personality_traits=personality_traits,
            behavior_patterns=behavior_patterns,
            engagement_preferences=engagement_preferences,
            temporal_preferences=temporal_patterns,
            diversity_score=diversity_score,
            curiosity_score=curiosity_score,
            consistency_score=consistency_score,
            confidence_score=self._calculate_confidence_score(user_interactions, user_content)
        )
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
        self.user_profiles[user_id] = interest_profile
        
        logger.info(f"âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id}")
        return interest_profile
    
    def _analyze_behavior_patterns(self, interactions: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³Ù„ÙˆÙƒ"""
        patterns = {}
        
        if len(interactions) == 0:
            return patterns
        
        # Ù…Ø¹Ø¯Ù„Ø§Øª Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        total_interactions = len(interactions)
        
        interaction_counts = interactions['interaction_type'].value_counts()
        
        patterns['like_rate'] = interaction_counts.get('like', 0) / total_interactions
        patterns['save_rate'] = interaction_counts.get('save', 0) / total_interactions
        patterns['share_rate'] = interaction_counts.get('share', 0) / total_interactions
        patterns['comment_rate'] = interaction_counts.get('comment', 0) / total_interactions
        patterns['view_rate'] = interaction_counts.get('view', 0) / total_interactions
        
        # Ù…Ø¹Ø¯Ù„ Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©
        if 'read_percentage' in interactions.columns:
            avg_read_percentage = interactions['read_percentage'].mean()
            patterns['completion_rate'] = avg_read_percentage / 100
        else:
            patterns['completion_rate'] = 0.5  # Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        
        # ØªÙØ¶ÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        if 'category' in interactions.columns:
            category_distribution = interactions['category'].value_counts(normalize=True)
            patterns['category_preferences'] = category_distribution.to_dict()
        
        # ØªØ­Ù„ÙŠÙ„ Ø¹Ù…Ù‚ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©
        if 'reading_time' in interactions.columns:
            avg_reading_time = interactions['reading_time'].mean()
            patterns['average_reading_depth'] = min(avg_reading_time / 300, 1.0)  # ØªØ·Ø¨ÙŠØ¹ Ù„Ù€ 5 Ø¯Ù‚Ø§Ø¦Ù‚
        
        # Ù…Ø¹Ø¯Ù„ Ø§Ø³ØªÙƒØ´Ø§Ù Ù…Ø­ØªÙˆÙ‰ Ø¬Ø¯ÙŠØ¯
        unique_articles = interactions['article_id'].nunique()
        patterns['exploration_rate'] = min(unique_articles / max(total_interactions, 1), 1.0)
        
        return patterns
    
    def _calculate_engagement_preferences(self, interactions: pd.DataFrame) -> Dict[str, float]:
        """Ø­Ø³Ø§Ø¨ ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„ØªÙØ§Ø¹Ù„"""
        preferences = {}
        
        if len(interactions) == 0:
            return preferences
        
        # ØªÙØ¶ÙŠÙ„ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„ØªÙØ§Ø¹Ù„
        if 'created_at' in interactions.columns:
            interactions['hour'] = pd.to_datetime(interactions['created_at']).dt.hour
            hourly_dist = interactions['hour'].value_counts(normalize=True)
            
            # ØªØ­Ø¯ÙŠØ¯ ÙØªØ±Ø§Øª Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ù…ÙØ¶Ù„Ø©
            morning_activity = hourly_dist.get(range(6, 12), pd.Series()).sum()
            afternoon_activity = hourly_dist.get(range(12, 18), pd.Series()).sum()
            evening_activity = hourly_dist.get(range(18, 24), pd.Series()).sum()
            night_activity = hourly_dist.get(range(0, 6), pd.Series()).sum()
            
            preferences['morning_preference'] = morning_activity
            preferences['afternoon_preference'] = afternoon_activity
            preferences['evening_preference'] = evening_activity
            preferences['night_preference'] = night_activity
        
        # ØªÙØ¶ÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªÙØ§Ø¹Ù„
        interaction_dist = interactions['interaction_type'].value_counts(normalize=True)
        for interaction_type, ratio in interaction_dist.items():
            preferences[f'{interaction_type}_preference'] = ratio
        
        # ØªÙØ¶ÙŠÙ„ Ø·ÙˆÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        if 'content_length' in interactions.columns:
            avg_content_length = interactions['content_length'].mean()
            preferences['content_length_preference'] = min(avg_content_length / 1000, 1.0)
        
        return preferences
    
    def _build_interests_dict(self, topical_interests: Dict[str, Any], 
                            temporal_patterns: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
        """Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…Ù†Ø¸Ù…"""
        interests = {}
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ÙŠØ©
        topics = topical_interests.get('topics', {})
        for topic_name, topic_data in topics.items():
            interests[topic_name] = {
                'type': InterestType.TOPICAL.value,
                'strength': topic_data.get('strength', 0.0),
                'keywords': topic_data.get('keywords', []),
                'description': topic_data.get('description', ''),
                'last_interaction': datetime.now()
            }
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©
        categories = topical_interests.get('categories', {})
        for category, category_data in categories.items():
            interests[f"ÙØ¦Ø©_{category}"] = {
                'type': InterestType.TOPICAL.value,
                'strength': category_data.get('preference_score', 0.0),
                'keywords': [category],
                'description': f"Ø§Ù‡ØªÙ…Ø§Ù… Ø¨ÙØ¦Ø© {category}",
                'last_interaction': datetime.now()
            }
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠØ© ÙƒØ§Ù‡ØªÙ…Ø§Ù…Ø§Øª
        primary_pattern = temporal_patterns.get('primary_usage_pattern', '')
        if primary_pattern:
            interests[f"Ù†Ù…Ø·_{primary_pattern}"] = {
                'type': InterestType.TEMPORAL.value,
                'strength': 0.8,
                'keywords': [primary_pattern],
                'description': f"Ù†Ù…Ø· Ø§Ø³ØªØ®Ø¯Ø§Ù… {primary_pattern}",
                'last_interaction': datetime.now()
            }
        
        return interests
    
    def _calculate_diversity_score(self, topical_interests: Dict[str, Any], 
                                 behavior_patterns: Dict[str, Any]) -> float:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙ†ÙˆØ¹"""
        diversity_score = 0.0
        
        # ØªÙ†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content_diversity = topical_interests.get('content_diversity', 0.0)
        diversity_score += content_diversity * 0.4
        
        # ØªÙ†ÙˆØ¹ Ø§Ù„ÙØ¦Ø§Øª
        categories_count = len(topical_interests.get('categories', {}))
        category_diversity = min(categories_count / 10, 1.0)
        diversity_score += category_diversity * 0.3
        
        # ØªÙ†ÙˆØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªÙØ§Ø¹Ù„
        interaction_types = len([k for k in behavior_patterns.keys() if k.endswith('_rate') and behavior_patterns[k] > 0])
        interaction_diversity = min(interaction_types / 5, 1.0)
        diversity_score += interaction_diversity * 0.3
        
        return min(diversity_score, 1.0)
    
    def _calculate_confidence_score(self, interactions: pd.DataFrame, 
                                  content: List[Dict[str, Any]]) -> float:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        confidence = 0.0
        
        # Ø¹Ø¯Ø¯ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª
        interaction_count = len(interactions)
        interaction_confidence = min(interaction_count / 100, 1.0)
        confidence += interaction_confidence * 0.4
        
        # ØªÙ†ÙˆØ¹ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª
        interaction_types = interactions['interaction_type'].nunique() if len(interactions) > 0 else 0
        type_confidence = min(interaction_types / 5, 1.0)
        confidence += type_confidence * 0.3
        
        # ÙØªØ±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if len(interactions) > 0 and 'created_at' in interactions.columns:
            date_range = (pd.to_datetime(interactions['created_at']).max() - 
                         pd.to_datetime(interactions['created_at']).min()).days
            time_confidence = min(date_range / 30, 1.0)
            confidence += time_confidence * 0.3
        
        return confidence
    
    def update_interest_profile(self, user_id: str, new_interactions: pd.DataFrame,
                              new_content: List[Dict[str, Any]]):
        """ØªØ­Ø¯ÙŠØ« Ù…Ù„Ù Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª"""
        if user_id not in self.user_profiles:
            return self.analyze_user_interests(user_id, new_interactions, new_content)
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¹ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
        current_profile = self.user_profiles[user_id]
        
        # ØªØ·Ø¨ÙŠÙ‚ ØªØ±Ø§Ø¬Ø¹ Ø²Ù…Ù†ÙŠ Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
        for interest_name, interest_data in current_profile.interests.items():
            interest_data['strength'] *= self.config.interest_decay_rate
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        new_profile = self.analyze_user_interests(user_id, new_interactions, new_content)
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª
        merged_interests = current_profile.interests.copy()
        for interest_name, interest_data in new_profile.interests.items():
            if interest_name in merged_interests:
                # Ø¯Ù…Ø¬ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©
                merged_interests[interest_name]['strength'] = max(
                    merged_interests[interest_name]['strength'],
                    interest_data['strength']
                )
            else:
                merged_interests[interest_name] = interest_data
        
        # ØªØµÙÙŠØ© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ø¶Ø¹ÙŠÙØ©
        filtered_interests = {
            name: data for name, data in merged_interests.items()
            if data['strength'] >= self.config.min_interest_strength
        }
        
        # ØªØ­Ø¯ÙŠØ¯ Ø£Ù‡Ù… Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª
        if len(filtered_interests) > self.config.max_interests_per_user:
            sorted_interests = sorted(
                filtered_interests.items(),
                key=lambda x: x[1]['strength'],
                reverse=True
            )[:self.config.max_interests_per_user]
            filtered_interests = dict(sorted_interests)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ù„Ù
        current_profile.interests = filtered_interests
        current_profile.last_updated = datetime.now()
        
        return current_profile
    
    def get_user_interest_summary(self, user_id: str) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ø®Øµ Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        if user_id not in self.user_profiles:
            return {}
        
        profile = self.user_profiles[user_id]
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù‚ÙˆØ©
        sorted_interests = sorted(
            profile.interests.items(),
            key=lambda x: x[1]['strength'],
            reverse=True
        )
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
        interests_by_type = defaultdict(list)
        for name, data in sorted_interests:
            interests_by_type[data['type']].append({
                'name': name,
                'strength': data['strength'],
                'description': data.get('description', '')
            })
        
        return {
            'user_id': user_id,
            'total_interests': len(profile.interests),
            'top_interests': sorted_interests[:5],
            'interests_by_type': dict(interests_by_type),
            'personality_summary': self._summarize_personality(profile.personality_traits),
            'behavior_summary': self._summarize_behavior(profile.behavior_patterns),
            'scores': {
                'diversity': profile.diversity_score,
                'curiosity': profile.curiosity_score,
                'consistency': profile.consistency_score,
                'confidence': profile.confidence_score
            },
            'last_updated': profile.last_updated.isoformat()
        }
    
    def _summarize_personality(self, traits: Dict[str, float]) -> Dict[str, str]:
        """ØªÙ„Ø®ÙŠØµ Ø§Ù„Ø´Ø®ØµÙŠØ©"""
        summary = {}
        
        trait_descriptions = {
            'openness': ('Ù…Ù†ÙØªØ­ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø§Ø±Ø¨', 'Ù…Ø­Ø§ÙØ¸'),
            'conscientiousness': ('Ù…Ù†Ø¸Ù… ÙˆÙ…Ù†Ø¶Ø¨Ø·', 'Ø¹ÙÙˆÙŠ ÙˆÙ…Ø±Ù†'),
            'extraversion': ('Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ ÙˆÙ†Ø´Ø·', 'Ù‡Ø§Ø¯Ø¦ ÙˆÙ…ØªØ­ÙØ¸'),
            'agreeableness': ('Ù…ØªØ¹Ø§ÙˆÙ† ÙˆÙ…ØªÙÙ‡Ù…', 'ØªÙ†Ø§ÙØ³ÙŠ ÙˆÙ…Ø¨Ø§Ø´Ø±'),
            'neuroticism': ('Ø­Ø³Ø§Ø³ ÙˆØ¹Ø§Ø·ÙÙŠ', 'Ù…Ø³ØªÙ‚Ø± ÙˆÙ‡Ø§Ø¯Ø¦'),
            'curiosity': ('ÙØ¶ÙˆÙ„ÙŠ ÙˆÙ…ØªØ¹Ù„Ù…', 'Ø¹Ù…Ù„ÙŠ ÙˆÙ…Ø±ÙƒØ²')
        }
        
        for trait, score in traits.items():
            if trait in trait_descriptions:
                high_desc, low_desc = trait_descriptions[trait]
                if score > 0.6:
                    summary[trait] = high_desc
                elif score < 0.4:
                    summary[trait] = low_desc
                else:
                    summary[trait] = 'Ù…ØªÙˆØ§Ø²Ù†'
        
        return summary
    
    def _summarize_behavior(self, patterns: Dict[str, Any]) -> Dict[str, str]:
        """ØªÙ„Ø®ÙŠØµ Ø§Ù„Ø³Ù„ÙˆÙƒ"""
        summary = {}
        
        # Ù†Ù…Ø· Ø§Ù„ØªÙØ§Ø¹Ù„
        interaction_rates = {
            'like_rate': patterns.get('like_rate', 0),
            'save_rate': patterns.get('save_rate', 0),
            'share_rate': patterns.get('share_rate', 0),
            'comment_rate': patterns.get('comment_rate', 0)
        }
        
        dominant_interaction = max(interaction_rates, key=interaction_rates.get)
        interaction_labels = {
            'like_rate': 'Ù…Ø¹Ø¬Ø¨ Ù†Ø´Ø·',
            'save_rate': 'Ø¬Ø§Ù…Ø¹ Ù„Ù„Ù…Ø­ØªÙˆÙ‰',
            'share_rate': 'Ù…Ø´Ø§Ø±Ùƒ Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ',
            'comment_rate': 'Ù…ØªÙØ§Ø¹Ù„ Ø¨Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª'
        }
        
        summary['interaction_style'] = interaction_labels.get(dominant_interaction, 'Ù…ØªÙØ§Ø¹Ù„ Ù…ØªÙ†ÙˆØ¹')
        
        # Ù†Ù…Ø· Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©
        completion_rate = patterns.get('completion_rate', 0.5)
        if completion_rate > 0.8:
            summary['reading_style'] = 'Ù‚Ø§Ø±Ø¦ Ø¹Ù…ÙŠÙ‚'
        elif completion_rate > 0.5:
            summary['reading_style'] = 'Ù‚Ø§Ø±Ø¦ Ù…ØªÙˆØ³Ø·'
        else:
            summary['reading_style'] = 'Ù…ØªØµÙØ­ Ø³Ø±ÙŠØ¹'
        
        # Ù†Ù…Ø· Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù
        exploration_rate = patterns.get('exploration_rate', 0.5)
        if exploration_rate > 0.7:
            summary['exploration_style'] = 'Ù…Ø³ØªÙƒØ´Ù Ù†Ø´Ø·'
        elif exploration_rate > 0.4:
            summary['exploration_style'] = 'Ù…ØªÙˆØ§Ø²Ù†'
        else:
            summary['exploration_style'] = 'ÙŠÙØ¶Ù„ Ø§Ù„Ù…Ø£Ù„ÙˆÙ'
        
        return summary
    
    def save_profiles(self, filepath: str):
        """Ø­ÙØ¸ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†"""
        logger.info(f"ğŸ’¾ Ø­ÙØ¸ Ù…Ù„ÙØ§Øª Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙÙŠ {filepath}")
        
        # ØªØ­ÙˆÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ù‚Ø§Ø¨Ù„ Ù„Ù„Ø­ÙØ¸
        serializable_profiles = {}
        for user_id, profile in self.user_profiles.items():
            serializable_profiles[user_id] = {
                'user_id': profile.user_id,
                'interests': profile.interests,
                'personality_traits': profile.personality_traits,
                'behavior_patterns': profile.behavior_patterns,
                'engagement_preferences': profile.engagement_preferences,
                'temporal_preferences': profile.temporal_preferences,
                'diversity_score': profile.diversity_score,
                'curiosity_score': profile.curiosity_score,
                'consistency_score': profile.consistency_score,
                'confidence_score': profile.confidence_score,
                'last_updated': profile.last_updated.isoformat()
            }
        
        save_data = {
            'config': self.config,
            'user_profiles': serializable_profiles,
            'save_timestamp': datetime.now().isoformat()
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(save_data, f)
        
        logger.info(f"âœ… ØªÙ… Ø­ÙØ¸ {len(self.user_profiles)} Ù…Ù„Ù Ù…Ø³ØªØ®Ø¯Ù…")
    
    def load_profiles(self, filepath: str):
        """ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†"""
        logger.info(f"ğŸ“‚ ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ù…Ù† {filepath}")
        
        try:
            with open(filepath, 'rb') as f:
                save_data = pickle.load(f)
            
            self.config = save_data['config']
            
            # Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
            self.user_profiles = {}
            for user_id, profile_data in save_data['user_profiles'].items():
                profile = InterestProfile(
                    user_id=profile_data['user_id'],
                    interests=profile_data['interests'],
                    personality_traits=profile_data['personality_traits'],
                    behavior_patterns=profile_data['behavior_patterns'],
                    engagement_preferences=profile_data['engagement_preferences'],
                    temporal_preferences=profile_data['temporal_preferences'],
                    diversity_score=profile_data['diversity_score'],
                    curiosity_score=profile_data['curiosity_score'],
                    consistency_score=profile_data['consistency_score'],
                    confidence_score=profile_data['confidence_score'],
                    last_updated=datetime.fromisoformat(profile_data['last_updated'])
                )
                self.user_profiles[user_id] = profile
            
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.user_profiles)} Ù…Ù„Ù Ù…Ø³ØªØ®Ø¯Ù…")
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†: {str(e)}")
            raise


# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙƒÙˆÙŠÙ†
    config = InterestAnalysisConfig(
        n_clusters=8,
        temporal_window_days=30,
        max_interests_per_user=15
    )
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©
    sample_interactions = pd.DataFrame({
        'user_id': ['user_1'] * 100,
        'article_id': [f'article_{i}' for i in range(100)],
        'interaction_type': np.random.choice(['view', 'like', 'save', 'share'], 100),
        'created_at': pd.date_range('2024-01-01', periods=100, freq='H'),
        'reading_time': np.random.randint(30, 600, 100),
        'read_percentage': np.random.randint(20, 100, 100),
        'category': np.random.choice(['Ø³ÙŠØ§Ø³Ø©', 'Ø±ÙŠØ§Ø¶Ø©', 'ØªÙ‚Ù†ÙŠØ©', 'Ø§Ù‚ØªØµØ§Ø¯'], 100)
    })
    
    sample_content = [
        {
            'id': f'article_{i}',
            'title': f'Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ù‚Ø§Ù„ {i}',
            'content': f'Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù‚Ø§Ù„ {i} ÙŠØªØ­Ø¯Ø« Ø¹Ù† Ù…ÙˆØ¶ÙˆØ¹ Ù…Ù‡Ù…',
            'category': np.random.choice(['Ø³ÙŠØ§Ø³Ø©', 'Ø±ÙŠØ§Ø¶Ø©', 'ØªÙ‚Ù†ÙŠØ©', 'Ø§Ù‚ØªØµØ§Ø¯']),
            'interaction_type': np.random.choice(['view', 'like', 'save'])
        }
        for i in range(50)
    ]
    
    # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ´ØºÙŠÙ„ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„
    analysis_engine = UserInterestAnalysisEngine(config)
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    interest_profile = analysis_engine.analyze_user_interests(
        user_id='user_1',
        user_interactions=sample_interactions,
        user_content=sample_content
    )
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ø®Øµ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª
    summary = analysis_engine.get_user_interest_summary('user_1')
    
    print("ğŸ¯ Ù…Ù„Ø®Øµ Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:")
    print(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª: {summary['total_interests']}")
    print(f"Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙ†ÙˆØ¹: {summary['scores']['diversity']:.2f}")
    print(f"Ù†Ù‚Ø§Ø· Ø§Ù„ÙØ¶ÙˆÙ„: {summary['scores']['curiosity']:.2f}")
    print(f"Ù†Ù‚Ø§Ø· Ø§Ù„Ø«Ù‚Ø©: {summary['scores']['confidence']:.2f}")
    
    print("\nğŸ” Ø£Ù‡Ù… Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª:")
    for name, data in summary['top_interests'][:3]:
        print(f"  {name}: {data['strength']:.2f}")
    
    print(f"\nğŸ§  Ù…Ù„Ø®Øµ Ø§Ù„Ø´Ø®ØµÙŠØ©:")
    for trait, desc in summary['personality_summary'].items():
        print(f"  {trait}: {desc}")
