#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø°ÙƒÙŠ - Ø³Ø¨Ù‚ Ø§Ù„Ø°ÙƒÙŠØ©
Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ¶Ù…Ø§Ù† Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
Sabq AI Recommendation Engine - Performance Testing
"""

import asyncio
import time
import statistics
import aiohttp
import psutil
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor
import pytest
import logging

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PerformanceTest:
    """ÙØ¦Ø© Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø´Ø§Ù…Ù„"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results = {}
        self.session: Optional[aiohttp.ClientSession] = None
        
    async def __aenter__(self):
        """Ø¨Ø¯Ø¡ Ø¬Ù„Ø³Ø© HTTP"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            connector=aiohttp.TCPConnector(limit=100)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Ø¥Ù†Ù‡Ø§Ø¡ Ø¬Ù„Ø³Ø© HTTP"""
        if self.session:
            await self.session.close()
    
    async def health_check(self) -> bool:
        """ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ù†Ø¸Ø§Ù…"""
        try:
            async with self.session.get(f"{self.base_url}/health") as response:
                data = await response.json()
                return data.get('status') == 'healthy'
        except Exception as e:
            logger.error(f"ÙØ´Ù„ ÙØ­Øµ Ø§Ù„ØµØ­Ø©: {e}")
            return False
    
    async def measure_response_time(self, endpoint: str, method: str = "GET", 
                                  data: Optional[Dict] = None, 
                                  iterations: int = 100) -> Dict[str, Any]:
        """Ù‚ÙŠØ§Ø³ Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©"""
        times = []
        errors = 0
        
        logger.info(f"Ø¨Ø¯Ø¡ Ø§Ø®ØªØ¨Ø§Ø± {endpoint} Ù…Ø¹ {iterations} Ø·Ù„Ø¨...")
        
        for i in range(iterations):
            start_time = time.time()
            
            try:
                if method.upper() == "POST":
                    async with self.session.post(
                        f"{self.base_url}{endpoint}", 
                        json=data
                    ) as response:
                        await response.read()
                        if response.status == 200:
                            times.append((time.time() - start_time) * 1000)  # ØªØ­ÙˆÙŠÙ„ Ù„Ù„Ù…Ù„ÙŠ Ø«Ø§Ù†ÙŠØ©
                        else:
                            errors += 1
                else:
                    async with self.session.get(f"{self.base_url}{endpoint}") as response:
                        await response.read()
                        if response.status == 200:
                            times.append((time.time() - start_time) * 1000)
                        else:
                            errors += 1
                            
            except Exception as e:
                logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø·Ù„Ø¨ {i+1}: {e}")
                errors += 1
            
            # Ø§Ù†ØªØ¸Ø§Ø± Ù‚ØµÙŠØ± Ù„ØªØ¬Ù†Ø¨ Ø¥ØºØ±Ø§Ù‚ Ø§Ù„Ø®Ø§Ø¯Ù…
            if i % 10 == 0 and i > 0:
                await asyncio.sleep(0.1)
        
        if not times:
            return {"error": "Ù„Ù… ØªÙ†Ø¬Ø­ Ø£ÙŠ Ø·Ù„Ø¨Ø§Øª"}
        
        return {
            "endpoint": endpoint,
            "total_requests": iterations,
            "successful_requests": len(times),
            "failed_requests": errors,
            "success_rate": (len(times) / iterations) * 100,
            "min_time": min(times),
            "max_time": max(times),
            "avg_time": statistics.mean(times),
            "median_time": statistics.median(times),
            "p95_time": self.percentile(times, 95),
            "p99_time": self.percentile(times, 99),
            "under_200ms": sum(1 for t in times if t < 200),
            "under_200ms_percentage": (sum(1 for t in times if t < 200) / len(times)) * 100
        }
    
    def percentile(self, data: List[float], p: int) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¦ÙŠÙ†"""
        sorted_data = sorted(data)
        index = (p / 100) * (len(sorted_data) - 1)
        lower = int(index)
        upper = lower + 1
        
        if upper >= len(sorted_data):
            return sorted_data[-1]
        
        weight = index - lower
        return sorted_data[lower] * (1 - weight) + sorted_data[upper] * weight
    
    async def load_test(self, endpoint: str, concurrent_users: int = 50, 
                       duration_seconds: int = 60) -> Dict[str, Any]:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø­Ù…ÙˆÙ„Ø©"""
        logger.info(f"Ø¨Ø¯Ø¡ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø­Ù…ÙˆÙ„Ø©: {concurrent_users} Ù…Ø³ØªØ®Ø¯Ù… Ù…ØªØ²Ø§Ù…Ù† Ù„Ù…Ø¯Ø© {duration_seconds} Ø«Ø§Ù†ÙŠØ©")
        
        start_time = time.time()
        end_time = start_time + duration_seconds
        total_requests = 0
        successful_requests = 0
        response_times = []
        
        async def worker():
            """Ø¹Ø§Ù…Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨Ø§Øª"""
            nonlocal total_requests, successful_requests, response_times
            
            while time.time() < end_time:
                request_start = time.time()
                
                try:
                    async with self.session.get(f"{self.base_url}{endpoint}") as response:
                        await response.read()
                        
                        total_requests += 1
                        if response.status == 200:
                            successful_requests += 1
                            response_times.append((time.time() - request_start) * 1000)
                            
                except Exception as e:
                    logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø·Ù„Ø¨: {e}")
                    total_requests += 1
                
                # Ø§Ù†ØªØ¸Ø§Ø± Ù‚ØµÙŠØ±
                await asyncio.sleep(0.01)
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ù…Ø§Ù„ Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†ÙŠÙ†
        tasks = [worker() for _ in range(concurrent_users)]
        await asyncio.gather(*tasks)
        
        actual_duration = time.time() - start_time
        
        return {
            "endpoint": endpoint,
            "concurrent_users": concurrent_users,
            "duration": actual_duration,
            "total_requests": total_requests,
            "successful_requests": successful_requests,
            "failed_requests": total_requests - successful_requests,
            "requests_per_second": total_requests / actual_duration,
            "success_rate": (successful_requests / total_requests) * 100 if total_requests > 0 else 0,
            "avg_response_time": statistics.mean(response_times) if response_times else 0,
            "p95_response_time": self.percentile(response_times, 95) if response_times else 0,
            "under_200ms_percentage": (sum(1 for t in response_times if t < 200) / len(response_times)) * 100 if response_times else 0
        }
    
    async def memory_usage_test(self, duration_seconds: int = 300) -> Dict[str, Any]:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        logger.info(f"Ø¨Ø¯Ø¡ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ù…Ø¯Ø© {duration_seconds} Ø«Ø§Ù†ÙŠØ©...")
        
        memory_readings = []
        cpu_readings = []
        start_time = time.time()
        
        while time.time() - start_time < duration_seconds:
            memory_info = psutil.virtual_memory()
            cpu_percent = psutil.cpu_percent(interval=1)
            
            memory_readings.append(memory_info.percent)
            cpu_readings.append(cpu_percent)
            
            await asyncio.sleep(5)  # Ù‚Ø±Ø§Ø¡Ø© ÙƒÙ„ 5 Ø«ÙˆØ§Ù†
        
        return {
            "duration": duration_seconds,
            "memory_usage": {
                "min": min(memory_readings),
                "max": max(memory_readings),
                "avg": statistics.mean(memory_readings),
                "final": memory_readings[-1] if memory_readings else 0
            },
            "cpu_usage": {
                "min": min(cpu_readings),
                "max": max(cpu_readings),
                "avg": statistics.mean(cpu_readings),
                "final": cpu_readings[-1] if cpu_readings else 0
            }
        }
    
    async def cache_performance_test(self) -> Dict[str, Any]:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        logger.info("Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª...")
        
        # Ø·Ù„Ø¨ Ø£ÙˆÙ„ÙŠ Ù„ØªØ¹Ø¨Ø¦Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        test_endpoint = "/api/v1/recommendations/user/test_user_123"
        
        # Ù‚ÙŠØ§Ø³ Ø£ÙˆÙ„ Ø·Ù„Ø¨ (Ø¨Ø¯ÙˆÙ† ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª)
        first_request = await self.measure_response_time(test_endpoint, iterations=1)
        
        # Ø§Ù†ØªØ¸Ø§Ø± Ù‚ØµÙŠØ±
        await asyncio.sleep(0.5)
        
        # Ù‚ÙŠØ§Ø³ Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ø«Ø§Ù†ÙŠ (Ù…Ø¹ ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª)
        second_request = await self.measure_response_time(test_endpoint, iterations=1)
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ø¯Ø© Ø·Ù„Ø¨Ø§Øª Ù…ØªØªØ§Ù„ÙŠØ© Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        cached_requests = await self.measure_response_time(test_endpoint, iterations=50)
        
        return {
            "first_request_time": first_request.get("avg_time", 0),
            "second_request_time": second_request.get("avg_time", 0),
            "cached_avg_time": cached_requests.get("avg_time", 0),
            "cache_improvement": (
                (first_request.get("avg_time", 0) - cached_requests.get("avg_time", 0)) 
                / first_request.get("avg_time", 1) * 100
            ),
            "cached_under_200ms_percentage": cached_requests.get("under_200ms_percentage", 0)
        }
    
    async def run_comprehensive_test(self) -> Dict[str, Any]:
        """ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„"""
        logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ø£Ø¯Ø§Ø¡...")
        
        # ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
        is_healthy = await self.health_check()
        if not is_healthy:
            return {"error": "Ø§Ù„Ù†Ø¸Ø§Ù… ØºÙŠØ± ØµØ­ÙŠØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª"}
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "system_health": is_healthy
        }
        
        # Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù†Ù‚Ø§Ø· Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        endpoints_to_test = [
            "/health",
            "/api/v1/recommendations/user/test_user_123",
            "/api/v1/recommendations/trending",
            "/api/v1/recommendations/similar/test_article_456",
            "/info"
        ]
        
        logger.info("ğŸ“Š Ø§Ø®ØªØ¨Ø§Ø± Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ø®ØªÙ„ÙØ©...")
        for endpoint in endpoints_to_test:
            try:
                result = await self.measure_response_time(endpoint, iterations=100)
                results[f"endpoint_{endpoint.replace('/', '_').replace(':', '')}"] = result
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ØªØ·Ù„Ø¨ (< 200ms)
                meets_requirement = result.get("under_200ms_percentage", 0) >= 90
                logger.info(f"âœ… {endpoint}: {result.get('avg_time', 0):.2f}ms Ù…ØªÙˆØ³Ø·ØŒ "
                          f"{result.get('under_200ms_percentage', 0):.1f}% < 200ms "
                          f"{'âœ… ÙŠÙ„Ø¨ÙŠ Ø§Ù„Ù…ØªØ·Ù„Ø¨' if meets_requirement else 'âŒ Ù„Ø§ ÙŠÙ„Ø¨ÙŠ Ø§Ù„Ù…ØªØ·Ù„Ø¨'}")
                
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± {endpoint}: {e}")
                results[f"endpoint_{endpoint.replace('/', '_').replace(':', '')}"] = {"error": str(e)}
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø­Ù…ÙˆÙ„Ø©
        logger.info("ğŸ”¥ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø­Ù…ÙˆÙ„Ø©...")
        try:
            load_result = await self.load_test("/api/v1/recommendations/trending", 
                                             concurrent_users=25, duration_seconds=30)
            results["load_test"] = load_result
            
            rps = load_result.get("requests_per_second", 0)
            success_rate = load_result.get("success_rate", 0)
            logger.info(f"ğŸ”¥ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø­Ù…ÙˆÙ„Ø©: {rps:.1f} Ø·Ù„Ø¨/Ø«Ø§Ù†ÙŠØ©ØŒ {success_rate:.1f}% Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø­Ù…ÙˆÙ„Ø©: {e}")
            results["load_test"] = {"error": str(e)}
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        logger.info("ğŸ’¾ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª...")
        try:
            cache_result = await self.cache_performance_test()
            results["cache_test"] = cache_result
            
            improvement = cache_result.get("cache_improvement", 0)
            logger.info(f"ğŸ’¾ ØªØ­Ø³Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {improvement:.1f}%")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")
            results["cache_test"] = {"error": str(e)}
        
        # ØªÙ‚ÙŠÙŠÙ… Ø´Ø§Ù…Ù„
        results["evaluation"] = self.evaluate_performance(results)
        
        logger.info("âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø´Ø§Ù…Ù„")
        return results
    
    def evaluate_performance(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø´Ø§Ù…Ù„"""
        evaluation = {
            "overall_score": 0,
            "requirements_met": {},
            "recommendations": []
        }
        
        # ØªÙ‚ÙŠÙŠÙ… Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© (< 200ms)
        response_time_scores = []
        for key, value in results.items():
            if key.startswith("endpoint_") and isinstance(value, dict):
                under_200ms = value.get("under_200ms_percentage", 0)
                avg_time = value.get("avg_time", 1000)
                
                if under_200ms >= 90:
                    response_time_scores.append(100)
                elif under_200ms >= 75:
                    response_time_scores.append(75)
                elif under_200ms >= 50:
                    response_time_scores.append(50)
                else:
                    response_time_scores.append(25)
                
                evaluation["requirements_met"][f"{key}_under_200ms"] = under_200ms >= 90
        
        # ØªÙ‚ÙŠÙŠÙ… Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­
        success_rate_score = 100
        for key, value in results.items():
            if key.startswith("endpoint_") and isinstance(value, dict):
                success_rate = value.get("success_rate", 0)
                if success_rate < 95:
                    success_rate_score = min(success_rate_score, success_rate)
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø­Ù…ÙˆÙ„Ø©
        load_test_score = 100
        if "load_test" in results and isinstance(results["load_test"], dict):
            load_result = results["load_test"]
            rps = load_result.get("requests_per_second", 0)
            success_rate = load_result.get("success_rate", 0)
            
            # Ø§Ù„Ù…ØªØ·Ù„Ø¨: > 1000 Ø·Ù„Ø¨/Ø«Ø§Ù†ÙŠØ© Ù…Ø¹ Ù…Ø¹Ø¯Ù„ Ù†Ø¬Ø§Ø­ > 95%
            if rps >= 1000 and success_rate >= 95:
                load_test_score = 100
            elif rps >= 500 and success_rate >= 90:
                load_test_score = 75
            elif rps >= 250 and success_rate >= 85:
                load_test_score = 50
            else:
                load_test_score = 25
            
            evaluation["requirements_met"]["load_test"] = rps >= 1000 and success_rate >= 95
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        response_time_avg = statistics.mean(response_time_scores) if response_time_scores else 0
        evaluation["overall_score"] = (response_time_avg * 0.5 + success_rate_score * 0.3 + load_test_score * 0.2)
        
        # Ø§Ù„ØªÙˆØµÙŠØ§Øª
        if response_time_avg < 75:
            evaluation["recommendations"].append("ØªØ­Ø³ÙŠÙ† Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªÙˆØµÙŠØ§Øª Ù„ØªÙ‚Ù„ÙŠÙ„ Ø²Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
        
        if success_rate_score < 95:
            evaluation["recommendations"].append("Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ¥ØµÙ„Ø§Ø­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡")
        
        if load_test_score < 75:
            evaluation["recommendations"].append("ØªØ­Ø³ÙŠÙ† Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹ ÙˆØ¥Ø¶Ø§ÙØ© Ù…ÙˆØ§Ø±Ø¯ Ø¥Ø¶Ø§ÙÙŠØ©")
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        if "cache_test" in results and isinstance(results["cache_test"], dict):
            cache_improvement = results["cache_test"].get("cache_improvement", 0)
            if cache_improvement < 50:
                evaluation["recommendations"].append("ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª")
        
        return evaluation

# Ø¯ÙˆØ§Ù„ Ø§Ø®ØªØ¨Ø§Ø± pytest
@pytest.mark.asyncio
async def test_response_time_under_200ms():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø£Ù† Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø£Ù‚Ù„ Ù…Ù† 200ms"""
    async with PerformanceTest() as tester:
        result = await tester.measure_response_time("/api/v1/recommendations/trending")
        assert result["under_200ms_percentage"] >= 90, f"ÙÙ‚Ø· {result['under_200ms_percentage']:.1f}% Ù…Ù† Ø§Ù„Ø·Ù„Ø¨Ø§Øª ÙƒØ§Ù†Øª Ø£Ù‚Ù„ Ù…Ù† 200ms"

@pytest.mark.asyncio
async def test_system_health():
    """Ø§Ø®ØªØ¨Ø§Ø± ØµØ­Ø© Ø§Ù„Ù†Ø¸Ø§Ù…"""
    async with PerformanceTest() as tester:
        is_healthy = await tester.health_check()
        assert is_healthy, "Ø§Ù„Ù†Ø¸Ø§Ù… ØºÙŠØ± ØµØ­ÙŠ"

@pytest.mark.asyncio
async def test_load_handling():
    """Ø§Ø®ØªØ¨Ø§Ø± ØªØ­Ù…Ù„ Ø§Ù„Ø­Ù…ÙˆÙ„Ø©"""
    async with PerformanceTest() as tester:
        result = await tester.load_test("/health", concurrent_users=10, duration_seconds=10)
        assert result["success_rate"] >= 95, f"Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­ Ù…Ù†Ø®ÙØ¶: {result['success_rate']:.1f}%"

# Ø¯Ø§Ù„Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù…Ù† Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±
async def main():
    """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø´Ø§Ù…Ù„"""
    async with PerformanceTest() as tester:
        results = await tester.run_comprehensive_test()
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"performance_test_results_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š ØªÙ… Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙÙŠ: {filename}")
        
        # Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ
        evaluation = results.get("evaluation", {})
        overall_score = evaluation.get("overall_score", 0)
        
        print(f"\nğŸ¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {overall_score:.1f}/100")
        
        if overall_score >= 90:
            print("ğŸ† Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø²!")
        elif overall_score >= 75:
            print("âœ… Ø£Ø¯Ø§Ø¡ Ø¬ÙŠØ¯")
        elif overall_score >= 60:
            print("âš ï¸ Ø£Ø¯Ø§Ø¡ Ù…Ù‚Ø¨ÙˆÙ„ - ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†")
        else:
            print("âŒ Ø£Ø¯Ø§Ø¡ Ø¶Ø¹ÙŠÙ - ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ† Ø¹Ø§Ø¬Ù„")
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙˆØµÙŠØ§Øª
        recommendations = evaluation.get("recommendations", [])
        if recommendations:
            print("\nğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª:")
            for i, rec in enumerate(recommendations, 1):
                print(f"  {i}. {rec}")

if __name__ == "__main__":
    asyncio.run(main())
