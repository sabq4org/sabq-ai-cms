# Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ© Ù„Ù€ ML - Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø°ÙƒÙŠ
# ML Infrastructure Pipeline for Intelligent Recommendation Engine

import asyncio
import asyncpg
import redis.asyncio as redis
import logging
import json
import pickle
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Union, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from enum import Enum
import os
from pathlib import Path
import aioboto3
import aiofiles
import yaml
from contextlib import asynccontextmanager
import threading
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import torch
import joblib

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ========================= Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… =========================

@dataclass
class DatabaseConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    host: str = "localhost"
    port: int = 5432
    database: str = "sabq_ai_recommendations"
    username: str = "postgres"
    password: str = ""
    pool_min_size: int = 5
    pool_max_size: int = 20
    timeout: int = 30

@dataclass
class RedisConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Redis"""
    host: str = "localhost"
    port: int = 6379
    db: int = 0
    password: Optional[str] = None
    pool_size: int = 10
    timeout: int = 30
    key_prefix: str = "sabq_ai_rec:"

@dataclass
class S3Config:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Amazon S3"""
    bucket_name: str = "sabq-ai-models"
    region: str = "us-east-1"
    access_key: Optional[str] = None
    secret_key: Optional[str] = None
    endpoint_url: Optional[str] = None

@dataclass
class MLConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ML"""
    model_storage_path: str = "/models"
    batch_size: int = 1000
    training_frequency: int = 3600  # seconds
    model_backup_frequency: int = 86400  # seconds
    max_workers: int = 4
    gpu_enabled: bool = torch.cuda.is_available()
    model_version: str = "1.0.0"

@dataclass
class SystemConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ø§Ù…Ø©"""
    database: DatabaseConfig
    redis: RedisConfig
    s3: S3Config
    ml: MLConfig
    environment: str = "development"
    debug: bool = False
    log_level: str = "INFO"

# ========================= Ø¥Ø¯Ø§Ø±Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª =========================

class DatabaseManager:
    """Ù…Ø¯ÙŠØ± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª PostgreSQL"""
    
    def __init__(self, config: DatabaseConfig):
        self.config = config
        self.pool = None
        self._lock = asyncio.Lock()
    
    async def initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            logger.info("ğŸ—„ï¸  ØªÙ‡ÙŠØ¦Ø© Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
            
            dsn = f"postgresql://{self.config.username}:{self.config.password}@{self.config.host}:{self.config.port}/{self.config.database}"
            
            self.pool = await asyncpg.create_pool(
                dsn,
                min_size=self.config.pool_min_size,
                max_size=self.config.pool_max_size,
                command_timeout=self.config.timeout
            )
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
            await self._create_tables()
            
            logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}")
            raise
    
    async def _create_tables(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©"""
        
        tables_sql = """
        -- Ø¬Ø¯ÙˆÙ„ ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
        CREATE TABLE IF NOT EXISTS user_interactions (
            id SERIAL PRIMARY KEY,
            user_id VARCHAR(255) NOT NULL,
            item_id VARCHAR(255) NOT NULL,
            interaction_type VARCHAR(50) NOT NULL,
            rating FLOAT,
            context JSONB,
            features BYTEA,
            timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            processed BOOLEAN DEFAULT FALSE,
            
            INDEX ON (user_id),
            INDEX ON (item_id),
            INDEX ON (timestamp),
            INDEX ON (processed)
        );
        
        -- Ø¬Ø¯ÙˆÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
        CREATE TABLE IF NOT EXISTS user_profiles (
            user_id VARCHAR(255) PRIMARY KEY,
            interests JSONB,
            personality_traits JSONB,
            behavior_patterns JSONB,
            preferences JSONB,
            last_updated TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            confidence_score FLOAT DEFAULT 0.0
        );
        
        -- Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…Ø®Ø¨Ø£Ø©
        CREATE TABLE IF NOT EXISTS cached_recommendations (
            cache_key VARCHAR(255) PRIMARY KEY,
            user_id VARCHAR(255) NOT NULL,
            recommendations JSONB NOT NULL,
            metadata JSONB,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            expires_at TIMESTAMP WITH TIME ZONE,
            
            INDEX ON (user_id),
            INDEX ON (expires_at)
        );
        
        -- Ø¬Ø¯ÙˆÙ„ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
        CREATE TABLE IF NOT EXISTS item_statistics (
            item_id VARCHAR(255) PRIMARY KEY,
            views_count INTEGER DEFAULT 0,
            likes_count INTEGER DEFAULT 0,
            saves_count INTEGER DEFAULT 0,
            shares_count INTEGER DEFAULT 0,
            comments_count INTEGER DEFAULT 0,
            total_reading_time INTEGER DEFAULT 0,
            average_rating FLOAT DEFAULT 0.0,
            popularity_score FLOAT DEFAULT 0.0,
            last_updated TIMESTAMP WITH TIME ZONE DEFAULT NOW()
        );
        
        -- Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø©
        CREATE TABLE IF NOT EXISTS trained_models (
            model_id VARCHAR(255) PRIMARY KEY,
            model_type VARCHAR(100) NOT NULL,
            model_version VARCHAR(50) NOT NULL,
            model_path VARCHAR(500),
            s3_key VARCHAR(500),
            performance_metrics JSONB,
            training_data_info JSONB,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            is_active BOOLEAN DEFAULT FALSE,
            
            INDEX ON (model_type),
            INDEX ON (is_active),
            INDEX ON (created_at)
        );
        
        -- Ø¬Ø¯ÙˆÙ„ Ø³Ø¬Ù„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        CREATE TABLE IF NOT EXISTS training_logs (
            id SERIAL PRIMARY KEY,
            model_id VARCHAR(255) NOT NULL,
            training_type VARCHAR(100) NOT NULL,
            status VARCHAR(50) NOT NULL,
            start_time TIMESTAMP WITH TIME ZONE,
            end_time TIMESTAMP WITH TIME ZONE,
            metrics JSONB,
            error_message TEXT,
            data_size INTEGER,
            
            INDEX ON (model_id),
            INDEX ON (training_type),
            INDEX ON (status),
            INDEX ON (start_time)
        );
        
        -- Ø¬Ø¯ÙˆÙ„ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
        CREATE TABLE IF NOT EXISTS performance_metrics (
            id SERIAL PRIMARY KEY,
            metric_type VARCHAR(100) NOT NULL,
            metric_name VARCHAR(100) NOT NULL,
            metric_value FLOAT NOT NULL,
            metadata JSONB,
            timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            
            INDEX ON (metric_type),
            INDEX ON (metric_name),
            INDEX ON (timestamp)
        );
        """
        
        async with self.pool.acquire() as conn:
            await conn.execute(tables_sql)
            logger.info("ğŸ“‹ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡/ØªØ­Ø¯ÙŠØ« Ø¬Ø¯Ø§ÙˆÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    
    async def save_interaction(self, user_id: str, item_id: str, 
                             interaction_type: str, rating: Optional[float] = None,
                             context: Optional[Dict] = None, 
                             features: Optional[np.ndarray] = None) -> int:
        """Ø­ÙØ¸ ØªÙØ§Ø¹Ù„ Ù…Ø³ØªØ®Ø¯Ù…"""
        
        features_bytes = None
        if features is not None:
            features_bytes = pickle.dumps(features)
        
        async with self.pool.acquire() as conn:
            interaction_id = await conn.fetchval("""
                INSERT INTO user_interactions 
                (user_id, item_id, interaction_type, rating, context, features)
                VALUES ($1, $2, $3, $4, $5, $6)
                RETURNING id
            """, user_id, item_id, interaction_type, rating, 
                json.dumps(context) if context else None, features_bytes)
            
            return interaction_id
    
    async def get_user_interactions(self, user_id: str, 
                                  limit: int = 1000,
                                  since: Optional[datetime] = None) -> List[Dict]:
        """Ø¬Ù„Ø¨ ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        
        query = """
            SELECT user_id, item_id, interaction_type, rating, context, 
                   features, timestamp
            FROM user_interactions
            WHERE user_id = $1
        """
        params = [user_id]
        
        if since:
            query += " AND timestamp >= $2"
            params.append(since)
        
        query += " ORDER BY timestamp DESC LIMIT $" + str(len(params) + 1)
        params.append(limit)
        
        async with self.pool.acquire() as conn:
            rows = await conn.fetch(query, *params)
            
            interactions = []
            for row in rows:
                interaction = dict(row)
                
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† JSON
                if interaction['context']:
                    interaction['context'] = json.loads(interaction['context'])
                
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ù…Ù† bytes
                if interaction['features']:
                    interaction['features'] = pickle.loads(interaction['features'])
                
                interactions.append(interaction)
            
            return interactions
    
    async def save_user_profile(self, user_id: str, profile_data: Dict):
        """Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        
        async with self.pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO user_profiles 
                (user_id, interests, personality_traits, behavior_patterns, preferences)
                VALUES ($1, $2, $3, $4, $5)
                ON CONFLICT (user_id) 
                DO UPDATE SET
                    interests = EXCLUDED.interests,
                    personality_traits = EXCLUDED.personality_traits,
                    behavior_patterns = EXCLUDED.behavior_patterns,
                    preferences = EXCLUDED.preferences,
                    last_updated = NOW()
            """, user_id,
                json.dumps(profile_data.get('interests', {})),
                json.dumps(profile_data.get('personality_traits', {})),
                json.dumps(profile_data.get('behavior_patterns', {})),
                json.dumps(profile_data.get('preferences', {})))
    
    async def get_user_profile(self, user_id: str) -> Optional[Dict]:
        """Ø¬Ù„Ø¨ Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        
        async with self.pool.acquire() as conn:
            row = await conn.fetchrow("""
                SELECT interests, personality_traits, behavior_patterns, 
                       preferences, last_updated, confidence_score
                FROM user_profiles
                WHERE user_id = $1
            """, user_id)
            
            if row:
                profile = dict(row)
                # ØªØ­ÙˆÙŠÙ„ JSON fields
                for field in ['interests', 'personality_traits', 'behavior_patterns', 'preferences']:
                    if profile[field]:
                        profile[field] = json.loads(profile[field])
                
                return profile
            
            return None
    
    async def update_item_statistics(self, item_id: str, interaction_type: str, 
                                   reading_time: Optional[int] = None,
                                   rating: Optional[float] = None):
        """ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù‚Ø§Ù„"""
        
        update_fields = []
        if interaction_type == 'view':
            update_fields.append("views_count = views_count + 1")
        elif interaction_type == 'like':
            update_fields.append("likes_count = likes_count + 1")
        elif interaction_type == 'save':
            update_fields.append("saves_count = saves_count + 1")
        elif interaction_type == 'share':
            update_fields.append("shares_count = shares_count + 1")
        elif interaction_type == 'comment':
            update_fields.append("comments_count = comments_count + 1")
        
        if reading_time:
            update_fields.append(f"total_reading_time = total_reading_time + {reading_time}")
        
        if update_fields:
            query = f"""
                INSERT INTO item_statistics (item_id, {interaction_type}s_count)
                VALUES ($1, 1)
                ON CONFLICT (item_id)
                DO UPDATE SET {', '.join(update_fields)}, last_updated = NOW()
            """
            
            async with self.pool.acquire() as conn:
                await conn.execute(query, item_id)
    
    async def get_popular_items(self, limit: int = 100) -> List[Dict]:
        """Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± Ø´Ø¹Ø¨ÙŠØ©"""
        
        async with self.pool.acquire() as conn:
            rows = await conn.fetch("""
                SELECT item_id, views_count, likes_count, saves_count, 
                       shares_count, comments_count, popularity_score
                FROM item_statistics
                ORDER BY popularity_score DESC
                LIMIT $1
            """, limit)
            
            return [dict(row) for row in rows]
    
    async def close(self):
        """Ø¥ØºÙ„Ø§Ù‚ Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        if self.pool:
            await self.pool.close()
            logger.info("ğŸ—„ï¸  ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")


# ========================= Ø¥Ø¯Ø§Ø±Ø© Redis =========================

class RedisManager:
    """Ù…Ø¯ÙŠØ± Redis Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
    
    def __init__(self, config: RedisConfig):
        self.config = config
        self.redis_client = None
        self.key_prefix = config.key_prefix
    
    async def initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§ØªØµØ§Ù„ Redis"""
        try:
            logger.info("ğŸ“¦ ØªÙ‡ÙŠØ¦Ø© Ø§ØªØµØ§Ù„ Redis...")
            
            self.redis_client = await redis.Redis(
                host=self.config.host,
                port=self.config.port,
                db=self.config.db,
                password=self.config.password,
                decode_responses=False,  # Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠØ©
                socket_timeout=self.config.timeout,
                socket_connect_timeout=self.config.timeout
            )
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„
            await self.redis_client.ping()
            
            logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Redis Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Redis: {str(e)}")
            raise
    
    def _get_key(self, key: str) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ Ù…Ø¹ Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø©"""
        return f"{self.key_prefix}{key}"
    
    async def set(self, key: str, value: Any, expire: Optional[int] = None) -> bool:
        """Ø­ÙØ¸ Ù‚ÙŠÙ…Ø© ÙÙŠ Redis"""
        try:
            redis_key = self._get_key(key)
            
            # ØªØ³Ù„Ø³Ù„ Ø§Ù„Ù‚ÙŠÙ…Ø©
            if isinstance(value, (dict, list)):
                serialized_value = json.dumps(value, default=str)
            elif isinstance(value, np.ndarray):
                serialized_value = pickle.dumps(value)
            else:
                serialized_value = str(value)
            
            if expire:
                await self.redis_client.setex(redis_key, expire, serialized_value)
            else:
                await self.redis_client.set(redis_key, serialized_value)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Redis {key}: {str(e)}")
            return False
    
    async def get(self, key: str, default: Any = None) -> Any:
        """Ø¬Ù„Ø¨ Ù‚ÙŠÙ…Ø© Ù…Ù† Redis"""
        try:
            redis_key = self._get_key(key)
            value = await self.redis_client.get(redis_key)
            
            if value is None:
                return default
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ù„ØºØ§Ø¡ Ø§Ù„ØªØ³Ù„Ø³Ù„
            try:
                # Ù…Ø­Ø§ÙˆÙ„Ø© JSON Ø£ÙˆÙ„Ø§Ù‹
                return json.loads(value.decode('utf-8'))
            except (json.JSONDecodeError, UnicodeDecodeError):
                try:
                    # Ù…Ø­Ø§ÙˆÙ„Ø© pickle
                    return pickle.loads(value)
                except:
                    # Ø¥Ø±Ø¬Ø§Ø¹ ÙƒÙ†Øµ
                    return value.decode('utf-8')
                    
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Redis {key}: {str(e)}")
            return default
    
    async def delete(self, key: str) -> bool:
        """Ø­Ø°Ù Ù‚ÙŠÙ…Ø© Ù…Ù† Redis"""
        try:
            redis_key = self._get_key(key)
            result = await self.redis_client.delete(redis_key)
            return result > 0
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø­Ø°Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Redis {key}: {str(e)}")
            return False
    
    async def exists(self, key: str) -> bool:
        """ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ù…ÙØªØ§Ø­"""
        try:
            redis_key = self._get_key(key)
            return await self.redis_client.exists(redis_key) > 0
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…ÙØªØ§Ø­ {key}: {str(e)}")
            return False
    
    async def set_hash(self, key: str, field_values: Dict[str, Any], expire: Optional[int] = None) -> bool:
        """Ø­ÙØ¸ hash ÙÙŠ Redis"""
        try:
            redis_key = self._get_key(key)
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ù„Ù„Ù†ØµÙˆØµ
            serialized_values = {}
            for field, value in field_values.items():
                if isinstance(value, (dict, list)):
                    serialized_values[field] = json.dumps(value, default=str)
                elif isinstance(value, np.ndarray):
                    serialized_values[field] = pickle.dumps(value).hex()
                else:
                    serialized_values[field] = str(value)
            
            await self.redis_client.hset(redis_key, mapping=serialized_values)
            
            if expire:
                await self.redis_client.expire(redis_key, expire)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ hash ÙÙŠ Redis {key}: {str(e)}")
            return False
    
    async def get_hash(self, key: str, field: Optional[str] = None) -> Any:
        """Ø¬Ù„Ø¨ hash Ù…Ù† Redis"""
        try:
            redis_key = self._get_key(key)
            
            if field:
                value = await self.redis_client.hget(redis_key, field)
                if value is None:
                    return None
                
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ù„ØºØ§Ø¡ Ø§Ù„ØªØ³Ù„Ø³Ù„
                try:
                    return json.loads(value.decode('utf-8'))
                except:
                    try:
                        return pickle.loads(bytes.fromhex(value.decode('utf-8')))
                    except:
                        return value.decode('utf-8')
            else:
                hash_data = await self.redis_client.hgetall(redis_key)
                if not hash_data:
                    return {}
                
                result = {}
                for k, v in hash_data.items():
                    key_str = k.decode('utf-8')
                    try:
                        result[key_str] = json.loads(v.decode('utf-8'))
                    except:
                        try:
                            result[key_str] = pickle.loads(bytes.fromhex(v.decode('utf-8')))
                        except:
                            result[key_str] = v.decode('utf-8')
                
                return result
                
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ hash Ù…Ù† Redis {key}: {str(e)}")
            return None if field else {}
    
    async def cache_recommendations(self, user_id: str, recommendations: List[Dict],
                                  recommendation_type: str, expire: int = 300) -> bool:
        """ØªØ®Ø²ÙŠÙ† ØªÙˆØµÙŠØ§Øª Ù…Ø¤Ù‚ØªØ§Ù‹"""
        cache_key = f"recommendations:{user_id}:{recommendation_type}"
        
        cache_data = {
            'recommendations': recommendations,
            'generated_at': datetime.now().isoformat(),
            'type': recommendation_type
        }
        
        return await self.set(cache_key, cache_data, expire)
    
    async def get_cached_recommendations(self, user_id: str, 
                                       recommendation_type: str) -> Optional[List[Dict]]:
        """Ø¬Ù„Ø¨ ØªÙˆØµÙŠØ§Øª Ù…Ø®Ø²Ù†Ø© Ù…Ø¤Ù‚ØªØ§Ù‹"""
        cache_key = f"recommendations:{user_id}:{recommendation_type}"
        
        cached_data = await self.get(cache_key)
        if cached_data and isinstance(cached_data, dict):
            return cached_data.get('recommendations')
        
        return None
    
    async def increment_counter(self, key: str, amount: int = 1) -> int:
        """Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø§Ø¯"""
        try:
            redis_key = self._get_key(key)
            return await self.redis_client.incrby(redis_key, amount)
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø§Ø¯ {key}: {str(e)}")
            return 0
    
    async def close(self):
        """Ø¥ØºÙ„Ø§Ù‚ Ø§ØªØµØ§Ù„ Redis"""
        if self.redis_client:
            await self.redis_client.close()
            logger.info("ğŸ“¦ ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§ØªØµØ§Ù„ Redis")


# ========================= Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† =========================

class S3Manager:
    """Ù…Ø¯ÙŠØ± Amazon S3 Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    
    def __init__(self, config: S3Config):
        self.config = config
        self.s3_client = None
        self.session = None
    
    async def initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§ØªØµØ§Ù„ S3"""
        try:
            logger.info("â˜ï¸  ØªÙ‡ÙŠØ¦Ø© Ø§ØªØµØ§Ù„ Amazon S3...")
            
            self.session = aioboto3.Session()
            
            kwargs = {
                'region_name': self.config.region
            }
            
            if self.config.access_key and self.config.secret_key:
                kwargs.update({
                    'aws_access_key_id': self.config.access_key,
                    'aws_secret_access_key': self.config.secret_key
                })
            
            if self.config.endpoint_url:
                kwargs['endpoint_url'] = self.config.endpoint_url
            
            self.s3_client = self.session.client('s3', **kwargs)
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¨ÙˆÙƒØª
            await self._ensure_bucket_exists()
            
            logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Amazon S3 Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© S3: {str(e)}")
            # S3 Ø§Ø®ØªÙŠØ§Ø±ÙŠØŒ Ù„Ø°Ø§ Ù„Ù† Ù†Ø«ÙŠØ± Ø§Ù„Ø®Ø·Ø£
            self.s3_client = None
    
    async def _ensure_bucket_exists(self):
        """Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¨ÙˆÙƒØª"""
        if not self.s3_client:
            return
        
        try:
            async with self.s3_client as s3:
                await s3.head_bucket(Bucket=self.config.bucket_name)
        except:
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨ÙˆÙƒØª Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
            try:
                async with self.s3_client as s3:
                    await s3.create_bucket(Bucket=self.config.bucket_name)
                logger.info(f"ğŸ“¦ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙˆÙƒØª S3: {self.config.bucket_name}")
            except Exception as e:
                logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙˆÙƒØª S3: {str(e)}")
    
    async def upload_model(self, model_path: str, s3_key: str) -> bool:
        """Ø±ÙØ¹ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ S3"""
        if not self.s3_client:
            logger.warning("âš ï¸ S3 ØºÙŠØ± Ù…ØªØ§Ø­ - ØªØ®Ø·ÙŠ Ø±ÙØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
            return False
        
        try:
            async with aiofiles.open(model_path, 'rb') as f:
                model_data = await f.read()
            
            async with self.s3_client as s3:
                await s3.put_object(
                    Bucket=self.config.bucket_name,
                    Key=s3_key,
                    Body=model_data,
                    Metadata={
                        'uploaded_at': datetime.now().isoformat(),
                        'model_type': 'recommendation_model'
                    }
                )
            
            logger.info(f"â˜ï¸  ØªÙ… Ø±ÙØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ S3: {s3_key}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø±ÙØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ S3: {str(e)}")
            return False
    
    async def download_model(self, s3_key: str, local_path: str) -> bool:
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† S3"""
        if not self.s3_client:
            logger.warning("âš ï¸ S3 ØºÙŠØ± Ù…ØªØ§Ø­ - ØªØ®Ø·ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
            return False
        
        try:
            async with self.s3_client as s3:
                response = await s3.get_object(
                    Bucket=self.config.bucket_name,
                    Key=s3_key
                )
                
                model_data = await response['Body'].read()
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
            Path(local_path).parent.mkdir(parents=True, exist_ok=True)
            
            async with aiofiles.open(local_path, 'wb') as f:
                await f.write(model_data)
            
            logger.info(f"â˜ï¸  ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† S3: {s3_key} -> {local_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† S3: {str(e)}")
            return False
    
    async def list_models(self, prefix: str = "models/") -> List[Dict]:
        """Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ S3"""
        if not self.s3_client:
            return []
        
        try:
            models = []
            async with self.s3_client as s3:
                response = await s3.list_objects_v2(
                    Bucket=self.config.bucket_name,
                    Prefix=prefix
                )
                
                if 'Contents' in response:
                    for obj in response['Contents']:
                        models.append({
                            'key': obj['Key'],
                            'size': obj['Size'],
                            'last_modified': obj['LastModified'],
                            'etag': obj['ETag']
                        })
            
            return models
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† S3: {str(e)}")
            return []


# ========================= Ù…Ø¯ÙŠØ± ML Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ =========================

class MLPipelineManager:
    """Ù…Ø¯ÙŠØ± Ø®Ø· Ø¥Ù†ØªØ§Ø¬ ML Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    
    def __init__(self, config: SystemConfig):
        self.config = config
        
        # Ù…Ø¯ÙŠØ±ÙŠ Ø§Ù„Ø®Ø¯Ù…Ø§Øª
        self.db_manager = DatabaseManager(config.database)
        self.redis_manager = RedisManager(config.redis)
        self.s3_manager = S3Manager(config.s3)
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø§Øª ML
        self.executor = ThreadPoolExecutor(max_workers=config.ml.max_workers)
        self.process_executor = ProcessPoolExecutor(max_workers=2)
        
        # Ù†Ù…Ø§Ø°Ø¬ ML Ù…Ø­Ù…Ù„Ø©
        self.loaded_models = {}
        self.model_metadata = {}
        
        # Ù…Ù‡Ø§Ù… Ø§Ù„Ø®Ù„ÙÙŠØ©
        self.background_tasks = []
        self.is_running = False
        
    async def initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"""
        logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… ML...")
        
        try:
            # ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            await self.db_manager.initialize()
            
            # ØªÙ‡ÙŠØ¦Ø© Redis
            await self.redis_manager.initialize()
            
            # ØªÙ‡ÙŠØ¦Ø© S3 (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            await self.s3_manager.initialize()
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            await self._load_models()
            
            # Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø®Ù„ÙÙŠØ©
            await self._start_background_tasks()
            
            self.is_running = True
            logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… ML Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… ML: {str(e)}")
            raise
    
    async def _load_models(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©"""
        logger.info("ğŸ“š ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø©...")
        
        try:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            async with self.db_manager.pool.acquire() as conn:
                models = await conn.fetch("""
                    SELECT model_id, model_type, model_version, model_path, s3_key
                    FROM trained_models
                    WHERE is_active = true
                    ORDER BY created_at DESC
                """)
            
            for model_record in models:
                model_id = model_record['model_id']
                model_type = model_record['model_type']
                model_path = model_record['model_path']
                s3_key = model_record['s3_key']
                
                # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ù„ÙŠØ§Ù‹ Ø£ÙˆÙ„Ø§Ù‹
                if model_path and os.path.exists(model_path):
                    success = await self._load_model_from_file(model_id, model_type, model_path)
                    if success:
                        continue
                
                # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ù…Ù† S3
                if s3_key:
                    local_path = os.path.join(self.config.ml.model_storage_path, f"{model_id}.pkl")
                    if await self.s3_manager.download_model(s3_key, local_path):
                        await self._load_model_from_file(model_id, model_type, local_path)
            
            logger.info(f"ğŸ“š ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.loaded_models)} Ù†Ù…ÙˆØ°Ø¬")
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {str(e)}")
    
    async def _load_model_from_file(self, model_id: str, model_type: str, model_path: str) -> bool:
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ù…Ù„Ù"""
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ thread Ù…Ù†ÙØµÙ„
            loop = asyncio.get_event_loop()
            model = await loop.run_in_executor(self.executor, joblib.load, model_path)
            
            self.loaded_models[model_id] = model
            self.model_metadata[model_id] = {
                'type': model_type,
                'path': model_path,
                'loaded_at': datetime.now()
            }
            
            logger.info(f"ğŸ“š ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_id} ({model_type})")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_id}: {str(e)}")
            return False
    
    async def _start_background_tasks(self):
        """Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø®Ù„ÙÙŠØ©"""
        logger.info("âš™ï¸ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø®Ù„ÙÙŠØ©...")
        
        # Ù…Ù‡Ù…Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        self.background_tasks.append(
            asyncio.create_task(self._process_interactions_worker())
        )
        
        # Ù…Ù‡Ù…Ø© ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
        self.background_tasks.append(
            asyncio.create_task(self._update_statistics_worker())
        )
        
        # Ù…Ù‡Ù…Ø© Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬
        self.background_tasks.append(
            asyncio.create_task(self._model_backup_worker())
        )
        
        # Ù…Ù‡Ù…Ø© ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        self.background_tasks.append(
            asyncio.create_task(self._cache_cleanup_worker())
        )
    
    async def _process_interactions_worker(self):
        """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©"""
        while self.is_running:
            try:
                # Ø¬Ù„Ø¨ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
                async with self.db_manager.pool.acquire() as conn:
                    interactions = await conn.fetch("""
                        SELECT id, user_id, item_id, interaction_type, rating, 
                               context, features, timestamp
                        FROM user_interactions
                        WHERE processed = false
                        ORDER BY timestamp
                        LIMIT 100
                    """)
                
                if interactions:
                    logger.info(f"âš¡ Ù…Ø¹Ø§Ù„Ø¬Ø© {len(interactions)} ØªÙØ§Ø¹Ù„ Ø¬Ø¯ÙŠØ¯...")
                    
                    # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ ØªÙØ§Ø¹Ù„
                    for interaction in interactions:
                        await self._process_single_interaction(interaction)
                        
                        # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
                        async with self.db_manager.pool.acquire() as conn:
                            await conn.execute("""
                                UPDATE user_interactions
                                SET processed = true
                                WHERE id = $1
                            """, interaction['id'])
                
                # Ø§Ù†ØªØ¸Ø§Ø± Ù‚Ø¨Ù„ Ø§Ù„Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
                await asyncio.sleep(10)
                
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª: {str(e)}")
                await asyncio.sleep(30)
    
    async def _process_single_interaction(self, interaction: Dict):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© ØªÙØ§Ø¹Ù„ ÙˆØ§Ø­Ø¯"""
        try:
            user_id = interaction['user_id']
            item_id = interaction['item_id']
            interaction_type = interaction['interaction_type']
            rating = interaction['rating']
            
            # ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù‚Ø§Ù„
            await self.db_manager.update_item_statistics(
                item_id, interaction_type, rating=rating
            )
            
            # ØªØ­Ø¯ÙŠØ« Ø¹Ø¯Ø§Ø¯Ø§Øª Redis
            await self.redis_manager.increment_counter(f"item_interactions:{item_id}")
            await self.redis_manager.increment_counter(f"user_interactions:{user_id}")
            
            # Ø¥Ø´Ø¹Ø§Ø± Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø± (Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ø­Ù…Ù„Ø©)
            # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ù…Ù†Ø·Ù‚ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù‡Ù†Ø§
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙØ§Ø¹Ù„: {str(e)}")
    
    async def _update_statistics_worker(self):
        """Ù…Ø­Ø¯Ø« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©"""
        while self.is_running:
            try:
                # ØªØ­Ø¯ÙŠØ« Ù†Ù‚Ø§Ø· Ø§Ù„Ø´Ø¹Ø¨ÙŠØ©
                async with self.db_manager.pool.acquire() as conn:
                    await conn.execute("""
                        UPDATE item_statistics
                        SET popularity_score = (
                            views_count * 1.0 +
                            likes_count * 3.0 +
                            saves_count * 4.0 +
                            shares_count * 5.0 +
                            comments_count * 4.5
                        ) / GREATEST(1, views_count + likes_count + saves_count + shares_count + comments_count)
                        WHERE last_updated < NOW() - INTERVAL '1 hour'
                    """)
                
                logger.info("ğŸ“Š ØªÙ… ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø´Ø¹Ø¨ÙŠØ©")
                
                # Ø§Ù†ØªØ¸Ø§Ø± Ø³Ø§Ø¹Ø© Ù‚Ø¨Ù„ Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªØ§Ù„ÙŠ
                await asyncio.sleep(3600)
                
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø­Ø¯Ø« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {str(e)}")
                await asyncio.sleep(1800)  # Ø§Ù†ØªØ¸Ø§Ø± Ù†ØµÙ Ø³Ø§Ø¹Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø®Ø·Ø£
    
    async def _model_backup_worker(self):
        """Ø¹Ø§Ù…Ù„ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬"""
        while self.is_running:
            try:
                # Ù†Ø³Ø® Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¥Ù„Ù‰ S3
                for model_id, model in self.loaded_models.items():
                    local_path = os.path.join(
                        self.config.ml.model_storage_path, 
                        f"{model_id}_backup.pkl"
                    )
                    
                    # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ù„ÙŠØ§Ù‹
                    await asyncio.get_event_loop().run_in_executor(
                        self.executor, joblib.dump, model, local_path
                    )
                    
                    # Ø±ÙØ¹ Ø¥Ù„Ù‰ S3
                    s3_key = f"models/backups/{model_id}_{int(time.time())}.pkl"
                    await self.s3_manager.upload_model(local_path, s3_key)
                
                logger.info("ğŸ’¾ ØªÙ… Ø¥Ø¬Ø±Ø§Ø¡ Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬")
                
                # Ø§Ù†ØªØ¸Ø§Ø± ÙŠÙˆÙ… ÙƒØ§Ù…Ù„
                await asyncio.sleep(86400)
                
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ: {str(e)}")
                await asyncio.sleep(3600)
    
    async def _cache_cleanup_worker(self):
        """Ø¹Ø§Ù…Ù„ ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        while self.is_running:
            try:
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…Ù†ØªÙ‡ÙŠØ© Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
                async with self.db_manager.pool.acquire() as conn:
                    deleted = await conn.execute("""
                        DELETE FROM cached_recommendations
                        WHERE expires_at < NOW()
                    """)
                
                if deleted:
                    logger.info(f"ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ {deleted} ØªÙˆØµÙŠØ© Ù…Ù†ØªÙ‡ÙŠØ© Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©")
                
                # Ø§Ù†ØªØ¸Ø§Ø± Ø³Ø§Ø¹Ø©
                await asyncio.sleep(3600)
                
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {str(e)}")
                await asyncio.sleep(1800)
    
    async def get_recommendations(self, user_id: str, 
                                recommendation_type: str = "personalized",
                                count: int = 10,
                                context: Optional[Dict] = None) -> List[Dict]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø£ÙˆÙ„Ø§Ù‹
        cached_recs = await self.redis_manager.get_cached_recommendations(
            user_id, recommendation_type
        )
        
        if cached_recs:
            logger.info(f"ğŸ“¦ Ø¥Ø±Ø¬Ø§Ø¹ ØªÙˆØµÙŠØ§Øª Ù…Ø®Ø²Ù†Ø© Ù…Ø¤Ù‚ØªØ§Ù‹ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id}")
            return cached_recs[:count]
        
        # Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆØµÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø©
        recommendations = await self._generate_recommendations(
            user_id, recommendation_type, count, context
        )
        
        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        await self.redis_manager.cache_recommendations(
            user_id, recommendations, recommendation_type, expire=300
        )
        
        return recommendations
    
    async def _generate_recommendations(self, user_id: str, 
                                      recommendation_type: str,
                                      count: int,
                                      context: Optional[Dict] = None) -> List[Dict]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆØµÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø©"""
        
        try:
            # Ø¬Ù„Ø¨ Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            user_profile = await self.db_manager.get_user_profile(user_id)
            
            # Ø¬Ù„Ø¨ ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø­Ø¯ÙŠØ«Ø©
            user_interactions = await self.db_manager.get_user_interactions(
                user_id, limit=100
            )
            
            # Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙƒØ¨Ø¯ÙŠÙ„
            popular_items = await self.db_manager.get_popular_items(count * 2)
            
            # ØªØ·Ø¨ÙŠÙ‚ Ù…Ù†Ø·Ù‚ Ø§Ù„ØªÙˆØµÙŠØ© (Ù…Ø¨Ø³Ø· Ù„Ù„ØªÙˆØ¶ÙŠØ­)
            recommendations = []
            
            if recommendation_type == "trending":
                # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø´Ø¹Ø¨ÙŠØ©
                for i, item in enumerate(popular_items[:count]):
                    recommendations.append({
                        'item_id': item['item_id'],
                        'score': 0.9 - (i * 0.05),
                        'reason': 'Ù…Ù‚Ø§Ù„ Ø±Ø§Ø¦Ø¬',
                        'metadata': {'rank': i + 1, 'type': 'trending'}
                    })
            
            elif recommendation_type == "personalized" and user_profile:
                # ØªÙˆØµÙŠØ§Øª Ø´Ø®ØµÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù
                interests = user_profile.get('interests', {})
                
                for i in range(count):
                    recommendations.append({
                        'item_id': f"personal_{user_id}_{i+1}",
                        'score': 0.8 - (i * 0.03),
                        'reason': 'Ù…Ø®ØµØµ Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§ØªÙƒ',
                        'metadata': {'interests': list(interests.keys())[:3]}
                    })
            
            else:
                # ØªÙˆØµÙŠØ§Øª Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
                for i in range(count):
                    recommendations.append({
                        'item_id': f"default_{i+1}",
                        'score': 0.7 - (i * 0.04),
                        'reason': 'ØªÙˆØµÙŠØ© Ø¹Ø§Ù…Ø©',
                        'metadata': {'type': 'default'}
                    })
            
            logger.info(f"ğŸ¯ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(recommendations)} ØªÙˆØµÙŠØ© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id}")
            
            return recommendations
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª: {str(e)}")
            return []
    
    async def record_interaction(self, user_id: str, item_id: str,
                               interaction_type: str, rating: Optional[float] = None,
                               context: Optional[Dict] = None,
                               features: Optional[np.ndarray] = None) -> bool:
        """ØªØ³Ø¬ÙŠÙ„ ØªÙØ§Ø¹Ù„ Ù…Ø³ØªØ®Ø¯Ù…"""
        
        try:
            # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            interaction_id = await self.db_manager.save_interaction(
                user_id, item_id, interaction_type, rating, context, features
            )
            
            # ØªØ­Ø¯ÙŠØ« Ø¹Ø¯Ø§Ø¯Ø§Øª ÙÙˆØ±ÙŠØ© ÙÙŠ Redis
            await self.redis_manager.increment_counter(f"interactions_today:{item_id}")
            await self.redis_manager.increment_counter(f"user_activity_today:{user_id}")
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø© Ù…Ø¤Ù‚ØªØ§Ù‹ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ù†Ø´Ø§Ø¦Ù‡Ø§)
            await self.redis_manager.delete(f"recommendations:{user_id}:personalized")
            
            logger.info(f"ğŸ“ ØªÙ… ØªØ³Ø¬ÙŠÙ„ ØªÙØ§Ø¹Ù„: {user_id} {interaction_type} {item_id}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙØ§Ø¹Ù„: {str(e)}")
            return False
    
    async def get_system_metrics(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù†Ø¸Ø§Ù…"""
        
        try:
            # Ù…Ù‚Ø§ÙŠÙŠØ³ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            async with self.db_manager.pool.acquire() as conn:
                db_stats = await conn.fetchrow("""
                    SELECT 
                        (SELECT COUNT(*) FROM user_interactions) as total_interactions,
                        (SELECT COUNT(*) FROM user_profiles) as total_users,
                        (SELECT COUNT(*) FROM item_statistics) as total_items,
                        (SELECT COUNT(*) FROM trained_models WHERE is_active = true) as active_models
                """)
            
            # Ù…Ù‚Ø§ÙŠÙŠØ³ Redis
            redis_info = {
                'connected': self.redis_manager.redis_client is not None,
                'cached_recommendations': 0  # ÙŠÙ…ÙƒÙ† ØªØ·ÙˆÙŠØ± Ù‡Ø°Ø§
            }
            
            # Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            model_stats = {
                'loaded_models': len(self.loaded_models),
                'model_types': list(set(meta['type'] for meta in self.model_metadata.values()))
            }
            
            # Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù†Ø¸Ø§Ù…
            system_stats = {
                'uptime_hours': (datetime.now() - datetime.now().replace(hour=0, minute=0, second=0)).total_seconds() / 3600,
                'background_tasks_running': len([t for t in self.background_tasks if not t.done()]),
                'is_healthy': self.is_running
            }
            
            return {
                'database': dict(db_stats) if db_stats else {},
                'redis': redis_info,
                'models': model_stats,
                'system': system_stats,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù†Ø¸Ø§Ù…: {str(e)}")
            return {'error': str(e)}
    
    async def shutdown(self):
        """Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù†Ø¸Ø§Ù…"""
        logger.info("ğŸ”š Ø¨Ø¯Ø¡ Ø¥ØºÙ„Ø§Ù‚ Ù†Ø¸Ø§Ù… ML...")
        
        self.is_running = False
        
        # Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø®Ù„ÙÙŠØ©
        for task in self.background_tasks:
            task.cancel()
        
        # Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„Ù…Ù‡Ø§Ù…
        await asyncio.gather(*self.background_tasks, return_exceptions=True)
        
        # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª
        await self.db_manager.close()
        await self.redis_manager.close()
        
        # Ø¥ØºÙ„Ø§Ù‚ Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ø®ÙŠÙˆØ·
        self.executor.shutdown(wait=True)
        self.process_executor.shutdown(wait=True)
        
        logger.info("âœ… ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ù†Ø¸Ø§Ù… ML Ø¨Ù†Ø¬Ø§Ø­")


# ========================= ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… =========================

async def create_ml_pipeline(config_path: str = "config.yaml") -> MLPipelineManager:
    """Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… ML"""
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙƒÙˆÙŠÙ†
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
    else:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
        config_dict = {
            'database': {
                'host': os.getenv('DB_HOST', 'localhost'),
                'port': int(os.getenv('DB_PORT', 5432)),
                'database': os.getenv('DB_NAME', 'sabq_ai_recommendations'),
                'username': os.getenv('DB_USER', 'postgres'),
                'password': os.getenv('DB_PASSWORD', '')
            },
            'redis': {
                'host': os.getenv('REDIS_HOST', 'localhost'),
                'port': int(os.getenv('REDIS_PORT', 6379)),
                'password': os.getenv('REDIS_PASSWORD')
            },
            's3': {
                'bucket_name': os.getenv('S3_BUCKET', 'sabq-ai-models'),
                'region': os.getenv('AWS_REGION', 'us-east-1'),
                'access_key': os.getenv('AWS_ACCESS_KEY_ID'),
                'secret_key': os.getenv('AWS_SECRET_ACCESS_KEY')
            },
            'ml': {
                'model_storage_path': os.getenv('MODEL_PATH', './models'),
                'gpu_enabled': torch.cuda.is_available()
            }
        }
    
    # Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†
    config = SystemConfig(
        database=DatabaseConfig(**config_dict.get('database', {})),
        redis=RedisConfig(**config_dict.get('redis', {})),
        s3=S3Config(**config_dict.get('s3', {})),
        ml=MLConfig(**config_dict.get('ml', {}))
    )
    
    # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø¯ÙŠØ±
    pipeline_manager = MLPipelineManager(config)
    await pipeline_manager.initialize()
    
    return pipeline_manager


# ========================= Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… =========================

if __name__ == "__main__":
    async def main():
        # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… ML
        pipeline = await create_ml_pipeline()
        
        try:
            # ØªØ³Ø¬ÙŠÙ„ Ø¨Ø¹Ø¶ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©
            await pipeline.record_interaction(
                "user_123", "article_456", "view", 
                context={"device": "mobile", "time": "morning"}
            )
            
            await pipeline.record_interaction(
                "user_123", "article_456", "like", rating=4.5
            )
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª
            recommendations = await pipeline.get_recommendations(
                "user_123", "personalized", count=5
            )
            
            print("ğŸ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª:")
            for rec in recommendations:
                print(f"  {rec['item_id']}: {rec['score']:.2f} - {rec['reason']}")
            
            # Ø¹Ø±Ø¶ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù†Ø¸Ø§Ù…
            metrics = await pipeline.get_system_metrics()
            print(f"\nğŸ“Š Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù†Ø¸Ø§Ù…:")
            print(f"  Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙˆÙ†: {metrics['database'].get('total_users', 0)}")
            print(f"  Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª: {metrics['database'].get('total_interactions', 0)}")
            print(f"  Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ù…Ù„Ø©: {metrics['models']['loaded_models']}")
            
            # Ø§Ù†ØªØ¸Ø§Ø± Ù‚Ù„ÙŠÙ„ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª
            await asyncio.sleep(5)
            
        finally:
            # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù†Ø¸Ø§Ù…
            await pipeline.shutdown()
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„
    asyncio.run(main())
