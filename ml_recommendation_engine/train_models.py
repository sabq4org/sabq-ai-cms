#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø°ÙƒÙŠ - Ø³Ø¨Ù‚ Ø§Ù„Ø°ÙƒÙŠØ©
Ù†Øµ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ©
Sabq AI Recommendation Engine - Model Training Script
"""

import asyncio
import logging
import sys
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
sys.path.append(str(Path(__file__).parent))

from config import settings, MODEL_CONFIG
from infrastructure.database_manager import DatabaseManager
from infrastructure.redis_manager import RedisManager
from infrastructure.s3_manager import S3Manager
from models.collaborative_filtering import CollaborativeFiltering
from models.content_based_filtering import ContentBasedFiltering
from models.deep_learning_models import DeepLearningModels
from models.hybrid_recommendation import HybridRecommendation
from models.user_interest_analysis import UserInterestAnalysis
from models.contextual_recommendations import ContextualRecommendations
from models.continuous_learning import ContinuousLearning

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
logging.basicConfig(
    level=getattr(logging, settings.log_level),
    format='[%(asctime)s] %(levelname)s in %(module)s: %(message)s'
)
logger = logging.getLogger(__name__)

class ModelTrainer:
    """Ù…Ø¯Ø±Ø¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„"""
    
    def __init__(self):
        self.db_manager = DatabaseManager()
        self.redis_manager = RedisManager()
        self.s3_manager = S3Manager()
        
        # Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        self.collaborative_model = CollaborativeFiltering()
        self.content_model = ContentBasedFiltering()
        self.deep_models = DeepLearningModels()
        self.hybrid_model = HybridRecommendation()
        self.user_interest_model = UserInterestAnalysis()
        self.contextual_model = ContextualRecommendations()
        self.continuous_model = ContinuousLearning()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        self.training_stats = {}
        
    async def initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª"""
        try:
            await self.db_manager.initialize()
            await self.redis_manager.initialize()
            await self.s3_manager.initialize()
            logger.info("ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª: {e}")
            raise
    
    async def load_training_data(self) -> Dict[str, pd.DataFrame]:
        """ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        logger.info("Ø¨Ø¯Ø¡ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
        
        try:
            # ØªØ­Ù…ÙŠÙ„ ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
            interactions_query = """
            SELECT 
                user_id,
                article_id,
                interaction_type,
                rating,
                reading_time,
                scroll_depth,
                created_at,
                context_data
            FROM user_interactions 
            WHERE created_at >= NOW() - INTERVAL '6 months'
            ORDER BY created_at DESC
            """
            
            interactions_df = await self.db_manager.fetch_dataframe(interactions_query)
            logger.info(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(interactions_df)} ØªÙØ§Ø¹Ù„ Ù…Ø³ØªØ®Ø¯Ù…")
            
            # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
            articles_query = """
            SELECT 
                id as article_id,
                title,
                content,
                category,
                tags,
                author_id,
                publish_date,
                view_count,
                like_count,
                share_count,
                reading_time_estimate,
                topic_classification,
                sentiment_score,
                keywords
            FROM articles 
            WHERE status = 'published'
            AND publish_date >= NOW() - INTERVAL '12 months'
            """
            
            articles_df = await self.db_manager.fetch_dataframe(articles_query)
            logger.info(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(articles_df)} Ù…Ù‚Ø§Ù„")
            
            # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
            users_query = """
            SELECT 
                id as user_id,
                age,
                gender,
                location,
                interests,
                reading_preferences,
                subscription_type,
                registration_date,
                last_active,
                behavior_profile
            FROM users 
            WHERE is_active = true
            """
            
            users_df = await self.db_manager.fetch_dataframe(users_query)
            logger.info(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(users_df)} Ù…Ø³ØªØ®Ø¯Ù…")
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø²Ù…Ù†ÙŠ
            context_query = """
            SELECT 
                user_id,
                session_id,
                device_type,
                browser,
                location,
                time_of_day,
                day_of_week,
                weather,
                mood_indicator,
                session_duration,
                created_at
            FROM user_sessions 
            WHERE created_at >= NOW() - INTERVAL '3 months'
            """
            
            context_df = await self.db_manager.fetch_dataframe(context_query)
            logger.info(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(context_df)} Ø¬Ù„Ø³Ø© Ù…Ø³ØªØ®Ø¯Ù…")
            
            return {
                'interactions': interactions_df,
                'articles': articles_df,
                'users': users_df,
                'context': context_df
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {e}")
            raise
    
    async def prepare_training_data(self, raw_data: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
        """ØªØ­Ø¶ÙŠØ± ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
        logger.info("Ø¨Ø¯Ø¡ ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨...")
        
        try:
            # Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            interactions = raw_data['interactions']
            articles = raw_data['articles']
            users = raw_data['users']
            context = raw_data['context']
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØµÙÙˆÙØ© Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª
            user_item_matrix = interactions.pivot_table(
                index='user_id',
                columns='article_id',
                values='rating',
                fill_value=0
            )
            
            # ØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            content_features = await self.content_model.extract_features(articles)
            
            # ØªØ­Ø¶ÙŠØ± Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
            user_profiles = await self.user_interest_model.build_profiles(
                interactions, users, context
            )
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø±
            train_interactions, test_interactions = self._split_temporal(
                interactions, test_ratio=0.2
            )
            
            logger.info("ØªÙ… ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
            
            return {
                'user_item_matrix': user_item_matrix,
                'content_features': content_features,
                'user_profiles': user_profiles,
                'train_interactions': train_interactions,
                'test_interactions': test_interactions,
                'articles': articles,
                'users': users,
                'context': context
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            raise
    
    def _split_temporal(self, interactions: pd.DataFrame, test_ratio: float = 0.2):
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø²Ù…Ù†ÙŠØ§Ù‹"""
        interactions_sorted = interactions.sort_values('created_at')
        split_point = int(len(interactions_sorted) * (1 - test_ratio))
        
        train_data = interactions_sorted.iloc[:split_point]
        test_data = interactions_sorted.iloc[split_point:]
        
        return train_data, test_data
    
    async def train_collaborative_filtering(self, prepared_data: Dict[str, Any]):
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„ØªØ¹Ø§ÙˆÙ†ÙŠØ©"""
        logger.info("Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„ØªØ¹Ø§ÙˆÙ†ÙŠØ©...")
        
        try:
            user_item_matrix = prepared_data['user_item_matrix']
            
            # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
            models_performance = {}
            
            # Matrix Factorization
            logger.info("ØªØ¯Ø±ÙŠØ¨ Matrix Factorization...")
            mf_performance = await self.collaborative_model.train_matrix_factorization(
                user_item_matrix,
                **MODEL_CONFIG['collaborative_filtering']
            )
            models_performance['matrix_factorization'] = mf_performance
            
            # Neural Collaborative Filtering
            logger.info("ØªØ¯Ø±ÙŠØ¨ Neural Collaborative Filtering...")
            ncf_performance = await self.collaborative_model.train_neural_collaborative_filtering(
                prepared_data['train_interactions'],
                **MODEL_CONFIG['neural_collaborative']
            )
            models_performance['neural_collaborative'] = ncf_performance
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            await self.collaborative_model.save_models(settings.model_path)
            
            self.training_stats['collaborative_filtering'] = models_performance
            logger.info("ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„ØªØ¹Ø§ÙˆÙ†ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„ØªØ¹Ø§ÙˆÙ†ÙŠØ©: {e}")
            raise
    
    async def train_content_based_filtering(self, prepared_data: Dict[str, Any]):
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆØ§Ø¦ÙŠØ©"""
        logger.info("Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆØ§Ø¦ÙŠØ©...")
        
        try:
            articles = prepared_data['articles']
            content_features = prepared_data['content_features']
            
            # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ù…Ø®ØªÙ„ÙØ©
            performance_metrics = {}
            
            # BERT Ø§Ù„Ø¹Ø±Ø¨ÙŠ
            logger.info("ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ BERT Ø§Ù„Ø¹Ø±Ø¨ÙŠ...")
            bert_performance = await self.content_model.train_bert_model(
                articles['content'].tolist(),
                articles['category'].tolist()
            )
            performance_metrics['bert'] = bert_performance
            
            # TF-IDF
            logger.info("ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ TF-IDF...")
            tfidf_performance = await self.content_model.train_tfidf_model(
                articles,
                **MODEL_CONFIG['content_based']
            )
            performance_metrics['tfidf'] = tfidf_performance
            
            # Word2Vec
            logger.info("ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Word2Vec...")
            w2v_performance = await self.content_model.train_word2vec_model(
                articles['content'].tolist()
            )
            performance_metrics['word2vec'] = w2v_performance
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            await self.content_model.save_models(settings.model_path)
            
            self.training_stats['content_based'] = performance_metrics
            logger.info("ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆØ§Ø¦ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆØ§Ø¦ÙŠØ©: {e}")
            raise
    
    async def train_deep_learning_models(self, prepared_data: Dict[str, Any]):
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚"""
        logger.info("Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚...")
        
        try:
            train_interactions = prepared_data['train_interactions']
            test_interactions = prepared_data['test_interactions']
            
            # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
            performance_metrics = {}
            
            # Deep Matrix Factorization
            logger.info("ØªØ¯Ø±ÙŠØ¨ Deep Matrix Factorization...")
            dmf_performance = await self.deep_models.train_deep_matrix_factorization(
                train_interactions,
                test_interactions,
                **MODEL_CONFIG['deep_learning']
            )
            performance_metrics['deep_mf'] = dmf_performance
            
            # Transformer-based Model
            logger.info("ØªØ¯Ø±ÙŠØ¨ Transformer Model...")
            transformer_performance = await self.deep_models.train_transformer_model(
                train_interactions,
                test_interactions
            )
            performance_metrics['transformer'] = transformer_performance
            
            # VAE Model
            logger.info("ØªØ¯Ø±ÙŠØ¨ VAE Model...")
            vae_performance = await self.deep_models.train_vae_model(
                prepared_data['user_item_matrix']
            )
            performance_metrics['vae'] = vae_performance
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            await self.deep_models.save_models(settings.model_path)
            
            self.training_stats['deep_learning'] = performance_metrics
            logger.info("ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚: {e}")
            raise
    
    async def train_hybrid_system(self, prepared_data: Dict[str, Any]):
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†"""
        logger.info("Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†...")
        
        try:
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø©
            await self.hybrid_model.load_base_models(settings.model_path)
            
            # ØªØ¯Ø±ÙŠØ¨ ÙˆØ­Ø¯Ø© Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ù…ØªÙƒÙŠÙ
            performance = await self.hybrid_model.train_adaptive_weighting(
                prepared_data['train_interactions'],
                prepared_data['test_interactions']
            )
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù‡Ø¬ÙŠÙ†
            await self.hybrid_model.save_model(settings.model_path)
            
            self.training_stats['hybrid'] = performance
            logger.info("ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ† Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†: {e}")
            raise
    
    async def train_contextual_models(self, prepared_data: Dict[str, Any]):
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©"""
        logger.info("Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©...")
        
        try:
            context_data = prepared_data['context']
            interactions = prepared_data['train_interactions']
            
            # ØªØ¯Ø±ÙŠØ¨ ÙƒØ§Ø´Ù Ø§Ù„Ù…Ø²Ø§Ø¬
            mood_performance = await self.contextual_model.train_mood_detector(
                context_data, interactions
            )
            
            # ØªØ¯Ø±ÙŠØ¨ Ù…Ø­Ù„Ù„ Ø§Ù„Ø³ÙŠØ§Ù‚
            context_performance = await self.contextual_model.train_context_analyzer(
                context_data, interactions
            )
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            await self.contextual_model.save_models(settings.model_path)
            
            self.training_stats['contextual'] = {
                'mood_detection': mood_performance,
                'context_analysis': context_performance
            }
            logger.info("ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©: {e}")
            raise
    
    async def setup_continuous_learning(self, prepared_data: Dict[str, Any]):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±"""
        logger.info("Ø¨Ø¯Ø¡ Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±...")
        
        try:
            # ØªÙ‡ÙŠØ¦Ø© Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ´ØºÙŠÙ„
            await self.continuous_model.initialize_memory(
                prepared_data['train_interactions']
            )
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ ÙƒØ§Ø´Ù Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…ÙŠ
            await self.continuous_model.setup_drift_detector(
                prepared_data['train_interactions']
            )
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ØªØ¹Ù„Ù… Ø§Ù„Ù†Ø´Ø·
            await self.continuous_model.setup_active_learner()
            
            logger.info("ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø± Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±: {e}")
            raise
    
    async def evaluate_models(self, prepared_data: Dict[str, Any]):
        """ØªÙ‚ÙŠÙŠÙ… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        logger.info("Ø¨Ø¯Ø¡ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬...")
        
        try:
            test_interactions = prepared_data['test_interactions']
            
            # ØªÙ‚ÙŠÙŠÙ… Ø´Ø§Ù…Ù„
            evaluation_results = {}
            
            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„ØªØ¹Ø§ÙˆÙ†ÙŠØ©
            cf_metrics = await self.collaborative_model.evaluate_models(test_interactions)
            evaluation_results['collaborative_filtering'] = cf_metrics
            
            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆØ§Ø¦ÙŠØ©
            cb_metrics = await self.content_model.evaluate_models(test_interactions)
            evaluation_results['content_based'] = cb_metrics
            
            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚
            dl_metrics = await self.deep_models.evaluate_models(test_interactions)
            evaluation_results['deep_learning'] = dl_metrics
            
            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†
            hybrid_metrics = await self.hybrid_model.evaluate_model(test_interactions)
            evaluation_results['hybrid'] = hybrid_metrics
            
            self.training_stats['evaluation'] = evaluation_results
            
            # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            self._print_evaluation_results(evaluation_results)
            
            logger.info("ØªÙ… ØªÙ‚ÙŠÙŠÙ… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")
            raise
    
    def _print_evaluation_results(self, results: Dict[str, Any]):
        """Ø·Ø¨Ø§Ø¹Ø© Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…"""
        print("\n" + "="*80)
        print("ØªÙ‚Ø±ÙŠØ± ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ - Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø°ÙƒÙŠ Ø³Ø¨Ù‚")
        print("="*80)
        
        for model_name, metrics in results.items():
            print(f"\n{model_name.upper()}:")
            print("-" * 40)
            
            if isinstance(metrics, dict):
                for metric_name, value in metrics.items():
                    if isinstance(value, float):
                        print(f"  {metric_name}: {value:.4f}")
                    else:
                        print(f"  {metric_name}: {value}")
            else:
                print(f"  Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù…: {metrics}")
        
        print("\n" + "="*80)
    
    async def save_models_to_cloud(self):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ"""
        logger.info("Ø¨Ø¯Ø¡ Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ S3...")
        
        try:
            model_files = []
            model_path = Path(settings.model_path)
            
            # Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            for file_path in model_path.rglob("*"):
                if file_path.is_file():
                    model_files.append(file_path)
            
            # Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª
            upload_results = []
            for file_path in model_files:
                relative_path = file_path.relative_to(model_path)
                s3_key = f"models/{relative_path}"
                
                result = await self.s3_manager.upload_file(
                    str(file_path),
                    s3_key
                )
                upload_results.append(result)
            
            logger.info(f"ØªÙ… Ø±ÙØ¹ {len(upload_results)} Ù…Ù„Ù Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ S3")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ S3: {e}")
            raise
    
    async def generate_training_report(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = Path(settings.log_path) / f"training_report_{timestamp}.json"
        
        try:
            import json
            
            training_report = {
                "timestamp": timestamp,
                "training_stats": self.training_stats,
                "model_config": MODEL_CONFIG,
                "settings": {
                    "environment": settings.environment,
                    "model_path": settings.model_path,
                    "max_recommendations": settings.max_recommendations
                }
            }
            
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(training_report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {report_path}")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {e}")
    
    async def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        try:
            await self.db_manager.close()
            await self.redis_manager.close()
            logger.info("ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯")
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯: {e}")

async def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
    trainer = ModelTrainer()
    
    try:
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
        await trainer.initialize()
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        raw_data = await trainer.load_training_data()
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        prepared_data = await trainer.prepare_training_data(raw_data)
        
        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        await trainer.train_collaborative_filtering(prepared_data)
        await trainer.train_content_based_filtering(prepared_data)
        await trainer.train_deep_learning_models(prepared_data)
        await trainer.train_hybrid_system(prepared_data)
        await trainer.train_contextual_models(prepared_data)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±
        await trainer.setup_continuous_learning(prepared_data)
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        await trainer.evaluate_models(prepared_data)
        
        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
        if settings.aws_access_key_id:
            await trainer.save_models_to_cloud()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        await trainer.generate_training_report()
        
        logger.info("ğŸ‰ ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {e}")
        raise
    finally:
        await trainer.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
